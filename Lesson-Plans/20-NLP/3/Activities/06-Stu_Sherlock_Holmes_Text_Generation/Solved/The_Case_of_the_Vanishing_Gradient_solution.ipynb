{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy, import the large english model (this is 600 MB), and disable tagger, ner and lemmatizer.\n",
    "# Download the large English model ~ 600 MB if you have not done so: `python -m spacy download en_core_web_lg`.\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg',disable=[\"tagger\", \"ner\", \"lemmatizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the text file with the read_file function.\n",
    "def read_file(filepath):\n",
    "    \"\"\"\n",
    "    Reads in a text file from the directory and saves the file contents to a variable.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to read and save\n",
    "        \n",
    "    Returns: \n",
    "        A string containing the file contents.\n",
    "    \"\"\"\n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()\n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tokenize and Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the separate_punc function to remove the puncutation. \n",
    "def separate_punc(holmes_text):\n",
    "    \"\"\"\n",
    "    Retrieves all the words in the text without punctuation \n",
    "\n",
    "    Args:\n",
    "        text (str): The input text from which words are extracted witout punctuation.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of words\n",
    "\n",
    "    \"\"\"\n",
    "    # Create a list comprehension to get only the tokens, i.e., words in the text.\n",
    "    return [token.text.lower() for token in nlp(holmes_text) \\\n",
    "            if token.text not in '\\n\\n \\n\\n\\n!\"“”-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“My dear fellow,” said Sherlock Holmes as we sat on either side of the\n",
      "fire in his lodgings at Baker Street, “life is infinitely stranger than\n",
      "anything which the mind of man could invent. We would not dare to\n",
      "conceive the things which are really mere commonplaces of existence. If\n",
      "we could fly out of that window hand in hand, hover over this great\n",
      "city, gently remove the roofs, and peep in at the queer things which\n",
      "are going on, the strange coincidences, the plannings, the\n",
      "cross-purposes, the wonderful chains of events, working through\n",
      "generations, and leading to the most _outré_ results, it would make all\n",
      "fiction with its conventionalities and foreseen conclusions most stale\n",
      "and unprofitable.”\n",
      "\n",
      "“And yet I am not convinced of it,” I answered. “The cases which come\n",
      "to light in the papers are, as a rule, bald enough, and vulgar enough.\n",
      "We have in our police reports realism pushed to its extreme limits, and\n",
      "yet the result is, it must be confessed, neither fascinating nor\n",
      "artistic.”\n",
      "\n",
      "“A certain selection and discretion must be used in producing a\n",
      "realistic effect,” remarked Holmes. “This is wanting in the police\n",
      "report, where more stress is laid, perhaps, upon the platitudes of the\n",
      "magistrate than upon the details, which to an observer contain the\n",
      "vital essence of the whole matter. Depend upon it, there is nothing so\n",
      "unnatural as the commonplace.”\n",
      "\n",
      "I smiled and shook my head. “I can quite understand your thinking so,”\n",
      "I said. “Of course, in your position of unofficial adviser and helper\n",
      "to everybody who is absolutely puzzled, throughout three continents,\n",
      "you are brought in contact with all that is strange and bizarre. But\n",
      "here”—I picked up the morning paper from the ground—“let us put it to a\n",
      "practical test. Here is the first heading upon which I come. ‘A\n",
      "husband’s cruelty to his wife.’ There is half a column of print, but I\n",
      "know without reading it that it is all perfectly familiar to me. There\n",
      "is, of course, the other woman, the drink, the push, the blow, the\n",
      "bruise, the sympathetic sister or landlady. The crudest of writers\n",
      "could invent nothing more crude.”\n",
      "\n",
      "“Indeed, your example is an unfortunate one for your argument,” said\n",
      "Holmes, taking the paper and glancing his eye down it. “This is the\n",
      "Dundas separation case, and, as it happens, I was engaged in clearing\n",
      "up some small points in connection with it. The husband was a\n",
      "teetotaler, there was no other woman, and the conduct complained of was\n",
      "that he had drifted into the habit of winding up every meal by taking\n",
      "out his false teeth and hurling them at his wife, which, you will\n",
      "allow, is not an action likely to occur to the imagination of the\n",
      "average story-teller. Take a pinch of snuff, Doctor, and acknowledge\n",
      "that I have scored over you in your example.”\n",
      "\n",
      "He held out his snuffbox of old gold, with a great amethyst in the\n",
      "centre of the lid. Its splendour was in such contrast to his homely\n",
      "ways and simple life that I could not help commenting upon it.\n",
      "\n",
      "“Ah,” said he, “I forgot that I had not seen you for some weeks. It is\n",
      "a little souvenir from the King of Bohemia in return for my assistance\n",
      "in the case of the Irene Adler papers.”\n",
      "\n",
      "“And the ring?” I asked, glancing at a remarkable brilliant which\n",
      "sparkled upon his finger.\n",
      "\n",
      "“It was from the reigning family of Holland, though the matter in which\n",
      "I served them was of such delicacy that I cannot confide it even to\n",
      "you, who have been good enough to chronicle one or two of my little\n",
      "problems.”\n",
      "\n",
      "“And have you any on hand just now?” I asked with interest.\n",
      "\n",
      "“Some ten or twelve, but none which present any feature of interest.\n",
      "They are important, you understand, without being interesting. Indeed,\n",
      "I have found that it is usually in unimportant matters that there is a\n",
      "field for the observation, and for the quick analysis of cause and\n",
      "effect which gives the charm to an investigation. The larger crimes are\n",
      "apt to be the simpler, for the bigger the crime the more obvious, as a\n",
      "rule, is the motive. In these cases, save for one rather intricate\n",
      "matter which has been referred to me from Marseilles, there is nothing\n",
      "which presents any features of interest. It is possible, however, that\n",
      "I may have something better before very many minutes are over, for this\n",
      "is one of my clients, or I am much mistaken.”\n",
      "\n",
      "He had risen from his chair and was standing between the parted blinds\n",
      "gazing down into the dull neutral-tinted London street. Looking over\n",
      "his shoulder, I saw that on the pavement opposite there stood a large\n",
      "woman with a heavy fur boa round her neck, and a large curling red\n",
      "feather in a broad-brimmed hat which was tilted in a coquettish Duchess\n",
      "of Devonshire fashion over her ear. From under this great panoply she\n",
      "peeped up in a nervous, hesitating fashion at our windows, while her\n",
      "body oscillated backward and forward, and her fingers fidgeted with her\n",
      "glove buttons. Suddenly, with a plunge, as of the swimmer who leaves\n",
      "the bank, she hurried across the road, and we heard the sharp clang of\n",
      "the bell.\n",
      "\n",
      "“I have seen those symptoms before,” said Holmes, throwing his\n",
      "cigarette into the fire. “Oscillation upon the pavement always means an\n",
      "_affaire de cœur_. She would like advice, but is not sure that the\n",
      "matter is not too delicate for communication. And yet even here we may\n",
      "discriminate. When a woman has been seriously wronged by a man she no\n",
      "longer oscillates, and the usual symptom is a broken bell wire. Here we\n",
      "may take it that there is a love matter, but that the maiden is not so\n",
      "much angry as perplexed, or grieved. But here she comes in person to\n",
      "resolve our doubts.”\n",
      "\n",
      "As he spoke there was a tap at the door, and the boy in buttons entered\n",
      "to announce Miss Mary Sutherland, while the lady herself loomed behind\n",
      "his small black figure like a full-sailed merchant-man behind a tiny\n",
      "pilot boat. Sherlock Holmes welcomed her with the easy courtesy for\n",
      "which he was remarkable, and, having closed the door and bowed her into\n",
      "an armchair, he looked her over in the minute and yet abstracted\n",
      "fashion which was peculiar to him.\n",
      "\n",
      "“Do you not find,” he said, “that with your short sight it is a little\n",
      "trying to do so much typewriting?”\n",
      "\n",
      "“I did at first,” she answered, “but now I know where the letters are\n",
      "without looking.” Then, suddenly realising the full purport of his\n",
      "words, she gave a violent start and looked up, with fear and\n",
      "astonishment upon her broad, good-humoured face. “You’ve heard about\n",
      "me, Mr. Holmes,” she cried, “else how could you know all that?”\n",
      "\n",
      "“Never mind,” said Holmes, laughing; “it is my business to know things.\n",
      "Perhaps I have trained myself to see what others overlook. If not, why\n",
      "should you come to consult me?”\n",
      "\n",
      "“I came to you, sir, because I heard of you from Mrs. Etherege, whose\n",
      "husband you found so easy when the police and everyone had given him up\n",
      "for dead. Oh, Mr. Holmes, I wish you would do as much for me. I’m not\n",
      "rich, but still I have a hundred a year in my own right, besides the\n",
      "little that I make by the machine, and I would give it all to know what\n",
      "has become of Mr. Hosmer Angel.”\n",
      "\n",
      "“Why did you come away to consult me in such a hurry?” asked Sherlock\n",
      "Holmes, with his finger-tips together and his eyes to the ceiling.\n",
      "\n",
      "Again a startled look came over the somewhat vacuous face of Miss Mary\n",
      "Sutherland. “Yes, I did bang out of the house,” she said, “for it made\n",
      "me angry to see the easy way in which Mr. Windibank—that is, my\n",
      "father—took it all. He would not go to the police, and he would not go\n",
      "to you, and so at last, as he would do nothing and kept on saying that\n",
      "there was no harm done, it made me mad, and I just on with my things\n",
      "and came right away to you.”\n",
      "\n",
      "“Your father,” said Holmes, “your stepfather, surely, since the name is\n",
      "different.”\n",
      "\n",
      "“Yes, my stepfather. I call him father, though it sounds funny, too,\n",
      "for he is only five years and two months older than myself.”\n",
      "\n",
      "“And your mother is alive?”\n",
      "\n",
      "“Oh, yes, mother is alive and well. I wasn’t best pleased, Mr. Holmes,\n",
      "when she married again so soon after father’s death, and a man who was\n",
      "nearly fifteen years younger than herself. Father was a plumber in the\n",
      "Tottenham Court Road, and he left a tidy business behind him, which\n",
      "mother carried on with Mr. Hardy, the foreman; but when Mr. Windibank\n",
      "came he made her sell the business, for he was very superior, being a\n",
      "traveller in wines. They got £ 4700 for the goodwill and interest,\n",
      "which wasn’t near as much as father could have got if he had been\n",
      "alive.”\n",
      "\n",
      "I had expected to see Sherlock Holmes impatient under this rambling and\n",
      "inconsequential narrative, but, on the contrary, he had listened with\n",
      "the greatest concentration of attention.\n",
      "\n",
      "“Your own little income,” he asked, “does it come out of the business?”\n",
      "\n",
      "“Oh, no, sir. It is quite separate and was left me by my uncle Ned in\n",
      "Auckland. It is in New Zealand stock, paying 4½ per cent. Two thousand\n",
      "five hundred pounds was the amount, but I can only touch the interest.”\n",
      "\n",
      "“You interest me extremely,” said Holmes. “And since you draw so large\n",
      "a sum as a hundred a year, with what you earn into the bargain, you no\n",
      "doubt travel a little and indulge yourself in every way. I believe that\n",
      "a single lady can get on very nicely upon an income of about £ 60.”\n",
      "\n",
      "“I could do with much less than that, Mr. Holmes, but you understand\n",
      "that as long as I live at home I don’t wish to be a burden to them, and\n",
      "so they have the use of the money just while I am staying with them. Of\n",
      "course, that is only just for the time. Mr. Windibank draws my interest\n",
      "every quarter and pays it over to mother, and I find that I can do\n",
      "pretty well with what I earn at typewriting. It brings me twopence a\n",
      "sheet, and I can often do from fifteen to twenty sheets in a day.”\n",
      "\n",
      "“You have made your position very clear to me,” said Holmes. “This is\n",
      "my friend, Dr. Watson, before whom you can speak as freely as before\n",
      "myself. Kindly tell us now all about your connection with Mr. Hosmer\n",
      "Angel.”\n",
      "\n",
      "A flush stole over Miss Sutherland’s face, and she picked nervously at\n",
      "the fringe of her jacket. “I met him first at the gasfitters’ ball,”\n",
      "she said. “They used to send father tickets when he was alive, and then\n",
      "afterwards they remembered us, and sent them to mother. Mr. Windibank\n",
      "did not wish us to go. He never did wish us to go anywhere. He would\n",
      "get quite mad if I wanted so much as to join a Sunday-school treat. But\n",
      "this time I was set on going, and I would go; for what right had he to\n",
      "prevent? He said the folk were not fit for us to know, when all\n",
      "father’s friends were to be there. And he said that I had nothing fit\n",
      "to wear, when I had my purple plush that I had never so much as taken\n",
      "out of the drawer. At last, when nothing else would do, he went off to\n",
      "France upon the business of the firm, but we went, mother and I, with\n",
      "Mr. Hardy, who used to be our foreman, and it was there I met Mr.\n",
      "Hosmer Angel.”\n",
      "\n",
      "“I suppose,” said Holmes, “that when Mr. Windibank came back from\n",
      "France he was very annoyed at your having gone to the ball.”\n",
      "\n",
      "“Oh, well, he was very good about it. He laughed, I remember, and\n",
      "shrugged his shoulders, and said there was no use denying anything to a\n",
      "woman, for she would have her way.”\n",
      "\n",
      "“I see. Then at the gasfitters’ ball you met, as I understand, a\n",
      "gentleman called Mr. Hosmer Angel.”\n",
      "\n",
      "“Yes, sir. I met him that night, and he called next day to ask if we\n",
      "had got home all safe, and after that we met him—that is to say, Mr.\n",
      "Holmes, I met him twice for walks, but after that father came back\n",
      "again, and Mr. Hosmer Angel could not come to the house any more.”\n",
      "\n",
      "“No?”\n",
      "\n",
      "“Well, you know father didn’t like anything of the sort. He wouldn’t\n",
      "have any visitors if he could help it, and he used to say that a woman\n",
      "should be happy in her own family circle. But then, as I used to say to\n",
      "mother, a woman wants her own circle to begin with, and I had not got\n",
      "mine yet.”\n",
      "\n",
      "“But how about Mr. Hosmer Angel? Did he make no attempt to see you?”\n",
      "\n",
      "“Well, father was going off to France again in a week, and Hosmer wrote\n",
      "and said that it would be safer and better not to see each other until\n",
      "he had gone. We could write in the meantime, and he used to write every\n",
      "day. I took the letters in in the morning, so there was no need for\n",
      "father to know.”\n",
      "\n",
      "“Were you engaged to the gentleman at this time?”\n",
      "\n",
      "“Oh, yes, Mr. Holmes. We were engaged after the first walk that we\n",
      "took. Hosmer—Mr. Angel—was a cashier in an office in Leadenhall\n",
      "Street—and—”\n",
      "\n",
      "“What office?”\n",
      "\n",
      "“That’s the worst of it, Mr. Holmes, I don’t know.”\n",
      "\n",
      "“Where did he live, then?”\n",
      "\n",
      "“He slept on the premises.”\n",
      "\n",
      "“And you don’t know his address?”\n",
      "\n",
      "“No—except that it was Leadenhall Street.”\n",
      "\n",
      "“Where did you address your letters, then?”\n",
      "\n",
      "“To the Leadenhall Street Post Office, to be left till called for. He\n",
      "said that if they were sent to the office he would be chaffed by all\n",
      "the other clerks about having letters from a lady, so I offered to\n",
      "typewrite them, like he did his, but he wouldn’t have that, for he said\n",
      "that when I wrote them they seemed to come from me, but when they were\n",
      "typewritten he always felt that the machine had come between us. That\n",
      "will just show you how fond he was of me, Mr. Holmes, and the little\n",
      "things that he would think of.”\n",
      "\n",
      "“It was most suggestive,” said Holmes. “It has long been an axiom of\n",
      "mine that the little things are infinitely the most important. Can you\n",
      "remember any other little things about Mr. Hosmer Angel?”\n",
      "\n",
      "“He was a very shy man, Mr. Holmes. He would rather walk with me in the\n",
      "evening than in the daylight, for he said that he hated to be\n",
      "conspicuous. Very retiring and gentlemanly he was. Even his voice was\n",
      "gentle. He’d had the quinsy and swollen glands when he was young, he\n",
      "told me, and it had left him with a weak throat, and a hesitating,\n",
      "whispering fashion of speech. He was always well dressed, very neat and\n",
      "plain, but his eyes were weak, just as mine are, and he wore tinted\n",
      "glasses against the glare.”\n",
      "\n",
      "“Well, and what happened when Mr. Windibank, your stepfather, returned\n",
      "to France?”\n",
      "\n",
      "“Mr. Hosmer Angel came to the house again and proposed that we should\n",
      "marry before father came back. He was in dreadful earnest and made me\n",
      "swear, with my hands on the Testament, that whatever happened I would\n",
      "always be true to him. Mother said he was quite right to make me swear,\n",
      "and that it was a sign of his passion. Mother was all in his favour\n",
      "from the first and was even fonder of him than I was. Then, when they\n",
      "talked of marrying within the week, I began to ask about father; but\n",
      "they both said never to mind about father, but just to tell him\n",
      "afterwards, and mother said she would make it all right with him. I\n",
      "didn’t quite like that, Mr. Holmes. It seemed funny that I should ask\n",
      "his leave, as he was only a few years older than me; but I didn’t want\n",
      "to do anything on the sly, so I wrote to father at Bordeaux, where the\n",
      "company has its French offices, but the letter came back to me on the\n",
      "very morning of the wedding.”\n",
      "\n",
      "“It missed him, then?”\n",
      "\n",
      "“Yes, sir; for he had started to England just before it arrived.”\n",
      "\n",
      "“Ha! that was unfortunate. Your wedding was arranged, then, for the\n",
      "Friday. Was it to be in church?”\n",
      "\n",
      "“Yes, sir, but very quietly. It was to be at St. Saviour’s, near King’s\n",
      "Cross, and we were to have breakfast afterwards at the St. Pancras\n",
      "Hotel. Hosmer came for us in a hansom, but as there were two of us he\n",
      "put us both into it and stepped himself into a four-wheeler, which\n",
      "happened to be the only other cab in the street. We got to the church\n",
      "first, and when the four-wheeler drove up we waited for him to step\n",
      "out, but he never did, and when the cabman got down from the box and\n",
      "looked there was no one there! The cabman said that he could not\n",
      "imagine what had become of him, for he had seen him get in with his own\n",
      "eyes. That was last Friday, Mr. Holmes, and I have never seen or heard\n",
      "anything since then to throw any light upon what became of him.”\n",
      "\n",
      "“It seems to me that you have been very shamefully treated,” said\n",
      "Holmes.\n",
      "\n",
      "“Oh, no, sir! He was too good and kind to leave me so. Why, all the\n",
      "morning he was saying to me that, whatever happened, I was to be true;\n",
      "and that even if something quite unforeseen occurred to separate us, I\n",
      "was always to remember that I was pledged to him, and that he would\n",
      "claim his pledge sooner or later. It seemed strange talk for a\n",
      "wedding-morning, but what has happened since gives a meaning to it.”\n",
      "\n",
      "“Most certainly it does. Your own opinion is, then, that some\n",
      "unforeseen catastrophe has occurred to him?”\n",
      "\n",
      "“Yes, sir. I believe that he foresaw some danger, or else he would not\n",
      "have talked so. And then I think that what he foresaw happened.”\n",
      "\n",
      "“But you have no notion as to what it could have been?”\n",
      "\n",
      "“None.”\n",
      "\n",
      "“One more question. How did your mother take the matter?”\n",
      "\n",
      "“She was angry, and said that I was never to speak of the matter\n",
      "again.”\n",
      "\n",
      "“And your father? Did you tell him?”\n",
      "\n",
      "“Yes; and he seemed to think, with me, that something had happened, and\n",
      "that I should hear of Hosmer again. As he said, what interest could\n",
      "anyone have in bringing me to the doors of the church, and then leaving\n",
      "me? Now, if he had borrowed my money, or if he had married me and got\n",
      "my money settled on him, there might be some reason, but Hosmer was\n",
      "very independent about money and never would look at a shilling of\n",
      "mine. And yet, what could have happened? And why could he not write?\n",
      "Oh, it drives me half-mad to think of it, and I can’t sleep a wink at\n",
      "night.” She pulled a little handkerchief out of her muff and began to\n",
      "sob heavily into it.\n",
      "\n",
      "“I shall glance into the case for you,” said Holmes, rising, “and I\n",
      "have no doubt that we shall reach some definite result. Let the weight\n",
      "of the matter rest upon me now, and do not let your mind dwell upon it\n",
      "further. Above all, try to let Mr. Hosmer Angel vanish from your\n",
      "memory, as he has done from your life.”\n",
      "\n",
      "“Then you don’t think I’ll see him again?”\n",
      "\n",
      "“I fear not.”\n",
      "\n",
      "“Then what has happened to him?”\n",
      "\n",
      "“You will leave that question in my hands. I should like an accurate\n",
      "description of him and any letters of his which you can spare.”\n",
      "\n",
      "“I advertised for him in last Saturday’s _Chronicle_,” said she. “Here\n",
      "is the slip and here are four letters from him.”\n",
      "\n",
      "“Thank you. And your address?”\n",
      "\n",
      "“No. 31 Lyon Place, Camberwell.”\n",
      "\n",
      "“Mr. Angel’s address you never had, I understand. Where is your\n",
      "father’s place of business?”\n",
      "\n",
      "“He travels for Westhouse & Marbank, the great claret importers of\n",
      "Fenchurch Street.”\n",
      "\n",
      "“Thank you. You have made your statement very clearly. You will leave\n",
      "the papers here, and remember the advice which I have given you. Let\n",
      "the whole incident be a sealed book, and do not allow it to affect your\n",
      "life.”\n",
      "\n",
      "“You are very kind, Mr. Holmes, but I cannot do that. I shall be true\n",
      "to Hosmer. He shall find me ready when he comes back.”\n",
      "\n",
      "For all the preposterous hat and the vacuous face, there was something\n",
      "noble in the simple faith of our visitor which compelled our respect.\n",
      "She laid her little bundle of papers upon the table and went her way,\n",
      "with a promise to come again whenever she might be summoned.\n",
      "\n",
      "Sherlock Holmes sat silent for a few minutes with his fingertips still\n",
      "pressed together, his legs stretched out in front of him, and his gaze\n",
      "directed upward to the ceiling. Then he took down from the rack the old\n",
      "and oily clay pipe, which was to him as a counsellor, and, having lit\n",
      "it, he leaned back in his chair, with the thick blue cloud-wreaths\n",
      "spinning up from him, and a look of infinite languor in his face.\n",
      "\n",
      "“Quite an interesting study, that maiden,” he observed. “I found her\n",
      "more interesting than her little problem, which, by the way, is rather\n",
      "a trite one. You will find parallel cases, if you consult my index, in\n",
      "Andover in ’77, and there was something of the sort at The Hague last\n",
      "year. Old as is the idea, however, there were one or two details which\n",
      "were new to me. But the maiden herself was most instructive.”\n",
      "\n",
      "“You appeared to read a good deal upon her which was quite invisible to\n",
      "me,” I remarked.\n",
      "\n",
      "“Not invisible but unnoticed, Watson. You did not know where to look,\n",
      "and so you missed all that was important. I can never bring you to\n",
      "realise the importance of sleeves, the suggestiveness of thumb-nails,\n",
      "or the great issues that may hang from a boot-lace. Now, what did you\n",
      "gather from that woman’s appearance? Describe it.”\n",
      "\n",
      "“Well, she had a slate-coloured, broad-brimmed straw hat, with a\n",
      "feather of a brickish red. Her jacket was black, with black beads sewn\n",
      "upon it, and a fringe of little black jet ornaments. Her dress was\n",
      "brown, rather darker than coffee colour, with a little purple plush at\n",
      "the neck and sleeves. Her gloves were greyish and were worn through at\n",
      "the right forefinger. Her boots I didn’t observe. She had small round,\n",
      "hanging gold earrings, and a general air of being fairly well-to-do in\n",
      "a vulgar, comfortable, easy-going way.”\n",
      "\n",
      "Sherlock Holmes clapped his hands softly together and chuckled.\n",
      "\n",
      "“’Pon my word, Watson, you are coming along wonderfully. You have\n",
      "really done very well indeed. It is true that you have missed\n",
      "everything of importance, but you have hit upon the method, and you\n",
      "have a quick eye for colour. Never trust to general impressions, my\n",
      "boy, but concentrate yourself upon details. My first glance is always\n",
      "at a woman’s sleeve. In a man it is perhaps better first to take the\n",
      "knee of the trouser. As you observe, this woman had plush upon her\n",
      "sleeves, which is a most useful material for showing traces. The double\n",
      "line a little above the wrist, where the typewritist presses against\n",
      "the table, was beautifully defined. The sewing-machine, of the hand\n",
      "type, leaves a similar mark, but only on the left arm, and on the side\n",
      "of it farthest from the thumb, instead of being right across the\n",
      "broadest part, as this was. I then glanced at her face, and, observing\n",
      "the dint of a pince-nez at either side of her nose, I ventured a remark\n",
      "upon short sight and typewriting, which seemed to surprise her.”\n",
      "\n",
      "“It surprised me.”\n",
      "\n",
      "“But, surely, it was obvious. I was then much surprised and interested\n",
      "on glancing down to observe that, though the boots which she was\n",
      "wearing were not unlike each other, they were really odd ones; the one\n",
      "having a slightly decorated toe-cap, and the other a plain one. One was\n",
      "buttoned only in the two lower buttons out of five, and the other at\n",
      "the first, third, and fifth. Now, when you see that a young lady,\n",
      "otherwise neatly dressed, has come away from home with odd boots,\n",
      "half-buttoned, it is no great deduction to say that she came away in a\n",
      "hurry.”\n",
      "\n",
      "“And what else?” I asked, keenly interested, as I always was, by my\n",
      "friend’s incisive reasoning.\n",
      "\n",
      "“I noted, in passing, that she had written a note before leaving home\n",
      "but after being fully dressed. You observed that her right glove was\n",
      "torn at the forefinger, but you did not apparently see that both glove\n",
      "and finger were stained with violet ink. She had written in a hurry and\n",
      "dipped her pen too deep. It must have been this morning, or the mark\n",
      "would not remain clear upon the finger. All this is amusing, though\n",
      "rather elementary, but I must go back to business, Watson. Would you\n",
      "mind reading me the advertised description of Mr. Hosmer Angel?”\n",
      "\n",
      "I held the little printed slip to the light. “Missing,” it said, “on\n",
      "the morning of the fourteenth, a gentleman named Hosmer Angel. About\n",
      "five ft. seven in. in height; strongly built, sallow complexion, black\n",
      "hair, a little bald in the centre, bushy, black side-whiskers and\n",
      "moustache; tinted glasses, slight infirmity of speech. Was dressed,\n",
      "when last seen, in black frock-coat faced with silk, black waistcoat,\n",
      "gold Albert chain, and grey Harris tweed trousers, with brown gaiters\n",
      "over elastic-sided boots. Known to have been employed in an office in\n",
      "Leadenhall Street. Anybody bringing,” &c, &c.\n",
      "\n",
      "“That will do,” said Holmes. “As to the letters,” he continued,\n",
      "glancing over them, “they are very commonplace. Absolutely no clue in\n",
      "them to Mr. Angel, save that he quotes Balzac once. There is one\n",
      "remarkable point, however, which will no doubt strike you.”\n",
      "\n",
      "“They are typewritten,” I remarked.\n",
      "\n",
      "“Not only that, but the signature is typewritten. Look at the neat\n",
      "little ‘Hosmer Angel’ at the bottom. There is a date, you see, but no\n",
      "superscription except Leadenhall Street, which is rather vague. The\n",
      "point about the signature is very suggestive—in fact, we may call it\n",
      "conclusive.”\n",
      "\n",
      "“Of what?”\n",
      "\n",
      "“My dear fellow, is it possible you do not see how strongly it bears\n",
      "upon the case?”\n",
      "\n",
      "“I cannot say that I do unless it were that he wished to be able to\n",
      "deny his signature if an action for breach of promise were instituted.”\n",
      "\n",
      "“No, that was not the point. However, I shall write two letters, which\n",
      "should settle the matter. One is to a firm in the City, the other is to\n",
      "the young lady’s stepfather, Mr. Windibank, asking him whether he could\n",
      "meet us here at six o’clock to-morrow evening. It is just as well that\n",
      "we should do business with the male relatives. And now, Doctor, we can\n",
      "do nothing until the answers to those letters come, so we may put our\n",
      "little problem upon the shelf for the interim.”\n",
      "\n",
      "I had had so many reasons to believe in my friend’s subtle powers of\n",
      "reasoning and extraordinary energy in action that I felt that he must\n",
      "have some solid grounds for the assured and easy demeanour with which\n",
      "he treated the singular mystery which he had been called upon to\n",
      "fathom. Once only had I known him to fail, in the case of the King of\n",
      "Bohemia and of the Irene Adler photograph; but when I looked back to\n",
      "the weird business of the Sign of Four, and the extraordinary\n",
      "circumstances connected with the Study in Scarlet, I felt that it would\n",
      "be a strange tangle indeed which he could not unravel.\n",
      "\n",
      "I left him then, still puffing at his black clay pipe, with the\n",
      "conviction that when I came again on the next evening I would find that\n",
      "he held in his hands all the clues which would lead up to the identity\n",
      "of the disappearing bridegroom of Miss Mary Sutherland.\n",
      "\n",
      "A professional case of great gravity was engaging my own attention at\n",
      "the time, and the whole of next day I was busy at the bedside of the\n",
      "sufferer. It was not until close upon six o’clock that I found myself\n",
      "free and was able to spring into a hansom and drive to Baker Street,\n",
      "half afraid that I might be too late to assist at the _dénouement_ of\n",
      "the little mystery. I found Sherlock Holmes alone, however, half\n",
      "asleep, with his long, thin form curled up in the recesses of his\n",
      "armchair. A formidable array of bottles and test-tubes, with the\n",
      "pungent cleanly smell of hydrochloric acid, told me that he had spent\n",
      "his day in the chemical work which was so dear to him.\n",
      "\n",
      "“Well, have you solved it?” I asked as I entered.\n",
      "\n",
      "“Yes. It was the bisulphate of baryta.”\n",
      "\n",
      "“No, no, the mystery!” I cried.\n",
      "\n",
      "“Oh, that! I thought of the salt that I have been working upon. There\n",
      "was never any mystery in the matter, though, as I said yesterday, some\n",
      "of the details are of interest. The only drawback is that there is no\n",
      "law, I fear, that can touch the scoundrel.”\n",
      "\n",
      "“Who was he, then, and what was his object in deserting Miss\n",
      "Sutherland?”\n",
      "\n",
      "The question was hardly out of my mouth, and Holmes had not yet opened\n",
      "his lips to reply, when we heard a heavy footfall in the passage and a\n",
      "tap at the door.\n",
      "\n",
      "“This is the girl’s stepfather, Mr. James Windibank,” said Holmes. “He\n",
      "has written to me to say that he would be here at six. Come in!”\n",
      "\n",
      "The man who entered was a sturdy, middle-sized fellow, some thirty\n",
      "years of age, clean-shaven, and sallow-skinned, with a bland,\n",
      "insinuating manner, and a pair of wonderfully sharp and penetrating\n",
      "grey eyes. He shot a questioning glance at each of us, placed his shiny\n",
      "top-hat upon the sideboard, and with a slight bow sidled down into the\n",
      "nearest chair.\n",
      "\n",
      "“Good-evening, Mr. James Windibank,” said Holmes. “I think that this\n",
      "typewritten letter is from you, in which you made an appointment with\n",
      "me for six o’clock?”\n",
      "\n",
      "“Yes, sir. I am afraid that I am a little late, but I am not quite my\n",
      "own master, you know. I am sorry that Miss Sutherland has troubled you\n",
      "about this little matter, for I think it is far better not to wash\n",
      "linen of the sort in public. It was quite against my wishes that she\n",
      "came, but she is a very excitable, impulsive girl, as you may have\n",
      "noticed, and she is not easily controlled when she has made up her mind\n",
      "on a point. Of course, I did not mind you so much, as you are not\n",
      "connected with the official police, but it is not pleasant to have a\n",
      "family misfortune like this noised abroad. Besides, it is a useless\n",
      "expense, for how could you possibly find this Hosmer Angel?”\n",
      "\n",
      "“On the contrary,” said Holmes quietly; “I have every reason to believe\n",
      "that I will succeed in discovering Mr. Hosmer Angel.”\n",
      "\n",
      "Mr. Windibank gave a violent start and dropped his gloves. “I am\n",
      "delighted to hear it,” he said.\n",
      "\n",
      "“It is a curious thing,” remarked Holmes, “that a typewriter has really\n",
      "quite as much individuality as a man’s handwriting. Unless they are\n",
      "quite new, no two of them write exactly alike. Some letters get more\n",
      "worn than others, and some wear only on one side. Now, you remark in\n",
      "this note of yours, Mr. Windibank, that in every case there is some\n",
      "little slurring over of the ‘e,’ and a slight defect in the tail of the\n",
      "‘r.’ There are fourteen other characteristics, but those are the more\n",
      "obvious.”\n",
      "\n",
      "“We do all our correspondence with this machine at the office, and no\n",
      "doubt it is a little worn,” our visitor answered, glancing keenly at\n",
      "Holmes with his bright little eyes.\n",
      "\n",
      "“And now I will show you what is really a very interesting study, Mr.\n",
      "Windibank,” Holmes continued. “I think of writing another little\n",
      "monograph some of these days on the typewriter and its relation to\n",
      "crime. It is a subject to which I have devoted some little attention. I\n",
      "have here four letters which purport to come from the missing man. They\n",
      "are all typewritten. In each case, not only are the ‘e’s’ slurred and\n",
      "the ‘r’s’ tailless, but you will observe, if you care to use my\n",
      "magnifying lens, that the fourteen other characteristics to which I\n",
      "have alluded are there as well.”\n",
      "\n",
      "Mr. Windibank sprang out of his chair and picked up his hat. “I cannot\n",
      "waste time over this sort of fantastic talk, Mr. Holmes,” he said. “If\n",
      "you can catch the man, catch him, and let me know when you have done\n",
      "it.”\n",
      "\n",
      "“Certainly,” said Holmes, stepping over and turning the key in the\n",
      "door. “I let you know, then, that I have caught him!”\n",
      "\n",
      "“What! where?” shouted Mr. Windibank, turning white to his lips and\n",
      "glancing about him like a rat in a trap.\n",
      "\n",
      "“Oh, it won’t do—really it won’t,” said Holmes suavely. “There is no\n",
      "possible getting out of it, Mr. Windibank. It is quite too transparent,\n",
      "and it was a very bad compliment when you said that it was impossible\n",
      "for me to solve so simple a question. That’s right! Sit down and let us\n",
      "talk it over.”\n",
      "\n",
      "Our visitor collapsed into a chair, with a ghastly face and a glitter\n",
      "of moisture on his brow. “It—it’s not actionable,” he stammered.\n",
      "\n",
      "“I am very much afraid that it is not. But between ourselves,\n",
      "Windibank, it was as cruel and selfish and heartless a trick in a petty\n",
      "way as ever came before me. Now, let me just run over the course of\n",
      "events, and you will contradict me if I go wrong.”\n",
      "\n",
      "The man sat huddled up in his chair, with his head sunk upon his\n",
      "breast, like one who is utterly crushed. Holmes stuck his feet up on\n",
      "the corner of the mantelpiece and, leaning back with his hands in his\n",
      "pockets, began talking, rather to himself, as it seemed, than to us.\n",
      "\n",
      "“The man married a woman very much older than himself for her money,”\n",
      "said he, “and he enjoyed the use of the money of the daughter as long\n",
      "as she lived with them. It was a considerable sum, for people in their\n",
      "position, and the loss of it would have made a serious difference. It\n",
      "was worth an effort to preserve it. The daughter was of a good, amiable\n",
      "disposition, but affectionate and warm-hearted in her ways, so that it\n",
      "was evident that with her fair personal advantages, and her little\n",
      "income, she would not be allowed to remain single long. Now her\n",
      "marriage would mean, of course, the loss of a hundred a year, so what\n",
      "does her stepfather do to prevent it? He takes the obvious course of\n",
      "keeping her at home and forbidding her to seek the company of people of\n",
      "her own age. But soon he found that that would not answer forever. She\n",
      "became restive, insisted upon her rights, and finally announced her\n",
      "positive intention of going to a certain ball. What does her clever\n",
      "stepfather do then? He conceives an idea more creditable to his head\n",
      "than to his heart. With the connivance and assistance of his wife he\n",
      "disguised himself, covered those keen eyes with tinted glasses, masked\n",
      "the face with a moustache and a pair of bushy whiskers, sunk that clear\n",
      "voice into an insinuating whisper, and doubly secure on account of the\n",
      "girl’s short sight, he appears as Mr. Hosmer Angel, and keeps off other\n",
      "lovers by making love himself.”\n",
      "\n",
      "“It was only a joke at first,” groaned our visitor. “We never thought\n",
      "that she would have been so carried away.”\n",
      "\n",
      "“Very likely not. However that may be, the young lady was very\n",
      "decidedly carried away, and, having quite made up her mind that her\n",
      "stepfather was in France, the suspicion of treachery never for an\n",
      "instant entered her mind. She was flattered by the gentleman’s\n",
      "attentions, and the effect was increased by the loudly expressed\n",
      "admiration of her mother. Then Mr. Angel began to call, for it was\n",
      "obvious that the matter should be pushed as far as it would go if a\n",
      "real effect were to be produced. There were meetings, and an\n",
      "engagement, which would finally secure the girl’s affections from\n",
      "turning towards anyone else. But the deception could not be kept up\n",
      "forever. These pretended journeys to France were rather cumbrous. The\n",
      "thing to do was clearly to bring the business to an end in such a\n",
      "dramatic manner that it would leave a permanent impression upon the\n",
      "young lady’s mind and prevent her from looking upon any other suitor\n",
      "for some time to come. Hence those vows of fidelity exacted upon a\n",
      "Testament, and hence also the allusions to a possibility of something\n",
      "happening on the very morning of the wedding. James Windibank wished\n",
      "Miss Sutherland to be so bound to Hosmer Angel, and so uncertain as to\n",
      "his fate, that for ten years to come, at any rate, she would not listen\n",
      "to another man. As far as the church door he brought her, and then, as\n",
      "he could go no farther, he conveniently vanished away by the old trick\n",
      "of stepping in at one door of a four-wheeler and out at the other. I\n",
      "think that was the chain of events, Mr. Windibank!”\n",
      "\n",
      "Our visitor had recovered something of his assurance while Holmes had\n",
      "been talking, and he rose from his chair now with a cold sneer upon his\n",
      "pale face.\n",
      "\n",
      "“It may be so, or it may not, Mr. Holmes,” said he, “but if you are so\n",
      "very sharp you ought to be sharp enough to know that it is you who are\n",
      "breaking the law now, and not me. I have done nothing actionable from\n",
      "the first, but as long as you keep that door locked you lay yourself\n",
      "open to an action for assault and illegal constraint.”\n",
      "\n",
      "“The law cannot, as you say, touch you,” said Holmes, unlocking and\n",
      "throwing open the door, “yet there never was a man who deserved\n",
      "punishment more. If the young lady has a brother or a friend, he ought\n",
      "to lay a whip across your shoulders. By Jove!” he continued, flushing\n",
      "up at the sight of the bitter sneer upon the man’s face, “it is not\n",
      "part of my duties to my client, but here’s a hunting crop handy, and I\n",
      "think I shall just treat myself to—” He took two swift steps to the\n",
      "whip, but before he could grasp it there was a wild clatter of steps\n",
      "upon the stairs, the heavy hall door banged, and from the window we\n",
      "could see Mr. James Windibank running at the top of his speed down the\n",
      "road.\n",
      "\n",
      "“There’s a cold-blooded scoundrel!” said Holmes, laughing, as he threw\n",
      "himself down into his chair once more. “That fellow will rise from\n",
      "crime to crime until he does something very bad, and ends on a gallows.\n",
      "The case has, in some respects, been not entirely devoid of interest.”\n",
      "\n",
      "“I cannot now entirely see all the steps of your reasoning,” I\n",
      "remarked.\n",
      "\n",
      "“Well, of course it was obvious from the first that this Mr. Hosmer\n",
      "Angel must have some strong object for his curious conduct, and it was\n",
      "equally clear that the only man who really profited by the incident, as\n",
      "far as we could see, was the stepfather. Then the fact that the two men\n",
      "were never together, but that the one always appeared when the other\n",
      "was away, was suggestive. So were the tinted spectacles and the curious\n",
      "voice, which both hinted at a disguise, as did the bushy whiskers. My\n",
      "suspicions were all confirmed by his peculiar action in typewriting his\n",
      "signature, which, of course, inferred that his handwriting was so\n",
      "familiar to her that she would recognise even the smallest sample of\n",
      "it. You see all these isolated facts, together with many minor ones,\n",
      "all pointed in the same direction.”\n",
      "\n",
      "“And how did you verify them?”\n",
      "\n",
      "“Having once spotted my man, it was easy to get corroboration. I knew\n",
      "the firm for which this man worked. Having taken the printed\n",
      "description. I eliminated everything from it which could be the result\n",
      "of a disguise—the whiskers, the glasses, the voice, and I sent it to\n",
      "the firm, with a request that they would inform me whether it answered\n",
      "to the description of any of their travellers. I had already noticed\n",
      "the peculiarities of the typewriter, and I wrote to the man himself at\n",
      "his business address asking him if he would come here. As I expected,\n",
      "his reply was typewritten and revealed the same trivial but\n",
      "characteristic defects. The same post brought me a letter from\n",
      "Westhouse & Marbank, of Fenchurch Street, to say that the description\n",
      "tallied in every respect with that of their employé, James Windibank.\n",
      "_Voilà tout_!”\n",
      "\n",
      "“And Miss Sutherland?”\n",
      "\n",
      "“If I tell her she will not believe me. You may remember the old\n",
      "Persian saying, ‘There is danger for him who taketh the tiger cub, and\n",
      "danger also for whoso snatches a delusion from a woman.’ There is as\n",
      "much sense in Hafiz as in Horace, and as much knowledge of the world.”\n"
     ]
    }
   ],
   "source": [
    "# Pass in the first four chapters of Moby Dick to the read_file function.\n",
    "holmes_text = read_file('Resources/A_Case_Of_Identity.txt')\n",
    "print(holmes_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and tokenize the text using the separate_punc function.\n",
    "tokens = separate_punc(holmes_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7106"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the length of the tokens list.\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dear', 'fellow', 'said', 'sherlock', 'holmes', 'as', 'we', 'sat', 'on', 'either', 'side', 'of', 'the', 'fire', 'in', 'his', 'lodgings', 'at', 'baker', 'street', 'life', 'is', 'infinitely', 'stranger', 'than', 'anything', 'which', 'the', 'mind', 'of', 'man', 'could', 'invent', 'we', 'would', 'not', 'dare', 'to', 'conceive', 'the', 'things', 'which', 'are', 'really', 'mere', 'commonplaces', 'of', 'existence', 'if', 'we', 'could', 'fly', 'out', 'of', 'that', 'window', 'hand', 'in', 'hand', 'hover', 'over', 'this', 'great', 'city', 'gently', 'remove', 'the', 'roofs', 'and', 'peep', 'in', 'at', 'the', 'queer', 'things', 'which', 'are', 'going', 'on', 'the', 'strange', 'coincidences', 'the', 'plannings', 'the', 'cross', 'purposes', 'the', 'wonderful', 'chains', 'of', 'events', 'working', 'through', 'generations', 'and', 'leading', 'to', 'the', 'most', 'outré', 'results', 'it', 'would', 'make', 'all', 'fiction', 'with', 'its', 'conventionalities', 'and', 'foreseen', 'conclusions', 'most', 'stale', 'and', 'unprofitable', 'and', 'yet', 'i', 'am', 'not', 'convinced', 'of', 'it', 'i', 'answered', 'the', 'cases', 'which', 'come', 'to', 'light', 'in', 'the', 'papers', 'are', 'as', 'a', 'rule', 'bald', 'enough', 'and', 'vulgar', 'enough', 'we', 'have', 'in', 'our', 'police', 'reports', 'realism', 'pushed', 'to', 'its', 'extreme', 'limits', 'and', 'yet', 'the', 'result', 'is', 'it', 'must', 'be', 'confessed', 'neither', 'fascinating', 'nor', 'artistic', 'a', 'certain', 'selection', 'and', 'discretion', 'must', 'be', 'used', 'in', 'producing', 'a', 'realistic', 'effect', 'remarked', 'holmes', 'this', 'is', 'wanting', 'in', 'the', 'police', 'report', 'where', 'more', 'stress', 'is', 'laid', 'perhaps', 'upon', 'the', 'platitudes', 'of', 'the', 'magistrate', 'than', 'upon', 'the', 'details', 'which', 'to', 'an', 'observer', 'contain', 'the', 'vital', 'essence', 'of', 'the', 'whole', 'matter', 'depend', 'upon', 'it', 'there', 'is', 'nothing', 'so', 'unnatural', 'as', 'the', 'commonplace', 'i', 'smiled', 'and', 'shook', 'my', 'head', 'i', 'can', 'quite', 'understand', 'your', 'thinking', 'so', 'i', 'said', 'of', 'course', 'in', 'your', 'position', 'of', 'unofficial', 'adviser', 'and', 'helper', 'to', 'everybody', 'who', 'is', 'absolutely', 'puzzled', 'throughout', 'three', 'continents', 'you', 'are', 'brought', 'in', 'contact', 'with', 'all', 'that', 'is', 'strange', 'and', 'bizarre', 'but', 'here”—i', 'picked', 'up', 'the', 'morning', 'paper', 'from', 'the', 'ground—“let', 'us', 'put', 'it', 'to', 'a', 'practical', 'test', 'here', 'is', 'the', 'first', 'heading']\n"
     ]
    }
   ],
   "source": [
    "# Look over the tokems to make sure all theo punctuation has been reomved.  \n",
    "# Some punctuation wasn't removed\n",
    "print(tokens[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lid', 'its', 'splendour', 'was', 'in', 'such', 'contrast', 'to', 'his', 'homely', 'ways', 'and', 'simple', 'life', 'that', 'i', 'could', 'not', 'help', 'commenting', 'upon', 'it', 'ah', 'said', 'he', 'i', 'forgot', 'that', 'i', 'had', 'not', 'seen', 'you', 'for', 'some', 'weeks', 'it', 'is', 'a', 'little', 'souvenir', 'from', 'the', 'king', 'of', 'bohemia', 'in', 'return', 'for', 'my', 'assistance', 'in', 'the', 'case', 'of', 'the', 'irene', 'adler', 'papers', 'and', 'the', 'ring', 'i', 'asked', 'glancing', 'at', 'a', 'remarkable', 'brilliant', 'which', 'sparkled', 'upon', 'his', 'finger', 'it', 'was', 'from', 'the', 'reigning', 'family', 'of', 'holland', 'though', 'the', 'matter', 'in', 'which', 'i', 'served', 'them', 'was', 'of', 'such', 'delicacy', 'that', 'i', 'can', 'not', 'confide', 'it', 'even', 'to', 'you', 'who', 'have', 'been', 'good', 'enough', 'to', 'chronicle', 'one', 'or', 'two', 'of', 'my', 'little', 'problems', 'and', 'have', 'you', 'any', 'on', 'hand', 'just', 'now', 'i', 'asked', 'with', 'interest', 'some', 'ten', 'or', 'twelve', 'but', 'none', 'which', 'present', 'any', 'feature', 'of', 'interest', 'they', 'are', 'important', 'you', 'understand', 'without', 'being', 'interesting', 'indeed', 'i', 'have', 'found', 'that', 'it', 'is', 'usually', 'in', 'unimportant', 'matters', 'that', 'there', 'is', 'a', 'field', 'for', 'the', 'observation', 'and', 'for', 'the', 'quick', 'analysis', 'of', 'cause', 'and', 'effect', 'which', 'gives', 'the', 'charm', 'to', 'an', 'investigation', 'the', 'larger', 'crimes', 'are', 'apt', 'to', 'be', 'the', 'simpler', 'for', 'the', 'bigger', 'the', 'crime', 'the', 'more', 'obvious', 'as', 'a', 'rule', 'is', 'the', 'motive', 'in', 'these', 'cases', 'save', 'for', 'one', 'rather', 'intricate', 'matter', 'which', 'has', 'been', 'referred', 'to', 'me', 'from', 'marseilles', 'there', 'is', 'nothing', 'which', 'presents', 'any', 'features', 'of', 'interest', 'it', 'is', 'possible', 'however', 'that', 'i', 'may', 'have', 'something', 'better', 'before', 'very', 'many', 'minutes', 'are', 'over', 'for', 'this', 'is', 'one', 'of', 'my', 'clients', 'or', 'i', 'am', 'much', 'mistaken', 'he', 'had', 'risen', 'from', 'his', 'chair', 'and', 'was', 'standing', 'between', 'the', 'parted', 'blinds', 'gazing', 'down', 'into', 'the', 'dull', 'neutral', 'tinted', 'london', 'street', 'looking', 'over', 'his', 'shoulder', 'i', 'saw', 'that', 'on', 'the', 'pavement', 'opposite', 'there', 'stood', 'a', 'large', 'woman', 'with']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[500:800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sequences of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize into sequences of tokens.\n",
    "# Use 25 words to predict the 26th word using \"+1\".\n",
    "train_len = 26 # = 25 training words plus one target word.\n",
    "\n",
    "# Create an empty list of sequences\n",
    "text_sequences = []\n",
    "\n",
    "# Use a for loop to create lists of 26 tokens for the whole text. \n",
    "for i in range(train_len, len(tokens)):\n",
    "    seq = tokens[i-train_len:i]\n",
    "    \n",
    "    # Add to list of sequences\n",
    "    text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7080"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The list of text_sequences should be 26 less than the total tokens.\n",
    "len(text_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Tokenization with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 12:44:57.689476: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import the Keras tokenization to format the data from words into a numerical format.\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Keras Tokenizer class \n",
    "tokenizer = Tokenizer()\n",
    "# Map each word with an index.\n",
    "tokenizer.fit_on_texts(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1596\n"
     ]
    }
   ],
   "source": [
    "# What is the size of the vocabulary\n",
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode each word in the text_sequences to the indices. \n",
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the List of Sequences to Arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy to convert the list of sequences to arrays.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7080"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the all 26 word list of lists to arrays.\n",
    "num_sequences = np.array(sequences)\n",
    "# Get the length of the array\n",
    "len(num_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input sequences and one-hot encode the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the to_categorical function to convert the arrays to binary values.\n",
    "import keras\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input variable, `X`, to the first 25 numbers of each array.\n",
    "X = num_sequences[:,:-1]\n",
    "# Set target variable, `y`, to the last number of each array.\n",
    "y = num_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7080, 25)\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "# Get the shape of X\n",
    "print(X.shape)\n",
    "# Get the length of each sequence.\n",
    "seq_len = X.shape[1]\n",
    "print(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7080,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the shape of y\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next,one-hot encode the target variable to get transform each index to a binary value. \n",
    "# We increase the vocabulary by 1 so we can predict the next word. \n",
    "y = to_categorical(y, num_classes=vocabulary_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7080, 1597)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the shape of y again.\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a LSTM  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies for LSTM model.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size, seq_len):\n",
    "    \"\"\"\n",
    "    Create and compile an LSTM-based sequential model for text generation.\n",
    "\n",
    "    Parameters:\n",
    "    - vocabulary_size (int): The size of the vocabulary, i.e., the number of unique words in the text.\n",
    "    - seq_len (int): The length of the input sequences, indicating the number of words in each sequence.\n",
    "\n",
    "    Returns:\n",
    "    - model (Sequential): A compiled Keras sequential model for text generation.\n",
    "\n",
    "    Model Architecture:\n",
    "    - Embedding Layer: Maps word indices to dense vectors.\n",
    "    - LSTM Layer 1: 150 units, returns full sequence.\n",
    "    - LSTM Layer 2: 150 units.\n",
    "    - Dense Layer: 150 units, ReLU activation.\n",
    "    - Output Layer: Dense layer with softmax activation for multi-class classification.\n",
    "\n",
    "    Compilation:\n",
    "    - Loss: Categorical crossentropy.\n",
    "    - Optimizer: Adam.\n",
    "    - Metrics: Accuracy.\n",
    "\n",
    "    Usage Example:\n",
    "    ```python\n",
    "    model = create_model(vocabulary_size=10000, seq_len=25)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, 25, input_length=seq_len))\n",
    "    model.add(LSTM(150, return_sequences=True))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dense(150, activation='relu'))\n",
    "\n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "   \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 25, 25)            39925     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 25, 150)           105600    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 150)               180600    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 150)               22650     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1597)              241147    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 589922 (2.25 MB)\n",
      "Trainable params: 589922 (2.25 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model and pass in the vocabulary (+1) and the seq_len (25 words).\n",
    "model = create_model(vocabulary_size+1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "56/56 [==============================] - 14s 185ms/step - loss: 6.5543 - accuracy: 0.0386\n",
      "Epoch 2/300\n",
      "56/56 [==============================] - 13s 239ms/step - loss: 6.0728 - accuracy: 0.0496\n",
      "Epoch 3/300\n",
      "56/56 [==============================] - 13s 227ms/step - loss: 6.0441 - accuracy: 0.0496\n",
      "Epoch 4/300\n",
      "56/56 [==============================] - 9s 169ms/step - loss: 6.0286 - accuracy: 0.0496\n",
      "Epoch 5/300\n",
      "56/56 [==============================] - 9s 167ms/step - loss: 5.9858 - accuracy: 0.0496\n",
      "Epoch 6/300\n",
      "56/56 [==============================] - 10s 179ms/step - loss: 5.8453 - accuracy: 0.0496\n",
      "Epoch 7/300\n",
      "56/56 [==============================] - 10s 185ms/step - loss: 5.7671 - accuracy: 0.0501\n",
      "Epoch 8/300\n",
      "56/56 [==============================] - 11s 195ms/step - loss: 5.6658 - accuracy: 0.0564\n",
      "Epoch 9/300\n",
      "56/56 [==============================] - 10s 178ms/step - loss: 5.5514 - accuracy: 0.0603\n",
      "Epoch 10/300\n",
      "56/56 [==============================] - 9s 168ms/step - loss: 5.4664 - accuracy: 0.0606\n",
      "Epoch 11/300\n",
      "56/56 [==============================] - 9s 155ms/step - loss: 5.3997 - accuracy: 0.0636\n",
      "Epoch 12/300\n",
      "56/56 [==============================] - 10s 170ms/step - loss: 5.3379 - accuracy: 0.0664\n",
      "Epoch 13/300\n",
      "56/56 [==============================] - 10s 170ms/step - loss: 5.2709 - accuracy: 0.0689\n",
      "Epoch 14/300\n",
      "56/56 [==============================] - 10s 171ms/step - loss: 5.1969 - accuracy: 0.0702\n",
      "Epoch 15/300\n",
      "56/56 [==============================] - 11s 187ms/step - loss: 5.1329 - accuracy: 0.0712\n",
      "Epoch 16/300\n",
      "56/56 [==============================] - 9s 165ms/step - loss: 5.0783 - accuracy: 0.0747\n",
      "Epoch 17/300\n",
      "56/56 [==============================] - 9s 164ms/step - loss: 5.0318 - accuracy: 0.0753\n",
      "Epoch 18/300\n",
      "56/56 [==============================] - 9s 167ms/step - loss: 4.9826 - accuracy: 0.0763\n",
      "Epoch 19/300\n",
      "56/56 [==============================] - 9s 161ms/step - loss: 4.9438 - accuracy: 0.0797\n",
      "Epoch 20/300\n",
      "56/56 [==============================] - 9s 163ms/step - loss: 4.8978 - accuracy: 0.0801\n",
      "Epoch 21/300\n",
      "56/56 [==============================] - 9s 159ms/step - loss: 4.8556 - accuracy: 0.0812\n",
      "Epoch 22/300\n",
      "56/56 [==============================] - 9s 160ms/step - loss: 4.8218 - accuracy: 0.0845\n",
      "Epoch 23/300\n",
      "56/56 [==============================] - 9s 166ms/step - loss: 4.7860 - accuracy: 0.0839\n",
      "Epoch 24/300\n",
      "56/56 [==============================] - 9s 165ms/step - loss: 4.7394 - accuracy: 0.0860\n",
      "Epoch 25/300\n",
      "56/56 [==============================] - 9s 162ms/step - loss: 4.7037 - accuracy: 0.0849\n",
      "Epoch 26/300\n",
      "56/56 [==============================] - 9s 159ms/step - loss: 4.6625 - accuracy: 0.0871\n",
      "Epoch 27/300\n",
      "56/56 [==============================] - 9s 167ms/step - loss: 4.6297 - accuracy: 0.0871\n",
      "Epoch 28/300\n",
      "56/56 [==============================] - 9s 158ms/step - loss: 4.5890 - accuracy: 0.0883\n",
      "Epoch 29/300\n",
      "56/56 [==============================] - 9s 160ms/step - loss: 4.5472 - accuracy: 0.0917\n",
      "Epoch 30/300\n",
      "56/56 [==============================] - 9s 158ms/step - loss: 4.5104 - accuracy: 0.0962\n",
      "Epoch 31/300\n",
      "56/56 [==============================] - 10s 177ms/step - loss: 4.4752 - accuracy: 0.0969\n",
      "Epoch 32/300\n",
      "56/56 [==============================] - 9s 163ms/step - loss: 4.4393 - accuracy: 0.0982\n",
      "Epoch 33/300\n",
      "56/56 [==============================] - 9s 169ms/step - loss: 4.4025 - accuracy: 0.1027\n",
      "Epoch 34/300\n",
      "56/56 [==============================] - 9s 166ms/step - loss: 4.3691 - accuracy: 0.1011\n",
      "Epoch 35/300\n",
      "56/56 [==============================] - 9s 161ms/step - loss: 4.3360 - accuracy: 0.1062\n",
      "Epoch 36/300\n",
      "56/56 [==============================] - 9s 157ms/step - loss: 4.3091 - accuracy: 0.1049\n",
      "Epoch 37/300\n",
      "56/56 [==============================] - 9s 157ms/step - loss: 4.2790 - accuracy: 0.1107\n",
      "Epoch 38/300\n",
      "56/56 [==============================] - 9s 166ms/step - loss: 4.2424 - accuracy: 0.1124\n",
      "Epoch 39/300\n",
      "56/56 [==============================] - 9s 156ms/step - loss: 4.2179 - accuracy: 0.1113\n",
      "Epoch 40/300\n",
      "56/56 [==============================] - 9s 160ms/step - loss: 4.1848 - accuracy: 0.1151\n",
      "Epoch 41/300\n",
      "56/56 [==============================] - 9s 165ms/step - loss: 4.1571 - accuracy: 0.1116\n",
      "Epoch 42/300\n",
      "56/56 [==============================] - 9s 157ms/step - loss: 4.1301 - accuracy: 0.1199\n",
      "Epoch 43/300\n",
      "56/56 [==============================] - 9s 163ms/step - loss: 4.0954 - accuracy: 0.1195\n",
      "Epoch 44/300\n",
      "56/56 [==============================] - 9s 162ms/step - loss: 4.0621 - accuracy: 0.1233\n",
      "Epoch 45/300\n",
      "56/56 [==============================] - 9s 161ms/step - loss: 4.0354 - accuracy: 0.1260\n",
      "Epoch 46/300\n",
      "56/56 [==============================] - 9s 158ms/step - loss: 4.0080 - accuracy: 0.1242\n",
      "Epoch 47/300\n",
      "56/56 [==============================] - 9s 155ms/step - loss: 3.9798 - accuracy: 0.1257\n",
      "Epoch 48/300\n",
      "56/56 [==============================] - 9s 160ms/step - loss: 3.9490 - accuracy: 0.1298\n",
      "Epoch 49/300\n",
      "56/56 [==============================] - 9s 164ms/step - loss: 3.9154 - accuracy: 0.1302\n",
      "Epoch 50/300\n",
      "56/56 [==============================] - 9s 157ms/step - loss: 3.9730 - accuracy: 0.1288\n",
      "Epoch 51/300\n",
      "56/56 [==============================] - 9s 168ms/step - loss: 3.9404 - accuracy: 0.1297\n",
      "Epoch 52/300\n",
      "56/56 [==============================] - 9s 164ms/step - loss: 3.8535 - accuracy: 0.1370\n",
      "Epoch 53/300\n",
      "56/56 [==============================] - 10s 177ms/step - loss: 3.8149 - accuracy: 0.1387\n",
      "Epoch 54/300\n",
      "56/56 [==============================] - 9s 168ms/step - loss: 3.7943 - accuracy: 0.1408\n",
      "Epoch 55/300\n",
      "56/56 [==============================] - 10s 172ms/step - loss: 3.7629 - accuracy: 0.1429\n",
      "Epoch 56/300\n",
      "56/56 [==============================] - 10s 175ms/step - loss: 3.7297 - accuracy: 0.1449\n",
      "Epoch 57/300\n",
      "56/56 [==============================] - 9s 163ms/step - loss: 3.6993 - accuracy: 0.1520\n",
      "Epoch 58/300\n",
      "56/56 [==============================] - 9s 165ms/step - loss: 3.6732 - accuracy: 0.1525\n",
      "Epoch 59/300\n",
      "56/56 [==============================] - 10s 173ms/step - loss: 3.6451 - accuracy: 0.1477\n",
      "Epoch 60/300\n",
      "56/56 [==============================] - 9s 160ms/step - loss: 3.6154 - accuracy: 0.1620\n",
      "Epoch 61/300\n",
      "56/56 [==============================] - 9s 162ms/step - loss: 3.5864 - accuracy: 0.1600\n",
      "Epoch 62/300\n",
      "56/56 [==============================] - 9s 169ms/step - loss: 3.5675 - accuracy: 0.1637\n",
      "Epoch 63/300\n",
      "56/56 [==============================] - 10s 171ms/step - loss: 3.5373 - accuracy: 0.1630\n",
      "Epoch 64/300\n",
      "56/56 [==============================] - 10s 185ms/step - loss: 3.5091 - accuracy: 0.1672\n",
      "Epoch 65/300\n",
      "56/56 [==============================] - 9s 167ms/step - loss: 3.4769 - accuracy: 0.1715\n",
      "Epoch 66/300\n",
      "56/56 [==============================] - 9s 166ms/step - loss: 3.4509 - accuracy: 0.1774\n",
      "Epoch 67/300\n",
      "56/56 [==============================] - 9s 162ms/step - loss: 3.4242 - accuracy: 0.1852\n",
      "Epoch 68/300\n",
      "56/56 [==============================] - 10s 174ms/step - loss: 3.4014 - accuracy: 0.1832\n",
      "Epoch 69/300\n",
      "56/56 [==============================] - 10s 173ms/step - loss: 3.3816 - accuracy: 0.1884\n",
      "Epoch 70/300\n",
      "56/56 [==============================] - 10s 183ms/step - loss: 3.3457 - accuracy: 0.1903\n",
      "Epoch 71/300\n",
      "56/56 [==============================] - 10s 183ms/step - loss: 3.3225 - accuracy: 0.1924\n",
      "Epoch 72/300\n",
      "56/56 [==============================] - 10s 175ms/step - loss: 3.2965 - accuracy: 0.1993\n",
      "Epoch 73/300\n",
      "56/56 [==============================] - 10s 176ms/step - loss: 3.2647 - accuracy: 0.2040\n",
      "Epoch 74/300\n",
      "56/56 [==============================] - 10s 177ms/step - loss: 3.2411 - accuracy: 0.2082\n",
      "Epoch 75/300\n",
      "56/56 [==============================] - 10s 173ms/step - loss: 3.2178 - accuracy: 0.2117\n",
      "Epoch 76/300\n",
      "56/56 [==============================] - 10s 172ms/step - loss: 3.1921 - accuracy: 0.2160\n",
      "Epoch 77/300\n",
      "56/56 [==============================] - 9s 162ms/step - loss: 3.1615 - accuracy: 0.2201\n",
      "Epoch 78/300\n",
      "56/56 [==============================] - 10s 171ms/step - loss: 3.1365 - accuracy: 0.2264\n",
      "Epoch 79/300\n",
      "56/56 [==============================] - 10s 171ms/step - loss: 3.1137 - accuracy: 0.2282\n",
      "Epoch 80/300\n",
      "56/56 [==============================] - 9s 165ms/step - loss: 3.0840 - accuracy: 0.2352\n",
      "Epoch 81/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 9s 160ms/step - loss: 3.0647 - accuracy: 0.2384\n",
      "Epoch 82/300\n",
      "56/56 [==============================] - 9s 160ms/step - loss: 3.0460 - accuracy: 0.2438\n",
      "Epoch 83/300\n",
      "56/56 [==============================] - 9s 162ms/step - loss: 3.0161 - accuracy: 0.2513\n",
      "Epoch 84/300\n",
      "56/56 [==============================] - 11s 202ms/step - loss: 3.0005 - accuracy: 0.2520\n",
      "Epoch 85/300\n",
      "56/56 [==============================] - 11s 203ms/step - loss: 2.9620 - accuracy: 0.2626\n",
      "Epoch 86/300\n",
      "56/56 [==============================] - 12s 211ms/step - loss: 2.9425 - accuracy: 0.2612\n",
      "Epoch 87/300\n",
      "56/56 [==============================] - 12s 220ms/step - loss: 2.9295 - accuracy: 0.2657\n",
      "Epoch 88/300\n",
      "56/56 [==============================] - 10s 184ms/step - loss: 2.9111 - accuracy: 0.2685\n",
      "Epoch 89/300\n",
      "56/56 [==============================] - 9s 157ms/step - loss: 2.8771 - accuracy: 0.2742\n",
      "Epoch 90/300\n",
      "56/56 [==============================] - 8s 149ms/step - loss: 2.8443 - accuracy: 0.2838\n",
      "Epoch 91/300\n",
      "56/56 [==============================] - 9s 153ms/step - loss: 2.8324 - accuracy: 0.2826\n",
      "Epoch 92/300\n",
      "56/56 [==============================] - 8s 150ms/step - loss: 2.8107 - accuracy: 0.2915\n",
      "Epoch 93/300\n",
      "56/56 [==============================] - 8s 149ms/step - loss: 2.7820 - accuracy: 0.2979\n",
      "Epoch 94/300\n",
      "56/56 [==============================] - 9s 158ms/step - loss: 2.7563 - accuracy: 0.3010\n",
      "Epoch 95/300\n",
      "56/56 [==============================] - 11s 195ms/step - loss: 2.7278 - accuracy: 0.3059\n",
      "Epoch 96/300\n",
      "56/56 [==============================] - 10s 182ms/step - loss: 2.7256 - accuracy: 0.3078\n",
      "Epoch 97/300\n",
      "56/56 [==============================] - 10s 177ms/step - loss: 2.7007 - accuracy: 0.3157\n",
      "Epoch 98/300\n",
      "56/56 [==============================] - 10s 178ms/step - loss: 2.6772 - accuracy: 0.3192\n",
      "Epoch 99/300\n",
      "56/56 [==============================] - 10s 174ms/step - loss: 2.6584 - accuracy: 0.3160\n",
      "Epoch 100/300\n",
      "56/56 [==============================] - 10s 182ms/step - loss: 2.6423 - accuracy: 0.3192\n",
      "Epoch 101/300\n",
      "56/56 [==============================] - 10s 186ms/step - loss: 2.6089 - accuracy: 0.3318\n",
      "Epoch 102/300\n",
      "56/56 [==============================] - 10s 177ms/step - loss: 2.6000 - accuracy: 0.3369\n",
      "Epoch 103/300\n",
      "56/56 [==============================] - 10s 175ms/step - loss: 2.5698 - accuracy: 0.3419\n",
      "Epoch 104/300\n",
      "56/56 [==============================] - 10s 176ms/step - loss: 2.5483 - accuracy: 0.3483\n",
      "Epoch 105/300\n",
      "56/56 [==============================] - 10s 176ms/step - loss: 2.5419 - accuracy: 0.3441\n",
      "Epoch 106/300\n",
      "56/56 [==============================] - 10s 179ms/step - loss: 2.5152 - accuracy: 0.3566\n",
      "Epoch 107/300\n",
      "56/56 [==============================] - 10s 177ms/step - loss: 2.4875 - accuracy: 0.3636\n",
      "Epoch 108/300\n",
      "56/56 [==============================] - 10s 175ms/step - loss: 2.4704 - accuracy: 0.3678\n",
      "Epoch 109/300\n",
      "56/56 [==============================] - 10s 177ms/step - loss: 2.4544 - accuracy: 0.3640\n",
      "Epoch 110/300\n",
      "56/56 [==============================] - 10s 176ms/step - loss: 2.4320 - accuracy: 0.3734\n",
      "Epoch 111/300\n",
      "56/56 [==============================] - 10s 182ms/step - loss: 2.4118 - accuracy: 0.3788\n",
      "Epoch 112/300\n",
      "56/56 [==============================] - 10s 181ms/step - loss: 2.3974 - accuracy: 0.3743\n",
      "Epoch 113/300\n",
      "56/56 [==============================] - 10s 179ms/step - loss: 2.3821 - accuracy: 0.3826\n",
      "Epoch 114/300\n",
      "56/56 [==============================] - 10s 177ms/step - loss: 2.3662 - accuracy: 0.3881\n",
      "Epoch 115/300\n",
      "56/56 [==============================] - 10s 175ms/step - loss: 2.3405 - accuracy: 0.3958\n",
      "Epoch 116/300\n",
      "56/56 [==============================] - 10s 179ms/step - loss: 2.3367 - accuracy: 0.3956\n",
      "Epoch 117/300\n",
      "56/56 [==============================] - 10s 181ms/step - loss: 2.3141 - accuracy: 0.3919\n",
      "Epoch 118/300\n",
      "56/56 [==============================] - 10s 176ms/step - loss: 2.2905 - accuracy: 0.4051\n",
      "Epoch 119/300\n",
      "56/56 [==============================] - 10s 175ms/step - loss: 2.2661 - accuracy: 0.4112\n",
      "Epoch 120/300\n",
      "56/56 [==============================] - 10s 179ms/step - loss: 2.2523 - accuracy: 0.4100\n",
      "Epoch 121/300\n",
      "56/56 [==============================] - 10s 179ms/step - loss: 2.2317 - accuracy: 0.4237\n",
      "Epoch 122/300\n",
      "56/56 [==============================] - 10s 179ms/step - loss: 2.2493 - accuracy: 0.4113\n",
      "Epoch 123/300\n",
      "56/56 [==============================] - 10s 179ms/step - loss: 2.2037 - accuracy: 0.4236\n",
      "Epoch 124/300\n",
      "56/56 [==============================] - 10s 179ms/step - loss: 2.1871 - accuracy: 0.4273\n",
      "Epoch 125/300\n",
      "56/56 [==============================] - 11s 200ms/step - loss: 2.1821 - accuracy: 0.4264\n",
      "Epoch 126/300\n",
      "56/56 [==============================] - 10s 181ms/step - loss: 2.1687 - accuracy: 0.4342\n",
      "Epoch 127/300\n",
      "56/56 [==============================] - 10s 176ms/step - loss: 2.1395 - accuracy: 0.4384\n",
      "Epoch 128/300\n",
      "56/56 [==============================] - 10s 176ms/step - loss: 2.1264 - accuracy: 0.4403\n",
      "Epoch 129/300\n",
      "56/56 [==============================] - 10s 177ms/step - loss: 2.1080 - accuracy: 0.4451\n",
      "Epoch 130/300\n",
      "56/56 [==============================] - 11s 200ms/step - loss: 2.0945 - accuracy: 0.4492\n",
      "Epoch 131/300\n",
      "56/56 [==============================] - 13s 238ms/step - loss: 2.0815 - accuracy: 0.4500\n",
      "Epoch 132/300\n",
      "56/56 [==============================] - 12s 213ms/step - loss: 2.0612 - accuracy: 0.4634\n",
      "Epoch 133/300\n",
      "56/56 [==============================] - 11s 205ms/step - loss: 2.0401 - accuracy: 0.4586\n",
      "Epoch 134/300\n",
      "56/56 [==============================] - 11s 192ms/step - loss: 2.0283 - accuracy: 0.4627\n",
      "Epoch 135/300\n",
      "56/56 [==============================] - 11s 195ms/step - loss: 2.0150 - accuracy: 0.4671\n",
      "Epoch 136/300\n",
      "56/56 [==============================] - 11s 195ms/step - loss: 2.0035 - accuracy: 0.4709\n",
      "Epoch 137/300\n",
      "56/56 [==============================] - 11s 201ms/step - loss: 1.9883 - accuracy: 0.4754\n",
      "Epoch 138/300\n",
      "56/56 [==============================] - 11s 200ms/step - loss: 1.9713 - accuracy: 0.4722\n",
      "Epoch 139/300\n",
      "56/56 [==============================] - 11s 201ms/step - loss: 1.9503 - accuracy: 0.4795\n",
      "Epoch 140/300\n",
      "56/56 [==============================] - 11s 194ms/step - loss: 1.9349 - accuracy: 0.4876\n",
      "Epoch 141/300\n",
      "56/56 [==============================] - 11s 200ms/step - loss: 1.9359 - accuracy: 0.4828\n",
      "Epoch 142/300\n",
      "56/56 [==============================] - 12s 212ms/step - loss: 1.9097 - accuracy: 0.4922\n",
      "Epoch 143/300\n",
      "56/56 [==============================] - 11s 204ms/step - loss: 1.8949 - accuracy: 0.4975\n",
      "Epoch 144/300\n",
      "56/56 [==============================] - 11s 201ms/step - loss: 1.8740 - accuracy: 0.5025\n",
      "Epoch 145/300\n",
      "56/56 [==============================] - 11s 200ms/step - loss: 1.8698 - accuracy: 0.5006\n",
      "Epoch 146/300\n",
      "56/56 [==============================] - 11s 196ms/step - loss: 1.8684 - accuracy: 0.5001\n",
      "Epoch 147/300\n",
      "56/56 [==============================] - 11s 198ms/step - loss: 1.8581 - accuracy: 0.5014\n",
      "Epoch 148/300\n",
      "56/56 [==============================] - 12s 215ms/step - loss: 1.8347 - accuracy: 0.5120\n",
      "Epoch 149/300\n",
      "56/56 [==============================] - 12s 211ms/step - loss: 1.8107 - accuracy: 0.5129\n",
      "Epoch 150/300\n",
      "56/56 [==============================] - 13s 236ms/step - loss: 1.7966 - accuracy: 0.5222\n",
      "Epoch 151/300\n",
      "56/56 [==============================] - 12s 212ms/step - loss: 1.7877 - accuracy: 0.5188\n",
      "Epoch 152/300\n",
      "56/56 [==============================] - 11s 201ms/step - loss: 1.7729 - accuracy: 0.5288\n",
      "Epoch 153/300\n",
      "56/56 [==============================] - 11s 204ms/step - loss: 1.7462 - accuracy: 0.5367\n",
      "Epoch 154/300\n",
      "56/56 [==============================] - 12s 212ms/step - loss: 1.7384 - accuracy: 0.5371\n",
      "Epoch 155/300\n",
      "56/56 [==============================] - 12s 213ms/step - loss: 1.7369 - accuracy: 0.5316\n",
      "Epoch 156/300\n",
      "56/56 [==============================] - 12s 222ms/step - loss: 1.7153 - accuracy: 0.5422\n",
      "Epoch 157/300\n",
      "56/56 [==============================] - 11s 202ms/step - loss: 1.7030 - accuracy: 0.5417\n",
      "Epoch 158/300\n",
      "56/56 [==============================] - 11s 187ms/step - loss: 1.6852 - accuracy: 0.5494\n",
      "Epoch 159/300\n",
      "56/56 [==============================] - 11s 189ms/step - loss: 1.6690 - accuracy: 0.5555\n",
      "Epoch 160/300\n",
      "56/56 [==============================] - 10s 182ms/step - loss: 1.6561 - accuracy: 0.5616\n",
      "Epoch 161/300\n",
      "56/56 [==============================] - 10s 184ms/step - loss: 1.6413 - accuracy: 0.5613\n",
      "Epoch 162/300\n",
      "56/56 [==============================] - 11s 189ms/step - loss: 1.6394 - accuracy: 0.5630\n",
      "Epoch 163/300\n",
      "56/56 [==============================] - 10s 186ms/step - loss: 1.6346 - accuracy: 0.5688\n",
      "Epoch 164/300\n",
      "56/56 [==============================] - 11s 191ms/step - loss: 1.6234 - accuracy: 0.5665\n",
      "Epoch 165/300\n",
      "56/56 [==============================] - 10s 186ms/step - loss: 1.6023 - accuracy: 0.5667\n",
      "Epoch 166/300\n",
      "56/56 [==============================] - 11s 192ms/step - loss: 1.5960 - accuracy: 0.5729\n",
      "Epoch 167/300\n",
      "56/56 [==============================] - 11s 189ms/step - loss: 1.5831 - accuracy: 0.5767\n",
      "Epoch 168/300\n",
      "56/56 [==============================] - 12s 209ms/step - loss: 1.5699 - accuracy: 0.5788\n",
      "Epoch 169/300\n",
      "56/56 [==============================] - 13s 242ms/step - loss: 1.5503 - accuracy: 0.5855\n",
      "Epoch 170/300\n",
      "56/56 [==============================] - 11s 195ms/step - loss: 1.5398 - accuracy: 0.5847\n",
      "Epoch 171/300\n",
      "56/56 [==============================] - 11s 201ms/step - loss: 1.5215 - accuracy: 0.5983\n",
      "Epoch 172/300\n",
      "56/56 [==============================] - 11s 192ms/step - loss: 1.5182 - accuracy: 0.5959\n",
      "Epoch 173/300\n",
      "56/56 [==============================] - 11s 188ms/step - loss: 1.4958 - accuracy: 0.5992\n",
      "Epoch 174/300\n",
      "56/56 [==============================] - 10s 187ms/step - loss: 1.4890 - accuracy: 0.5980\n",
      "Epoch 175/300\n",
      "56/56 [==============================] - 12s 218ms/step - loss: 1.4828 - accuracy: 0.6023\n",
      "Epoch 176/300\n",
      "56/56 [==============================] - 11s 198ms/step - loss: 1.4625 - accuracy: 0.6096\n",
      "Epoch 177/300\n",
      "56/56 [==============================] - 10s 184ms/step - loss: 1.4549 - accuracy: 0.6088\n",
      "Epoch 178/300\n",
      "56/56 [==============================] - 11s 203ms/step - loss: 1.4386 - accuracy: 0.6136\n",
      "Epoch 179/300\n",
      "56/56 [==============================] - 11s 202ms/step - loss: 1.4284 - accuracy: 0.6153\n",
      "Epoch 180/300\n",
      "56/56 [==============================] - 11s 201ms/step - loss: 1.4081 - accuracy: 0.6237\n",
      "Epoch 181/300\n",
      "56/56 [==============================] - 12s 206ms/step - loss: 1.4076 - accuracy: 0.6192\n",
      "Epoch 182/300\n",
      "56/56 [==============================] - 12s 212ms/step - loss: 1.3971 - accuracy: 0.6253\n",
      "Epoch 183/300\n",
      "56/56 [==============================] - 10s 184ms/step - loss: 1.3779 - accuracy: 0.6339\n",
      "Epoch 184/300\n",
      "56/56 [==============================] - 11s 197ms/step - loss: 1.3560 - accuracy: 0.6398\n",
      "Epoch 185/300\n",
      "56/56 [==============================] - 11s 200ms/step - loss: 1.3527 - accuracy: 0.6398\n",
      "Epoch 186/300\n",
      "56/56 [==============================] - 11s 198ms/step - loss: 1.3491 - accuracy: 0.6410\n",
      "Epoch 187/300\n",
      "56/56 [==============================] - 11s 198ms/step - loss: 1.3361 - accuracy: 0.6435\n",
      "Epoch 188/300\n",
      "56/56 [==============================] - 11s 196ms/step - loss: 1.3275 - accuracy: 0.6395\n",
      "Epoch 189/300\n",
      "56/56 [==============================] - 11s 199ms/step - loss: 1.3125 - accuracy: 0.6487\n",
      "Epoch 190/300\n",
      "56/56 [==============================] - 11s 200ms/step - loss: 1.3007 - accuracy: 0.6524\n",
      "Epoch 191/300\n",
      "56/56 [==============================] - 11s 196ms/step - loss: 1.2859 - accuracy: 0.6555\n",
      "Epoch 192/300\n",
      "56/56 [==============================] - 10s 186ms/step - loss: 1.2849 - accuracy: 0.6592\n",
      "Epoch 193/300\n",
      "56/56 [==============================] - 10s 186ms/step - loss: 1.2677 - accuracy: 0.6621\n",
      "Epoch 194/300\n",
      "56/56 [==============================] - 10s 187ms/step - loss: 1.2484 - accuracy: 0.6684\n",
      "Epoch 195/300\n",
      "56/56 [==============================] - 11s 192ms/step - loss: 1.2404 - accuracy: 0.6715\n",
      "Epoch 196/300\n",
      "56/56 [==============================] - 11s 191ms/step - loss: 1.2300 - accuracy: 0.6723\n",
      "Epoch 197/300\n",
      "56/56 [==============================] - 10s 187ms/step - loss: 1.2267 - accuracy: 0.6758\n",
      "Epoch 198/300\n",
      "56/56 [==============================] - 12s 207ms/step - loss: 1.2057 - accuracy: 0.6880\n",
      "Epoch 199/300\n",
      "56/56 [==============================] - 11s 195ms/step - loss: 1.2059 - accuracy: 0.6780\n",
      "Epoch 200/300\n",
      "56/56 [==============================] - 10s 186ms/step - loss: 1.1964 - accuracy: 0.6794\n",
      "Epoch 201/300\n",
      "56/56 [==============================] - 10s 186ms/step - loss: 1.1764 - accuracy: 0.6870\n",
      "Epoch 202/300\n",
      "56/56 [==============================] - 11s 189ms/step - loss: 1.1687 - accuracy: 0.6893\n",
      "Epoch 203/300\n",
      "56/56 [==============================] - 10s 187ms/step - loss: 1.1554 - accuracy: 0.6968\n",
      "Epoch 204/300\n",
      "56/56 [==============================] - 10s 183ms/step - loss: 1.1551 - accuracy: 0.6883\n",
      "Epoch 205/300\n",
      "56/56 [==============================] - 10s 186ms/step - loss: 1.1377 - accuracy: 0.7000\n",
      "Epoch 206/300\n",
      "56/56 [==============================] - 11s 192ms/step - loss: 1.1202 - accuracy: 0.7058\n",
      "Epoch 207/300\n",
      "56/56 [==============================] - 11s 194ms/step - loss: 1.1038 - accuracy: 0.7099\n",
      "Epoch 208/300\n",
      "56/56 [==============================] - 11s 190ms/step - loss: 1.0977 - accuracy: 0.7116\n",
      "Epoch 209/300\n",
      "56/56 [==============================] - 10s 184ms/step - loss: 1.0826 - accuracy: 0.7161\n",
      "Epoch 210/300\n",
      "56/56 [==============================] - 10s 183ms/step - loss: 1.0835 - accuracy: 0.7130\n",
      "Epoch 211/300\n",
      "56/56 [==============================] - 11s 189ms/step - loss: 1.0717 - accuracy: 0.7222\n",
      "Epoch 212/300\n",
      "56/56 [==============================] - 10s 187ms/step - loss: 1.0602 - accuracy: 0.7242\n",
      "Epoch 213/300\n",
      "56/56 [==============================] - 11s 189ms/step - loss: 1.0612 - accuracy: 0.7232\n",
      "Epoch 214/300\n",
      "56/56 [==============================] - 10s 187ms/step - loss: 1.0467 - accuracy: 0.7311\n",
      "Epoch 215/300\n",
      "56/56 [==============================] - 11s 188ms/step - loss: 1.0476 - accuracy: 0.7215\n",
      "Epoch 216/300\n",
      "56/56 [==============================] - 11s 188ms/step - loss: 1.0200 - accuracy: 0.7387\n",
      "Epoch 217/300\n",
      "56/56 [==============================] - 11s 190ms/step - loss: 0.9963 - accuracy: 0.7444\n",
      "Epoch 218/300\n",
      "56/56 [==============================] - 11s 191ms/step - loss: 0.9852 - accuracy: 0.7525\n",
      "Epoch 219/300\n",
      "56/56 [==============================] - 11s 191ms/step - loss: 0.9780 - accuracy: 0.7487\n",
      "Epoch 220/300\n",
      "56/56 [==============================] - 11s 192ms/step - loss: 0.9709 - accuracy: 0.7510\n",
      "Epoch 221/300\n",
      "56/56 [==============================] - 10s 185ms/step - loss: 0.9659 - accuracy: 0.7521\n",
      "Epoch 222/300\n",
      "56/56 [==============================] - 11s 193ms/step - loss: 0.9500 - accuracy: 0.7542\n",
      "Epoch 223/300\n",
      "56/56 [==============================] - 11s 191ms/step - loss: 0.9461 - accuracy: 0.7548\n",
      "Epoch 224/300\n",
      "56/56 [==============================] - 11s 187ms/step - loss: 0.9346 - accuracy: 0.7643\n",
      "Epoch 225/300\n",
      "56/56 [==============================] - 11s 189ms/step - loss: 0.9197 - accuracy: 0.7633\n",
      "Epoch 226/300\n",
      "56/56 [==============================] - 11s 190ms/step - loss: 0.9156 - accuracy: 0.7674\n",
      "Epoch 227/300\n",
      "56/56 [==============================] - 10s 185ms/step - loss: 0.9120 - accuracy: 0.7654\n",
      "Epoch 228/300\n",
      "56/56 [==============================] - 11s 191ms/step - loss: 0.9036 - accuracy: 0.7705\n",
      "Epoch 229/300\n",
      "56/56 [==============================] - 11s 202ms/step - loss: 0.8847 - accuracy: 0.7760\n",
      "Epoch 230/300\n",
      "56/56 [==============================] - 11s 204ms/step - loss: 0.8847 - accuracy: 0.7760\n",
      "Epoch 231/300\n",
      "56/56 [==============================] - 17s 297ms/step - loss: 0.8662 - accuracy: 0.7828\n",
      "Epoch 232/300\n",
      "56/56 [==============================] - 15s 270ms/step - loss: 0.8534 - accuracy: 0.7818\n",
      "Epoch 233/300\n",
      "56/56 [==============================] - 14s 248ms/step - loss: 0.8525 - accuracy: 0.7866\n",
      "Epoch 234/300\n",
      "56/56 [==============================] - 12s 219ms/step - loss: 0.8326 - accuracy: 0.7917\n",
      "Epoch 235/300\n",
      "56/56 [==============================] - 12s 219ms/step - loss: 0.8259 - accuracy: 0.7903\n",
      "Epoch 236/300\n",
      "56/56 [==============================] - 14s 241ms/step - loss: 0.8092 - accuracy: 0.8010\n",
      "Epoch 237/300\n",
      "56/56 [==============================] - 14s 251ms/step - loss: 0.8091 - accuracy: 0.8034\n",
      "Epoch 238/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 12s 212ms/step - loss: 0.8132 - accuracy: 0.7980\n",
      "Epoch 239/300\n",
      "56/56 [==============================] - 13s 224ms/step - loss: 0.8075 - accuracy: 0.7983\n",
      "Epoch 240/300\n",
      "56/56 [==============================] - 12s 216ms/step - loss: 0.7939 - accuracy: 0.7997\n",
      "Epoch 241/300\n",
      "56/56 [==============================] - 13s 235ms/step - loss: 0.7723 - accuracy: 0.8096\n",
      "Epoch 242/300\n",
      "56/56 [==============================] - 13s 233ms/step - loss: 0.7565 - accuracy: 0.8162\n",
      "Epoch 243/300\n",
      "56/56 [==============================] - 12s 206ms/step - loss: 0.7527 - accuracy: 0.8165\n",
      "Epoch 244/300\n",
      "56/56 [==============================] - 12s 216ms/step - loss: 0.7549 - accuracy: 0.8136\n",
      "Epoch 245/300\n",
      "56/56 [==============================] - 10s 183ms/step - loss: 0.7357 - accuracy: 0.8185\n",
      "Epoch 246/300\n",
      "56/56 [==============================] - 10s 182ms/step - loss: 0.7286 - accuracy: 0.8210\n",
      "Epoch 247/300\n",
      "56/56 [==============================] - 10s 181ms/step - loss: 0.7220 - accuracy: 0.8256\n",
      "Epoch 248/300\n",
      "56/56 [==============================] - 10s 178ms/step - loss: 0.7017 - accuracy: 0.8318\n",
      "Epoch 249/300\n",
      "56/56 [==============================] - 9s 166ms/step - loss: 0.6953 - accuracy: 0.8356\n",
      "Epoch 250/300\n",
      "56/56 [==============================] - 10s 175ms/step - loss: 0.6945 - accuracy: 0.8323\n",
      "Epoch 251/300\n",
      "56/56 [==============================] - 10s 178ms/step - loss: 0.6834 - accuracy: 0.8336\n",
      "Epoch 252/300\n",
      "56/56 [==============================] - 11s 193ms/step - loss: 0.6699 - accuracy: 0.8404\n",
      "Epoch 253/300\n",
      "56/56 [==============================] - 9s 155ms/step - loss: 0.6721 - accuracy: 0.8380\n",
      "Epoch 254/300\n",
      "56/56 [==============================] - 9s 159ms/step - loss: 0.6722 - accuracy: 0.8393\n",
      "Epoch 255/300\n",
      "56/56 [==============================] - 9s 161ms/step - loss: 0.6636 - accuracy: 0.8401\n",
      "Epoch 256/300\n",
      "56/56 [==============================] - 9s 160ms/step - loss: 0.6627 - accuracy: 0.8395\n",
      "Epoch 257/300\n",
      "56/56 [==============================] - 10s 183ms/step - loss: 0.6449 - accuracy: 0.8486\n",
      "Epoch 258/300\n",
      "56/56 [==============================] - 10s 185ms/step - loss: 0.6251 - accuracy: 0.8530\n",
      "Epoch 259/300\n",
      "56/56 [==============================] - 10s 178ms/step - loss: 0.6101 - accuracy: 0.8569\n",
      "Epoch 260/300\n",
      "56/56 [==============================] - 9s 163ms/step - loss: 0.6000 - accuracy: 0.8627\n",
      "Epoch 261/300\n",
      "56/56 [==============================] - 9s 160ms/step - loss: 0.5775 - accuracy: 0.8732\n",
      "Epoch 262/300\n",
      "56/56 [==============================] - 9s 164ms/step - loss: 0.5760 - accuracy: 0.8716\n",
      "Epoch 263/300\n",
      "56/56 [==============================] - 9s 165ms/step - loss: 0.5720 - accuracy: 0.8718\n",
      "Epoch 264/300\n",
      "56/56 [==============================] - 9s 160ms/step - loss: 0.5698 - accuracy: 0.8688\n",
      "Epoch 265/300\n",
      "56/56 [==============================] - 9s 161ms/step - loss: 0.5602 - accuracy: 0.8709\n",
      "Epoch 266/300\n",
      "56/56 [==============================] - 9s 169ms/step - loss: 0.5465 - accuracy: 0.8767\n",
      "Epoch 267/300\n",
      "56/56 [==============================] - 9s 161ms/step - loss: 0.5450 - accuracy: 0.8785\n",
      "Epoch 268/300\n",
      "56/56 [==============================] - 9s 166ms/step - loss: 0.5606 - accuracy: 0.8729\n",
      "Epoch 269/300\n",
      "56/56 [==============================] - 10s 179ms/step - loss: 0.5659 - accuracy: 0.8658\n",
      "Epoch 270/300\n",
      "56/56 [==============================] - 9s 157ms/step - loss: 0.5407 - accuracy: 0.8781\n",
      "Epoch 271/300\n",
      "56/56 [==============================] - 10s 173ms/step - loss: 0.5355 - accuracy: 0.8815\n",
      "Epoch 272/300\n",
      "56/56 [==============================] - 11s 191ms/step - loss: 0.5274 - accuracy: 0.8801\n",
      "Epoch 273/300\n",
      "56/56 [==============================] - 11s 201ms/step - loss: 0.5084 - accuracy: 0.8863\n",
      "Epoch 274/300\n",
      "56/56 [==============================] - 9s 166ms/step - loss: 0.5047 - accuracy: 0.8917\n",
      "Epoch 275/300\n",
      "56/56 [==============================] - 9s 164ms/step - loss: 0.4930 - accuracy: 0.8907\n",
      "Epoch 276/300\n",
      "56/56 [==============================] - 9s 165ms/step - loss: 0.5146 - accuracy: 0.8836\n",
      "Epoch 277/300\n",
      "56/56 [==============================] - 9s 167ms/step - loss: 0.5016 - accuracy: 0.8914\n",
      "Epoch 278/300\n",
      "56/56 [==============================] - 9s 162ms/step - loss: 0.4779 - accuracy: 0.8953\n",
      "Epoch 279/300\n",
      "56/56 [==============================] - 9s 163ms/step - loss: 0.4673 - accuracy: 0.9014\n",
      "Epoch 280/300\n",
      "56/56 [==============================] - 9s 164ms/step - loss: 0.4511 - accuracy: 0.9064\n",
      "Epoch 281/300\n",
      "56/56 [==============================] - 9s 161ms/step - loss: 0.4426 - accuracy: 0.9110\n",
      "Epoch 282/300\n",
      "56/56 [==============================] - 11s 203ms/step - loss: 0.4468 - accuracy: 0.9073\n",
      "Epoch 283/300\n",
      "56/56 [==============================] - 11s 198ms/step - loss: 0.4490 - accuracy: 0.9041\n",
      "Epoch 284/300\n",
      "56/56 [==============================] - 14s 258ms/step - loss: 0.4389 - accuracy: 0.9078\n",
      "Epoch 285/300\n",
      "56/56 [==============================] - 14s 240ms/step - loss: 0.4191 - accuracy: 0.9158\n",
      "Epoch 286/300\n",
      "56/56 [==============================] - 11s 199ms/step - loss: 0.4010 - accuracy: 0.9203\n",
      "Epoch 287/300\n",
      "56/56 [==============================] - 11s 191ms/step - loss: 0.3975 - accuracy: 0.9219\n",
      "Epoch 288/300\n",
      "56/56 [==============================] - 11s 191ms/step - loss: 0.3905 - accuracy: 0.9236\n",
      "Epoch 289/300\n",
      "56/56 [==============================] - 11s 199ms/step - loss: 0.3895 - accuracy: 0.9254\n",
      "Epoch 290/300\n",
      "56/56 [==============================] - 12s 220ms/step - loss: 0.3770 - accuracy: 0.9294\n",
      "Epoch 291/300\n",
      "56/56 [==============================] - 11s 203ms/step - loss: 0.3629 - accuracy: 0.9345\n",
      "Epoch 292/300\n",
      "56/56 [==============================] - 11s 189ms/step - loss: 0.3569 - accuracy: 0.9364\n",
      "Epoch 293/300\n",
      "56/56 [==============================] - 11s 187ms/step - loss: 0.3625 - accuracy: 0.9329\n",
      "Epoch 294/300\n",
      "56/56 [==============================] - 10s 187ms/step - loss: 0.3514 - accuracy: 0.9357\n",
      "Epoch 295/300\n",
      "56/56 [==============================] - 11s 193ms/step - loss: 0.3527 - accuracy: 0.9359\n",
      "Epoch 296/300\n",
      "56/56 [==============================] - 10s 179ms/step - loss: 0.3480 - accuracy: 0.9367\n",
      "Epoch 297/300\n",
      "56/56 [==============================] - 11s 191ms/step - loss: 0.3332 - accuracy: 0.9397\n",
      "Epoch 298/300\n",
      "56/56 [==============================] - 10s 180ms/step - loss: 0.3314 - accuracy: 0.9387\n",
      "Epoch 299/300\n",
      "56/56 [==============================] - 10s 186ms/step - loss: 0.3270 - accuracy: 0.9403\n",
      "Epoch 300/300\n",
      "56/56 [==============================] - 10s 183ms/step - loss: 0.3163 - accuracy: 0.9472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fac3563d090>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model with 290-300 epochs and a batch size of 128.\n",
    "model.fit(X, y, epochs=300, batch_size=128,verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dump function from the pickle module.\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to file\n",
    "model.save('sherlock_model_300.keras')\n",
    "# Save the tokenizer\n",
    "dump(tokenizer, open('sherlock_tokenizer_300', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating New Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the load function from the pickle module.\n",
    "from pickle import load\n",
    "# Import the dependencies needed to load the model and tokenizers, and process the text.\n",
    "from keras.models import load_model\n",
    "from keras_preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    '''\n",
    "    Generate text using a trained language model.\n",
    "    \n",
    "    INPUTS:\n",
    "     - model: Trained language model (e.g., LSTM) capable of text generation.\n",
    "     - tokenizer: Tokenizer that was fit on text data.\n",
    "     - seq_len: Length of the training sequences used to train the model.\n",
    "     - seed_text: A raw string text serving as the seed for text generation.\n",
    "     - num_gen_words: The number of words to be generated by model.\n",
    "    '''\n",
    "    \n",
    "    # Final Output\n",
    "    output_text = []\n",
    "    \n",
    "    # Intial Seed Sequence\n",
    "    input_text = seed_text\n",
    "    \n",
    "    # Create num_gen_words\n",
    "    for i in range(num_gen_words):\n",
    "        \n",
    "        # Take the input text string and encode it to a sequence\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        # Pad sequences to our trained rate of 25 words.\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "        # Predict Class Probabilities for each word\n",
    "        pred_w = model.predict(pad_encoded, verbose=0)[0]\n",
    "        \n",
    "        pred_word_ind = np.argmax(pred_w, axis= -1)\n",
    "        \n",
    "        # Grab word\n",
    "        pred_word = tokenizer.index_word[pred_word_ind] \n",
    "        \n",
    "        # Update the sequence of input text (shifting one over with the new word)\n",
    "        input_text += ' ' + pred_word\n",
    "        \n",
    "        output_text.append(pred_word)\n",
    "        \n",
    "    # Make it look like a sentence.\n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a 26 word text string from the first four chapters of Moby Dick.\n",
    "# If less than 26 the accuracy is off. \n",
    "text = \"\"\"Her jacket was black, with black beads sewn\n",
    "upon it, and a fringe of little black jet ornaments. Her dress was\n",
    "brown, rather darker than\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['her', 'jacket', 'was', 'black', 'with', 'black', 'beads', 'sewn', 'upon', 'it', 'and', 'a', 'fringe', 'of', 'little', 'black', 'jet', 'ornaments', 'her', 'dress', 'was', 'brown', 'rather', 'darker', 'than']\n",
      "her jacket was black with black beads sewn upon it and a fringe of little black jet ornaments her dress was brown rather darker than\n"
     ]
    }
   ],
   "source": [
    "# Create tokens by using the separate_punc function.\n",
    "text_tokens = separate_punc(text)\n",
    "print(text_tokens)\n",
    "# Join the tokens and set them to the \"seed_text\" variable. \n",
    "seed_text = ' '.join(text_tokens)\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to the saved trained 300 epoch model. \n",
    "model = load_model('Resources/sherlock_model_300.keras')\n",
    "# Set the tokenizer to the trained tokenizer from the model. \n",
    "tokenizer = load(open('Resources/sherlock_tokenizer_300', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coffee colour with a little purple plush at the neck and sleeves her gloves were greyish and were worn through at the right forefinger her'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the generate_text function and pass in the required parameters. We set the num_gen_words =50. \n",
    "generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy, import the large english model (this is 600 MB), and disable tagger, ner and lemmatizer.\n",
    "# Download the large English model ~ 600 MB if you have not done so: `python -m spacy download en_core_web_lg`.\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg',disable=[\"tagger\", \"ner\", \"lemmatizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the text file with the read_file function.\n",
    "def read_file(filepath):\n",
    "    \"\"\"\n",
    "    Reads in a text file from the directory and saves the file contents to a variable.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to read and save\n",
    "        \n",
    "    Returns: \n",
    "        A string containing the file contents.\n",
    "    \"\"\"\n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()\n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tokenize and Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the separate_punc function to remove the puncutation. \n",
    "def separate_punc():\n",
    "    \"\"\"\n",
    "    Retrieves all the words in the text without punctuation \n",
    "\n",
    "    Args:\n",
    "        text (str): The input text from which words are extracted witout punctuation.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of words\n",
    "\n",
    "    \"\"\"\n",
    "    # Create a list comprehension to get only the tokens, i.e., words in the text.\n",
    "    return [token.text.lower() for token in nlp(holmes_text) \\\n",
    "            if token.text not in '\\n\\n \\n\\n\\n!\"“”-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in the first four chapters of Moby Dick to the read_file function.\n",
    "holmes_text = read_file('Resources/A_Case_Of_Identity.txt')\n",
    "print(holmes_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and tokenize the text using the separate_punc function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7106"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the length of the tokens list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dear', 'fellow', 'said', 'sherlock', 'holmes', 'as', 'we', 'sat', 'on', 'either', 'side', 'of', 'the', 'fire', 'in', 'his', 'lodgings', 'at', 'baker', 'street', 'life', 'is', 'infinitely', 'stranger', 'than', 'anything', 'which', 'the', 'mind', 'of', 'man', 'could', 'invent', 'we', 'would', 'not', 'dare', 'to', 'conceive', 'the', 'things', 'which', 'are', 'really', 'mere', 'commonplaces', 'of', 'existence', 'if', 'we', 'could', 'fly', 'out', 'of', 'that', 'window', 'hand', 'in', 'hand', 'hover', 'over', 'this', 'great', 'city', 'gently', 'remove', 'the', 'roofs', 'and', 'peep', 'in', 'at', 'the', 'queer', 'things', 'which', 'are', 'going', 'on', 'the', 'strange', 'coincidences', 'the', 'plannings', 'the', 'cross', 'purposes', 'the', 'wonderful', 'chains', 'of', 'events', 'working', 'through', 'generations', 'and', 'leading', 'to', 'the', 'most', 'outré', 'results', 'it', 'would', 'make', 'all', 'fiction', 'with', 'its', 'conventionalities', 'and', 'foreseen', 'conclusions', 'most', 'stale', 'and', 'unprofitable', 'and', 'yet', 'i', 'am', 'not', 'convinced', 'of', 'it', 'i', 'answered', 'the', 'cases', 'which', 'come', 'to', 'light', 'in', 'the', 'papers', 'are', 'as', 'a', 'rule', 'bald', 'enough', 'and', 'vulgar', 'enough', 'we', 'have', 'in', 'our', 'police', 'reports', 'realism', 'pushed', 'to', 'its', 'extreme', 'limits', 'and', 'yet', 'the', 'result', 'is', 'it', 'must', 'be', 'confessed', 'neither', 'fascinating', 'nor', 'artistic', 'a', 'certain', 'selection', 'and', 'discretion', 'must', 'be', 'used', 'in', 'producing', 'a', 'realistic', 'effect', 'remarked', 'holmes', 'this', 'is', 'wanting', 'in', 'the', 'police', 'report', 'where', 'more', 'stress', 'is', 'laid', 'perhaps', 'upon', 'the', 'platitudes', 'of', 'the', 'magistrate', 'than', 'upon', 'the', 'details', 'which', 'to', 'an', 'observer', 'contain', 'the', 'vital', 'essence', 'of', 'the', 'whole', 'matter', 'depend', 'upon', 'it', 'there', 'is', 'nothing', 'so', 'unnatural', 'as', 'the', 'commonplace', 'i', 'smiled', 'and', 'shook', 'my', 'head', 'i', 'can', 'quite', 'understand', 'your', 'thinking', 'so', 'i', 'said', 'of', 'course', 'in', 'your', 'position', 'of', 'unofficial', 'adviser', 'and', 'helper', 'to', 'everybody', 'who', 'is', 'absolutely', 'puzzled', 'throughout', 'three', 'continents', 'you', 'are', 'brought', 'in', 'contact', 'with', 'all', 'that', 'is', 'strange', 'and', 'bizarre', 'but', 'here”—i', 'picked', 'up', 'the', 'morning', 'paper', 'from', 'the', 'ground—“let', 'us', 'put', 'it', 'to', 'a', 'practical', 'test', 'here', 'is', 'the', 'first', 'heading']\n"
     ]
    }
   ],
   "source": [
    "# Look over the tokems to make sure all theo punctuation has been reomved.  \n",
    "# Some punctuation wasn't removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lid', 'its', 'splendour', 'was', 'in', 'such', 'contrast', 'to', 'his', 'homely', 'ways', 'and', 'simple', 'life', 'that', 'i', 'could', 'not', 'help', 'commenting', 'upon', 'it', 'ah', 'said', 'he', 'i', 'forgot', 'that', 'i', 'had', 'not', 'seen', 'you', 'for', 'some', 'weeks', 'it', 'is', 'a', 'little', 'souvenir', 'from', 'the', 'king', 'of', 'bohemia', 'in', 'return', 'for', 'my', 'assistance', 'in', 'the', 'case', 'of', 'the', 'irene', 'adler', 'papers', 'and', 'the', 'ring', 'i', 'asked', 'glancing', 'at', 'a', 'remarkable', 'brilliant', 'which', 'sparkled', 'upon', 'his', 'finger', 'it', 'was', 'from', 'the', 'reigning', 'family', 'of', 'holland', 'though', 'the', 'matter', 'in', 'which', 'i', 'served', 'them', 'was', 'of', 'such', 'delicacy', 'that', 'i', 'can', 'not', 'confide', 'it', 'even', 'to', 'you', 'who', 'have', 'been', 'good', 'enough', 'to', 'chronicle', 'one', 'or', 'two', 'of', 'my', 'little', 'problems', 'and', 'have', 'you', 'any', 'on', 'hand', 'just', 'now', 'i', 'asked', 'with', 'interest', 'some', 'ten', 'or', 'twelve', 'but', 'none', 'which', 'present', 'any', 'feature', 'of', 'interest', 'they', 'are', 'important', 'you', 'understand', 'without', 'being', 'interesting', 'indeed', 'i', 'have', 'found', 'that', 'it', 'is', 'usually', 'in', 'unimportant', 'matters', 'that', 'there', 'is', 'a', 'field', 'for', 'the', 'observation', 'and', 'for', 'the', 'quick', 'analysis', 'of', 'cause', 'and', 'effect', 'which', 'gives', 'the', 'charm', 'to', 'an', 'investigation', 'the', 'larger', 'crimes', 'are', 'apt', 'to', 'be', 'the', 'simpler', 'for', 'the', 'bigger', 'the', 'crime', 'the', 'more', 'obvious', 'as', 'a', 'rule', 'is', 'the', 'motive', 'in', 'these', 'cases', 'save', 'for', 'one', 'rather', 'intricate', 'matter', 'which', 'has', 'been', 'referred', 'to', 'me', 'from', 'marseilles', 'there', 'is', 'nothing', 'which', 'presents', 'any', 'features', 'of', 'interest', 'it', 'is', 'possible', 'however', 'that', 'i', 'may', 'have', 'something', 'better', 'before', 'very', 'many', 'minutes', 'are', 'over', 'for', 'this', 'is', 'one', 'of', 'my', 'clients', 'or', 'i', 'am', 'much', 'mistaken', 'he', 'had', 'risen', 'from', 'his', 'chair', 'and', 'was', 'standing', 'between', 'the', 'parted', 'blinds', 'gazing', 'down', 'into', 'the', 'dull', 'neutral', 'tinted', 'london', 'street', 'looking', 'over', 'his', 'shoulder', 'i', 'saw', 'that', 'on', 'the', 'pavement', 'opposite', 'there', 'stood', 'a', 'large', 'woman', 'with']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sequences of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize into sequences of tokens.\n",
    "# Use 25 words to predict the 26th word using \"+1\".\n",
    "\n",
    "\n",
    "# Create an empty list of sequences\n",
    "\n",
    "# Use a for loop to create lists of 26 tokens for the whole text. \n",
    "\n",
    "    \n",
    "    # Add to list of sequences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7080"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The list of text_sequences should be 26 less than the total tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Tokenization with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Keras tokenization to format the data from words into a numerical format.\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Keras Tokenizer class \n",
    "\n",
    "# Map each word with an index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1596\n"
     ]
    }
   ],
   "source": [
    "# What is the size of the vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode each word in the text_sequences to the indices. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the List of Sequences to Arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy to convert the list of sequences to arrays.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7080"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the all 26 word list of lists to arrays.\n",
    "\n",
    "# Get the length of the array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input sequences and one-hot encode the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the to_categorical function to convert the arrays to binary values.\n",
    "import keras\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input variable, `X`, to the first 25 numbers of each array.\n",
    "\n",
    "# Set target variable, `y`, to the last number of each array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7080, 25)\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "# Get the shape of X\n",
    "\n",
    "# Get the length of each sequence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7080,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the shape of y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next,one-hot encode the target variable to get transform each index to a binary value. \n",
    "# We increase the vocabulary by 1 so we can predict the next word. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7080, 1597)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the shape of y again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a LSTM  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies for LSTM model.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size, seq_len):\n",
    "    \"\"\"\n",
    "    Create and compile an LSTM-based sequential model for text generation.\n",
    "\n",
    "    Parameters:\n",
    "    - vocabulary_size (int): The size of the vocabulary, i.e., the number of unique words in the text.\n",
    "    - seq_len (int): The length of the input sequences, indicating the number of words in each sequence.\n",
    "\n",
    "    Returns:\n",
    "    - model (Sequential): A compiled Keras sequential model for text generation.\n",
    "\n",
    "    Model Architecture:\n",
    "    - Embedding Layer: Maps word indices to dense vectors.\n",
    "    - LSTM Layer 1: 150 units, returns full sequence.\n",
    "    - LSTM Layer 2: 150 units.\n",
    "    - Dense Layer: 150 units, ReLU activation.\n",
    "    - Output Layer: Dense layer with softmax activation for multi-class classification.\n",
    "\n",
    "    Compilation:\n",
    "    - Loss: Categorical crossentropy.\n",
    "    - Optimizer: Adam.\n",
    "    - Metrics: Accuracy.\n",
    "\n",
    "    Usage Example:\n",
    "    ```python\n",
    "    model = create_model(vocabulary_size=10000, seq_len=25)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, 25, input_length=seq_len))\n",
    "    model.add(LSTM(150, return_sequences=True))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dense(150, activation='relu'))\n",
    "\n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "   \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the model and pass in the vocabulary (+1) and the seq_len (25 words).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model with 290-300 epochs and a batch size of 128.\n",
    "model.fit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dump function from the pickle module.\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to file\n",
    "model.save('<name>.keras')\n",
    "# Save the tokenizer\n",
    "dump(tokenizer, open('<name>_tokenizer', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating New Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the load function from the pickle module.\n",
    "from pickle import load\n",
    "# Import the dependencies needed to load the model and tokenizers, and process the text.\n",
    "from keras.models import load_model\n",
    "from keras_preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    '''\n",
    "    Generate text using a trained language model.\n",
    "    \n",
    "    INPUTS:\n",
    "     - model: Trained language model (e.g., LSTM) capable of text generation.\n",
    "     - tokenizer: Tokenizer that was fit on text data.\n",
    "     - seq_len: Length of the training sequences used to train the model.\n",
    "     - seed_text: A raw string text serving as the seed for text generation.\n",
    "     - num_gen_words: The number of words to be generated by model.\n",
    "    '''\n",
    "    \n",
    "    # Final Output\n",
    "    output_text = []\n",
    "    \n",
    "    # Intial Seed Sequence\n",
    "    input_text = seed_text\n",
    "    \n",
    "    # Create num_gen_words\n",
    "    for i in range(num_gen_words):\n",
    "        \n",
    "        # Take the input text string and encode it to a sequence\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        # Pad sequences to our trained rate of 25 words.\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "        # Predict Class Probabilities for each word\n",
    "        pred_w = model.predict(pad_encoded, verbose=0)[0]\n",
    "        \n",
    "        pred_word_ind = np.argmax(pred_w, axis= -1)\n",
    "        \n",
    "        # Grab word\n",
    "        pred_word = tokenizer.index_word[pred_word_ind] \n",
    "        \n",
    "        # Update the sequence of input text (shifting one over with the new word)\n",
    "        input_text += ' ' + pred_word\n",
    "        \n",
    "        output_text.append(pred_word)\n",
    "        \n",
    "    # Make it look like a sentence.\n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a 26 word text string from the first four chapters of Moby Dick.\n",
    "# If less than 26 the accuracy is off. \n",
    "text = \"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokens by using the separate_punc function.\n",
    "\n",
    "# Join the tokens and set them to the \"seed_text\" variable. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to the saved trained 300 epoch model. \n",
    "model = load_model('<name>.keras')\n",
    "# Set the tokenizer to the trained tokenizer from the model. \n",
    "tokenizer = load(open('<name>_tokenizer', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the generate_text function and pass in the required parameters. Set the `num_gen_words` to 25. \n",
    "generate_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

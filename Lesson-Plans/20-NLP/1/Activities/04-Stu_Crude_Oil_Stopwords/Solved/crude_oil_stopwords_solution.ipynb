{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Reuters database from the nltk corpus \n",
    "from nltk.corpus import reuters, stopwords\n",
    "# Import tokenizers and pandas\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# Import regular expressions\n",
    "import re\n",
    "\n",
    "# Import nltk the sentence tokenizer.\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TURKEY CALLS FOR DIALOGUE TO SOLVE DISPUTE\n",
      "  Turkey said today its disputes with\n",
      "  Greece, including rights on the continental shelf in the Aegean\n",
      "  Sea, should be solved through negotiations.\n",
      "      A Foreign Ministry statement said the latest crisis between\n",
      "  the two NATO members stemmed from the continental shelf dispute\n",
      "  and an agreement on this issue would effect the security,\n",
      "  economy and other rights of both countries.\n",
      "      \"As the issue is basicly political, a solution can only be\n",
      "  found by bilateral negotiations,\" the statement said. Greece has\n",
      "  repeatedly said the issue was legal and could be solved at the\n",
      "  International Court of Justice.\n",
      "      The two countries approached armed confrontation last month\n",
      "  after Greece announced it planned oil exploration work in the\n",
      "  Aegean and Turkey said it would also search for oil.\n",
      "      A face-off was averted when Turkey confined its research to\n",
      "  territorrial waters. \"The latest crises created an historic\n",
      "  opportunity to solve the disputes between the two countries,\"\n",
      "  the Foreign Ministry statement said.\n",
      "      Turkey's ambassador in Athens, Nazmi Akiman, was due to\n",
      "  meet Prime Minister Andreas Papandreou today for the Greek\n",
      "  reply to a message sent last week by Turkish Prime Minister\n",
      "  Turgut Ozal. The contents of the message were not disclosed.\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the second article from the crude category of the Reuters library and print out the article.\n",
    "crude_article = reuters.raw(fileids=reuters.fileids(categories='crude')[2])\n",
    "print(crude_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to clean the article using stopwords and regular expressions.\n",
    "def clean_text(article):\n",
    "    \"\"\"\n",
    "    Preprocesses a given text article by performing the following steps:\n",
    "    \n",
    "    1. Removes stopwords (common words in English language).\n",
    "    2. Uses regular expressions to remove non-alphabet characters (e.g., punctuation).\n",
    "    3. Tokenizes the cleaned text into words.\n",
    "    4. Filters out words that are in the stopwords list.\n",
    "    \n",
    "    Parameters:\n",
    "        article (str): The input text article to be processed.\n",
    "\n",
    "    Returns:\n",
    "        list of str: A list of preprocessed words from the input article.\n",
    "    \"\"\"\n",
    "    # Get the stopwords\n",
    "    sw = set(stopwords.words('english'))\n",
    "    # Use regex to substitute everything that is not a letter with an empty string.\n",
    "    regex = re.compile(\"[^a-zA-Z ]\") \n",
    "    re_clean = regex.sub(' ', article)\n",
    "    # Tokenize the words \n",
    "    words = word_tokenize(re_clean)\n",
    "    # Retrieve only the words that aren't in the stopwords.\n",
    "    output = [word.lower() for word in words if word.lower() not in sw]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'athens', 'disclosed', 'greece', 'said', 'minister', 'greek', 'planned', 'bilateral', 'search', 'found', 'historic', 'including', 'created', 'oil', 'akiman', 'turkish', 'nazmi', 'solved', 'message', 'exploration', 'papandreou', 'two', 'prime', 'stemmed', 'solve', 'would', 'turkey', 'opportunity', 'basicly', 'meet', 'announced', 'contents', 'rights', 'solution', 'armed', 'also', 'court', 'security', 'research', 'shelf', 'today', 'andreas', 'could', 'international', 'month', 'sent', 'agreement', 'latest', 'approached', 'foreign', 'countries', 'aegean', 'statement', 'nato', 'legal', 'sea', 'political', 'issue', 'negotiations', 'ozal', 'ministry', 'last', 'face', 'week', 'due', 'economy', 'territorrial', 'continental', 'calls', 'disputes', 'waters', 'confrontation', 'confined', 'reply', 'dispute', 'averted', 'justice', 'effect', 'members', 'work', 'repeatedly', 'turgut', 'dialogue', 'ambassador', 'crisis', 'crises'}\n"
     ]
    }
   ],
   "source": [
    "# Call the function with the article and print out the unique words. \n",
    "result = clean_text(crude_article)\n",
    "print(set(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a second function that does the same as the first function, but adds custom stopwords to the NLTK stopwords.\n",
    "def clean_text_again(article):\n",
    "    \"\"\"\n",
    "    Preprocesses a given text article by performing the following steps:\n",
    "    \n",
    "    1. Removes stopwords (common words in English language).\n",
    "    2. Creates a custom dictionary of stopwords. \n",
    "    3. Uses regular expressions to remove non-alphabet characters (e.g., punctuation).\n",
    "    4. Tokenizes the cleaned text into words.\n",
    "    5. Filters out words that are not stopwords.\n",
    "    \n",
    "    Parameters:\n",
    "        article (str): The input text article to be processed.\n",
    "\n",
    "    Returns:\n",
    "        list of str: A list of preprocessed words from the input article.\n",
    "    \"\"\"\n",
    "    # Get the stopwords\n",
    "    sw = set(stopwords.words('english'))\n",
    "    # Create a custom dictionary of stopwords. \n",
    "    sw_addons = {'said', 'sent', 'found', 'including', 'today', 'announced', 'week', 'basicly','also'}\n",
    "    # Use regex to substitute everything that is not a letter with an empty string.\n",
    "    regex = re.compile(\"[^a-zA-Z ]\")\n",
    "    re_clean = regex.sub('', article)\n",
    "    # Tokenize the words \n",
    "    words = word_tokenize(re_clean)\n",
    "    # Retrieve only the words not in the stopwords. Create a union of the sw and sw_addons.\n",
    "    output = [word.lower() for word in words if word.lower() not in sw.union(sw_addons)]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'athens', 'disclosed', 'greece', 'minister', 'greek', 'planned', 'bilateral', 'search', 'historic', 'created', 'oil', 'akiman', 'turkish', 'nazmi', 'solved', 'message', 'exploration', 'papandreou', 'two', 'prime', 'stemmed', 'solve', 'would', 'turkey', 'opportunity', 'meet', 'contents', 'crises', 'rights', 'solution', 'armed', 'court', 'security', 'research', 'shelf', 'andreas', 'could', 'international', 'month', 'agreement', 'latest', 'approached', 'foreign', 'countries', 'aegean', 'statement', 'nato', 'legal', 'sea', 'political', 'issue', 'negotiations', 'ministry', 'turkeys', 'last', 'due', 'economy', 'territorrial', 'continental', 'calls', 'disputes', 'waters', 'confrontation', 'confined', 'faceoff', 'reply', 'dispute', 'averted', 'justice', 'effect', 'members', 'work', 'repeatedly', 'turgut', 'dialogue', 'ambassador', 'crisis', 'ozal'}\n"
     ]
    }
   ],
   "source": [
    "# Call the function with the article and print out the unique words.\n",
    "result2 = clean_text_again(crude_article)\n",
    "print(set(result2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import reuters and stopwords \n",
    "from nltk.corpus import reuters, stopwords\n",
    "# Import ngrams\n",
    "from nltk.util import ngrams\n",
    "# Import the WordNetLemmatizer class \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "# Import the word tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Import regular expressions\n",
    "import re\n",
    "# Import pandas \n",
    "import pandas as pd\n",
    "# Import the Counter class from the collections library.\n",
    "from collections import Counter\n",
    "\n",
    "# Download \"punkt\" sentence tokenizer and \"wordnet\" that the lemmatizer uses.\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the lemmatizer\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the articles about grains.\n",
    "ids = reuters.fileids(categories='grain')\n",
    "corpus = [reuters.raw(i) for i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the process_text function that processes the words for the article and and lemmatizes the words to their root.\n",
    "def process_text(article):\n",
    "    \"\"\"\n",
    "    Preprocesses a given text article by performing the following steps:\n",
    "    \n",
    "    1. Removes stopwords (common words in English language).\n",
    "    2. Uses regular expressions to remove non-alphabet characters (e.g., punctuation).\n",
    "    3. Tokenizes the cleaned text into words.\n",
    "    4. Lemmatizes the words to their base form.\n",
    "    5. Filters out words that are in the stopwords list.\n",
    "    \n",
    "    Parameters:\n",
    "        article (str): The input text article to be processed.\n",
    "\n",
    "    Returns:\n",
    "        list of str: A list of preprocessed words from the input article.\n",
    "    \"\"\"\n",
    "    # Get the stopwords\n",
    "    sw = set(stopwords.words('english'))\n",
    "    # Use regex to substitute everything that is not a letter with an empty string.\n",
    "    regex = re.compile(\"[^a-zA-Z ]\")\n",
    "    re_clean = regex.sub('', article)\n",
    "    # Tokenize the words \n",
    "    words = word_tokenize(re_clean)\n",
    "    # Lemmatize the words\n",
    "    lem = [lemma.lemmatize(word) for word in words]\n",
    "    # Retrieve only the words that aren't in the stopwords\n",
    "    output = [word.lower() for word in lem if word.lower() not in sw]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word_counter function that takes all the articles and processes them with the process_text function.\n",
    "def word_counter(corpus): \n",
    "    \"\"\"\n",
    "    Counts and returns the top ten most common words in a given corpus of text.\n",
    "\n",
    "    This function processes the text in the input corpus by:\n",
    "    1. Combining all articles into one large string.\n",
    "    2. Passing the combined text to the `process_text` function, which removes stopwords, \n",
    "       non-alphabet characters, tokenizes the text, and lemmatizes words.\n",
    "    3. Counts the occurrences of each word and retrieves the top ten most common words.\n",
    "\n",
    "    Parameters:\n",
    "        corpus (list of str): A list of text articles to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the top ten words found in the corpus,\n",
    "        with two columns: \"word\" (the word itself) and \"count\" (the number of times it appears).\n",
    "    \"\"\"\n",
    "    # Combine all articles in corpus into one large string\n",
    "    big_string = ' '.join(corpus)\n",
    "    # Pass the combined articles to the process_text function\n",
    "    processed = process_text(big_string)\n",
    "    # Get the top ten most common words.\n",
    "    top_10 = dict(Counter(processed).most_common(10))\n",
    "    # Create a DataFrame with the top ten words with \"word\" and \"count\" columns.\n",
    "    return pd.DataFrame(list(top_10.items()), columns=['word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>said</td>\n",
       "      <td>1803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tonne</td>\n",
       "      <td>1224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mln</td>\n",
       "      <td>1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wheat</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>us</td>\n",
       "      <td>842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>grain</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>corn</td>\n",
       "      <td>579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>year</td>\n",
       "      <td>549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pct</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>export</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  count\n",
       "0    said   1803\n",
       "1   tonne   1224\n",
       "2     mln   1080\n",
       "3   wheat    998\n",
       "4      us    842\n",
       "5   grain    704\n",
       "6    corn    579\n",
       "7    year    549\n",
       "8     pct    472\n",
       "9  export    443"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass the corpus of articles to the word_counter function.\n",
    "word_counter(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_counter(corpus): \n",
    "    \"\"\"\n",
    "    Counts and returns the top ten most common bigrams in a given corpus of text.\n",
    "\n",
    "    This function processes the text in the input corpus by:\n",
    "    1. Combining all articles into one large string.\n",
    "    2. Passing the combined text to the `process_text` function, which removes stopwords, \n",
    "       non-alphabet characters, tokenizes the text, and lemmatizes words.\n",
    "    3. Creates bigrams (pairs of adjacent words) from the processed text.\n",
    "    4. Counts the occurrences of each bigram and retrieves the top ten most common bigrams.\n",
    "\n",
    "    Parameters:\n",
    "        corpus (list of str): A list of text articles to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the top ten bigrams found in the corpus,\n",
    "        with two columns: \"bigram\" (the pair of adjacent words) and \"count\" (the number of times it appears).\n",
    "    \"\"\"\n",
    "    # Combine all articles in corpus into one large string\n",
    "    big_string = ' '.join(corpus)\n",
    "    # Pass the combined articles to the process_text function\n",
    "    processed = process_text(big_string)\n",
    "    # Create bigrams from the processed text.\n",
    "    bigrams = ngrams(processed, n=2)\n",
    "    # Get the top ten most common bigrams\n",
    "    top_10 = dict(Counter(bigrams).most_common(10))\n",
    "    # Create a DataFrame with the top ten bigrams with \"bigram\" and \"count\" columns.\n",
    "    return pd.DataFrame(list(top_10.items()), columns=['bigram', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(mln, tonne)</td>\n",
       "      <td>482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(nil, nil)</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(us, agriculture)</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(department, said)</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(agriculture, department)</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(source, said)</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(soviet, union)</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(official, said)</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(mln, dlrs)</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(last, year)</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      bigram  count\n",
       "0               (mln, tonne)    482\n",
       "1                 (nil, nil)    204\n",
       "2          (us, agriculture)    161\n",
       "3         (department, said)    147\n",
       "4  (agriculture, department)    140\n",
       "5             (source, said)    129\n",
       "6            (soviet, union)    119\n",
       "7           (official, said)    117\n",
       "8                (mln, dlrs)    115\n",
       "9               (last, year)    111"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass the corpus of articles to the bigram_counter function.\n",
    "bigram_counter(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

The history of video games began in the 1950s and 1960s as computer scientists began designing simple games and simulations on minicomputers and mainframes. Spacewar! was developed by Massachusetts Institute of Technology (MIT) student hobbyists in 1962 as one of the first such games on a video display. The first consumer video game hardware was released in the early 1970s. The first home video game console was the Magnavox Odyssey, and the first arcade video games were Computer Space and Pong. After its home console conversions, numerous companies sprang up to capture Pong's success in both the arcade and the home by cloning the game, causing a series of boom and bust cycles due to oversaturation and lack of innovation.

By the mid-1970s, low-cost programmable microprocessors replaced the discrete transistorâ€“transistor logic circuitry of early hardware, and the first ROM cartridge-based home consoles arrived, including the Atari Video Computer System (VCS). Coupled with rapid growth in the golden age of arcade video games, including Space Invaders and Pac-Man, the home console market also flourished. The 1983 video game crash in the United States was characterized by a flood of too many games, often of poor or cloned qualities, and the sector saw competition from inexpensive personal computers and new types of games being developed for them. The crash prompted Japan's video game industry to take leadership of the market, which had only suffered minor impacts from the crash. Nintendo released its Nintendo Entertainment System in the United States in 1985, helping to rebound the failing video games sector. The latter part of the 1980s and early 1990s included video games driven by improvements and standardization in personal computers and the console war competition between Nintendo and Sega as they fought for market share in the United States. The first major handheld video game consoles appeared in the 1990s, led by Nintendo's Game Boy platform.

In the early 1990s, advancements in microprocessor technology gave rise to real-time 3D polygonal graphic rendering in game consoles, as well as in PCs by way of graphics cards. Optical media via CD-ROMs began to be incorporated into personal computers and consoles, including Sony's fledgling PlayStation console line, pushing Sega out of the console hardware market while diminishing Nintendo's role. By the late 1990s, the Internet also gained widespread consumer use, and video games began incorporating online elements. Microsoft entered the console hardware market in the early 2000s with its Xbox line, fearing that Sony's PlayStation positioned as a game console and entertainment device, would displace personal computers. While Sony and Microsoft continued to develop hardware for comparable top-end console features, Nintendo opted to focus on innovative gameplay. Nintendo developed the Wii with motion-sensing controls, which helped to draw in non-traditional players and helped to resecure Nintendo's position in the industry; Nintendo followed this same model in the release of the Nintendo Switch.

From the 2000s and into the 2010s, the industry has seen a shift of demographics as mobile gaming on smartphones and tablets displaced handheld consoles, and casual gaming became an increasingly larger sector of the market, as well as a growth in the number of players from China and other areas not traditionally tied to the industry. To take advantage of these shifts, traditional revenue models were supplanted with ongoing revenue stream models such as free-to-play, freemium, and subscription-based games. As triple-A video game production became more costly and risk-averse, opportunities for more experimental and innovative independent game development grew over the 2000s and 2010s, aided by the popularity of mobile and casual gaming and the ease of digital distribution. Hardware and software technology continues to drive improvement in video games, with support for high-definition video at high framerates and for virtual and augmented reality-based games.
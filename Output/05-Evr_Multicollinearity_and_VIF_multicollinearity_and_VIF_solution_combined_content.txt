##--CODE--##
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

##--CODE--##
# Import the data
df = pd.read_csv("https://static.bc-edx.com/ai/ail-v-1-0/m12/lesson_1/datasets/car-data-encoded.csv").dropna()
df.head()

##--CODE--##
# Get the features (everything except the "price" column)
X = df.copy().drop(columns="price")
X.head()

##--CODE--##
# Get the target column
y = df["price"].values.reshape(-1,1)
y[0:5]

##--CODE--##
# Use the Sklearn `train_test_split()` function to split the data into training and testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

##--CODE--##
# Create a function to calculate VIF
from statsmodels.stats.outliers_influence import variance_inflation_factor

def calc_vif(X):
    vif = pd.DataFrame()
    vif["variables"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

    return(vif)

##--CODE--##
# Calculate vif for the dataframe

calc_vif(X).sort_values("VIF")

##--CODE--##
# Investigate the engine-location column to see why it returned a VIF of NaN
X['engine-location']

##--CODE--##
# Use value_counts to confirm
X['engine-location'].value_counts()

##--CODE--##
# Create another X variable by dropping engine-location 
# and the 4 columns with the highest VIF scores

X_vif = X.drop(columns=['engine-location', 'width', 'wheel-base', 'length', 'height'])

# Recalculate the VIF scores
calc_vif(X_vif).sort_values('VIF')

##--CODE--##
# Split the data into training and testing sets
X_full_train, X_full_test, X_vif_train, X_vif_test, y_train, y_test = train_test_split(X, X_vif, y, random_state=14)

##--CODE--##
# Train two models using the different X variables

# Create the models
lr1 = LinearRegression()
lr2 = LinearRegression()

# Fit the models
lr1.fit(X_full_train, y_train)
lr2.fit(X_vif_train, y_train)

##--CODE--##
# Provided code to create the adjusted r-squared function
def r2_adj(x, y, model):
    r2 = model.score(x,y)
    n_cols = x.shape[1]
    return 1 - (1 - r2) * (len(y) - 1) / (len(y) - n_cols - 1)

##--CODE--##
# Compare the adjusted r-squared of the two models
adj_score1 = r2_adj(X_full_test, y_test, lr1)
adj_score2 = r2_adj(X_vif_test, y_test, lr2)
print(f"1 Feature Adjusted R2: {adj_score1}")
print(f"2 Feature Adjusted R2: {adj_score2}")

##--CODE--##



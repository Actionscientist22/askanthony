##--CODE--##
# Import reuters database from the nltk corpus 
from nltk.corpus import reuters
# Import tokenizers
from nltk.tokenize import sent_tokenize, word_tokenize

# Download the reuters corpora and the "punkt" sentence tokenizer.
import nltk
nltk.download("reuters")
nltk.download('punkt')

## The NLTK Reuters corpus

##--CODE--##
# The reuters corpus includes over 10,000 news articles, many of which are about financial markets
# These articles are tagged by topic, or category
# Get the categories
print(reuters.categories())

##--CODE--##
# We'll find the first article about cocoa.
reuters.fileids(categories = 'cocoa')[0]

##--CODE--##
# Get the raw text from the first article. 
article = reuters.raw('test/15095')
print(article)

## Tokenizing with Python `split()`

##--CODE--##
# We can mimic tokenization first by using `split()` on the article.
article.split('.')

##--CODE--##
# Then we split the first sentence on the whitespace.
sent = article.split('.')[0]
print(sent.split(' '))

## NLTK tokenization

##--CODE--##
# NLTK tokenizes in similar way by using the `sent_tokenize` function
sent_tokenize(article)

##--CODE--##
# We can tokenize the first sentence with the `word_tokenize` function.
sent = sent_tokenize(article)[0]
print(word_tokenize(sent))

**Question: What differences are there between using Python to split the sentence and tokenizer functions?**
- Python keeps the escape characeter (\n)
- NLTK tokenizer doesn't keep the escape character and period, and splits the parentheses around "CPA". 

##--CODE--##



##--CODE--##
# Import the modules
import pandas as pd
from sklearn.cluster import KMeans

### Read in the CSV file and prepare the Pandas DataFrame

##--CODE--##
# Read the csv file into a pandas DataFrame
customers_transformed_df = pd.read_csv("Resources/customers.csv")

# Review the DataFrame
customers_transformed_df.head()

### Step 1: Use PCA to reduce the dimensionality of the transformed customers DataFrame to 2 principal components

##--CODE--##
# Import the PCA module
from sklearn.decomposition import PCA

##--CODE--##
# Instantiate the PCA instance and declare the number of PCA variables
pca=PCA(n_components=2)

##--CODE--##
# Fit the PCA model on the transformed credit card DataFrame
customers_pca = pca.fit_transform(customers_transformed_df)

# Review the first 5 rows of the array of list data
customers_pca[:5]

### Step 2: Using the `explained_variance_ratio_` function from PCA, calculate the percentage of the total variance that is captured by the two PCA variables.

##--CODE--##
# Calculate the PCA explained variance ratio
pca.explained_variance_ratio_

**Question:** What is the explained variance ratio captured by the two PCA variables?
    
**Answer:** About 85% of the total variance is condensed into the 2 PCA variables.

### Step 3: Using the `customer_pca` data, create a Pandas DataFrame called customers_pca_df. The columns of the DataFrame should be called "PCA1" and "PCA2".

##--CODE--##
# Create the PCA DataFrame
customers_pca_df = pd.DataFrame(
    customers_pca,
    columns=["PCA1", "PCA2"]
)

# Review the PCA DataFrame
customers_pca_df.head()

### Step 4: Using the `customers_pca_df` Dataframe, utilize the elbow method to determine the optimal value of k.

##--CODE--##
# Create a a list to store inertia values and the values of k
inertia = []
k = list(range(1, 11))

# Create a for-loop where each value of k is evaluated using the K-means algorithm
# Fit the model using the service_ratings DataFrame
# Append the value of the computed inertia from the `inertia_` attribute of the KMeans model instance
for i in k:
    k_model = KMeans(n_clusters=i, n_init='auto', random_state=0)
    k_model.fit(customers_pca_df)
    inertia.append(k_model.inertia_)

# Define a DataFrame to hold the values for k and the corresponding inertia
elbow_data = {"k": k, "inertia": inertia}

# Create the DataFrame from the elbow data
df_elbow = pd.DataFrame(elbow_data)

# Review the DataFrame
df_elbow.head()

##--CODE--##
# Plot the DataFrame
df_elbow.plot.line(
    x="k", 
    y="inertia", 
    title="Elbow Curve", 
    xticks=k
)

### Step 5: Segment the `customers_pca_df`  DataFrame using the K-means algorithm.

##--CODE--##
# Define the model Kmeans model using the optimal value of k for the number of clusters.
model = KMeans(n_clusters=3, n_init='auto', random_state=0)

# Fit the model
model.fit(customers_pca_df)

# Make predictions
k_3 = model.predict(customers_pca_df)

# Create a copy of the customers_pca_df DataFrame
customer_pca_predictions_df = customers_pca_df.copy()

# Add a class column with the labels
customer_pca_predictions_df["customer_segments"] = k_3

##--CODE--##
# Plot the clusters
customer_pca_predictions_df.plot.scatter(
    x="PCA1",
    y="PCA2",
    c="customer_segments",
    colormap='winter')

### Step 6: Segment the `customers_transformed_df` DataFrame with all factors using the K-means algorithm

##--CODE--##
# Define the model Kmeans model using k=3 clusters
model = KMeans(n_clusters=3, n_init='auto', random_state=0)

# Fit the model
model.fit(customers_transformed_df)

# Make predictions
k_3 = model.predict(customers_transformed_df)

# Create a copy of the customers_transformed_df DataFrame
customers_transformed_predictions_df = customers_transformed_df.copy()

# Add a class column with the labels
customers_transformed_predictions_df["customer_segments"] = k_3

##--CODE--##
# Plot the clusters using the first any two feature columns
customers_transformed_predictions_df.plot.scatter(
    x="feature_1",
    y="feature_2",
    c="customer_segments",
    colormap='winter')

### Step 7. Which features have the strongest influence on each component? And, plot the most influencial features for each component. 


##--CODE--##
# Determine which feature has the stronger influence on each principal component. 
# Use the columns from the original DataFrame. 
pca_component_weights = pd.DataFrame(pca.components_.T, columns=['PCA1', 'PCA2'], index=customers_transformed_df.columns)
pca_component_weights

**Answer:** 
- "feature_6" and "feature_9" have the strongest positive influence on PCA1. 
- "feature_7" and "feature_10" have the strongest positive influence on PCA2, whereas "feature_6" has the strong negative influence on PCA2.

### Step 8: Create a scatter plot of the most influential features for each component and customer segments.

##--CODE--##
# Plot the features that are the most influencial for each component. 
customers_transformed_predictions_df.plot.scatter(
    x="feature_9",
    y="feature_7",
    c="customer_segments",
    colormap='winter')

##--CODE--##
# Plot the clusters using the most influencial features for each component. 
customers_transformed_predictions_df.plot.scatter(
    x="feature_9",
    y="feature_10",
    c="customer_segments",
    colormap='winter')

### Step 9: What is the difference between the segmentation results of the PCA DataFrame and most influential features for each component? 

**Answer:** It appears that the customer segmentation information using the DataFrame with "feature_9" and "feature_10" yields similar results that the PCA analysis. 

##--CODE--##



##--CODE--##
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler

##--CODE--##
# Load data
file_path = "https://static.bc-edx.com/ai/ail-v-1-0/m13/lesson_2/datasets/app-data.csv"
app_data = pd.read_csv(file_path)
app_data.head()

##--CODE--##
# Define features set
X = app_data.copy()
X.drop("Result", axis=1, inplace=True)
X.head()

##--CODE--##
# Define target vector
y = app_data["Result"]
y.head()

##--CODE--##
# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3)

##--CODE--##
# Create a StandardScaler() model and fit it to the training data
X_scaler = StandardScaler().fit(X_train)

##--CODE--##
# Transform the training and testing data by using the X_scaler model
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

# K-nearest neighbors

##--CODE--##
# Loop through different k values to find which has the highest accuracy.
# Note: We use only odd numbers because we don't want any ties.


    
# Plot the results
plt.plot(range(1, 20, 2), train_scores, marker='o', label="training scores")
plt.plot(range(1, 20, 2), test_scores, marker="x", label="testing scores")
plt.xlabel("k neighbors")
plt.ylabel("accuracy score")
plt.legend()
plt.show()

##--CODE--##
# Choose the best k, and refit the KNN classifier by using that k value.
# Note that k: 9 provides the best accuracy where the classifier starts to stablize


# Print the score for the test data.


##--CODE--##



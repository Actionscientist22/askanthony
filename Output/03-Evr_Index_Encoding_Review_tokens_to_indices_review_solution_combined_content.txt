##--CODE--##
# Import the dependencies
from keras.preprocessing.text import Tokenizer

##--CODE--##
# Define a list of sentences to tokenize. 
sentences = ["I love my dog.", "I love my family.", "My dog is a lab"]

##--CODE--##
# Create an instance of the Tokenizer
tokenizer = Tokenizer()
# Fit the tokenizer on the documents
tokenizer.fit_on_texts(sentences)

##--CODE--##
# Create a dictionary mapping of words to their indices
print(tokenizer.word_index)

##--CODE--##
# Encode each word in the text_sequences to the indices. 
sequences = tokenizer.texts_to_sequences(sentences)
sequences

##--CODE--##
# Regenerate the sentences from the indcies.
[sequence for sequence in tokenizer.sequences_to_texts_generator(sequences)]

##--CODE--##



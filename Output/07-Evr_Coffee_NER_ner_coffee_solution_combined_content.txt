##--CODE--##
# Import the dependencies
import spacy
from nltk.corpus import reuters
from spacy import displacy
from collections import Counter
import pandas as pd

# Load the small English language model for spacy
nlp = spacy.load("en_core_web_sm")

##--CODE--##
# Analyze a sentence using spacy
doc = nlp(u"""Patrick Mahomes is a quarterback for the Kansas City Chiefs in the American Conference, 
which is one of two conferences in the National Football League.""")

# Access the tagged entities with .text and .label_
[ent.text +" ---> "+ ent.label_ for ent in doc.ents]

##--CODE--##
# Get all the categories in the Reuters corpus. 
categories = reuters.categories()
print(categories)

##--CODE--##
# Locate and store a single article from the Reuters stories with the category "coffee".
article = reuters.raw(fileids = reuters.fileids(categories='coffee')[3])
print(article)

##--CODE--##
# Analyze the article with spacy
doc = nlp(article)

# Render NER visualization with displacy to determine entities for extraction
displacy.render(doc, style='ent')

##--CODE--##
# Store all Reuters articles with category "coffee".
articles = reuters.raw(categories='coffee')

# Analyze the articles with spaCy
doc = nlp(articles)

##--CODE--##
# Extract geopolitical "GPE" and organizational entities "ORG" using a list comprehension.
geo_org_entities = [ent.text for ent in doc.ents if ent.label_ in ['GPE', 'ORG']]

##--CODE--##
# Print the first 20 entities.
geo_org_entities[:20]

##--CODE--##
# Using a list comprehension convert each entity to lowercase and remove the newline character. 
entities = [i.lower().replace('\n','') for i in geo_org_entities]

# Print the entities
print(entities)

##--CODE--##
# Create a variable, most_freq_entities, that stores the most frequent entities 
# using the most_common() function from the Counter module.
most_freq_entities = Counter(entities).most_common()

# Print the first 10 most frequent entities
print(most_freq_entities[:10])

##--CODE--##
# Use list comprehensions to retrieve each entity and the number of occurrences for each entity in separate lists.
entity = [most_freq_entities[i][0] for i, _ in enumerate(most_freq_entities)]
frequency = [most_freq_entities[i][1] for i, _ in enumerate(most_freq_entities)]

##--CODE--##
# Create a DataFrame that has columns to hold each entity and the number of times each entity appears.
common_entities_df = pd.DataFrame(
    {
        'entity':entity,
        'frequency':frequency
    }
)

# Sort the DataFrame
common_entities_df.sort_values(by=['frequency'], ascending=False).reset_index(drop=True)

# Display the first ten rows. 
common_entities_df.head(10)

##--CODE--##
# Display the last ten rows. 
common_entities_df.tail(10)

##--CODE--##



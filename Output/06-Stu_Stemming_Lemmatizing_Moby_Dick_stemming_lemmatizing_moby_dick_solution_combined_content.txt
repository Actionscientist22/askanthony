##--CODE--##
# Import the WordNetLemmatizer class 
from nltk.stem import WordNetLemmatizer 
# Import the SnowballStemmer class
from nltk.stem.snowball import SnowballStemmer
# Import the word tokenizer
from nltk.tokenize import word_tokenize
# Import gutenberg and stopwords 
from nltk.corpus import gutenberg, stopwords
# Import regular expressions
import re

# Download "punkt" sentence tokenizer and "wordnet" that the lemmatizer uses.
import nltk
nltk.download('punkt')
nltk.download('wordnet')

##--CODE--##
# Instantiate the lemmatizer
lemmatizer = WordNetLemmatizer()
# Instantiate the Snowball Stemmer and pass in the "english" language parameter.
s_stemmer = SnowballStemmer(language='english')

##--CODE--##
# Read in the entire novel
book = gutenberg.raw(fileids=('melville-moby_dick.txt'))

# Use regular expression to get all the text after "CHATPER 1".
import re

match = re.search(r'CHAPTER 1.*', book, re.DOTALL)
if match:
    # Print the matched text and everything that follows
    moby_dick = match.group(0)
print(moby_dick)

##--CODE--##
# Write a function that processes the words in Moby Dick and gets the stem of the words.
def process_text_stemming(book):
    """
    Preprocesses a given text article by performing the following steps:
    
    1. Removes stopwords (common words in English language).
    2. Uses regular expressions to remove non-alphabet characters (e.g., punctuation).
    3. Tokenizes the cleaned text into words.
    4. Gets the stem of the word.
    5. Filters out words that are not stopwords.
    
    Parameters:
        article (str): The input text article to be processed.

    Returns:
        list of str: A list of preprocessed words from the input article.
    """
    # Get the stopwords
    sw = set(stopwords.words('english'))
    # Use regex to substitute everything that is not a letter with an empty string.
    regex = re.compile("[^a-zA-Z ]")
    re_clean = regex.sub(' ', book)
    # Tokenize the words 
    words = word_tokenize(re_clean)
    # Get the stem of the words
    stem = [s_stemmer.stem(word) for word in words]
    # Retrieve only the words that aren't in the stopwords.
    output = [word.lower() for word in stem if word.lower() not in sw]
    return output

##--CODE--##
# Print the processed book as a set. 
print(set(process_text_stemming(moby_dick)))

##--CODE--##
# Write a function that processes the words in Moby Dick and lemmatizes the words to their root words.
def process_text_lemmatizaton(book):
    """
    Preprocesses a given text article by performing the following steps:
    
    1. Removes stopwords (common words in English language).
    2. Uses regular expressions to remove non-alphabet characters (e.g., punctuation).
    3. Tokenizes the cleaned text into words.
    4. Lemmatizes the words to their base form.
    5. Filters out words that are not stopwords.
    
    Parameters:
        article (str): The input text article to be processed.

    Returns:
        list of str: A list of preprocessed words from the input article.
    """
    # Get the stopwords
    sw = set(stopwords.words('english'))
    # Use regex to substitute everything that is not a letter with an empty string.
    regex = re.compile("[^a-zA-Z ]")
    re_clean = regex.sub(' ', book)
    # Tokenize the words 
    words = word_tokenize(re_clean)
    # Lemmatize the words
    lem = [lemmatizer.lemmatize(word) for word in words]
    # Retrieve only the words that aren't in the stopwords.
    output = [word.lower() for word in lem if word.lower() not in sw]
    return output

##--CODE--##
# Print the processed book as a set. 
print(set(process_text_lemmatizaton(moby_dick)))

##--CODE--##
# Write a function that processes the words in Moby Dick and lemmatizes the adverbs to their root words.
def process_adv_lemmatizaton(book):
    """
    Preprocesses a given text article by performing the following steps:
    
    1. Removes stopwords (common words in English language).
    2. Uses regular expressions to remove non-alphabet characters (e.g., punctuation).
    3. Tokenizes the cleaned text into words.
    4. Lemmatizes the words to their verb form.
    5. Filters out words that are not stopwords.
    
    Parameters:
        article (str): The input text article to be processed.

    Returns:
        list of str: A list of preprocessed words from the input article.
    """
    # Get the stopwords
    sw = set(stopwords.words('english'))
    # Use regex to substitute everything that is not a letter with an empty string.
    regex = re.compile("[^a-zA-Z ]")
    re_clean = regex.sub(' ', book)
    # Tokenize the words 
    words = word_tokenize(re_clean)
    # Lemmatize the words
    lem = [lemmatizer.lemmatize(word,pos='r') for word in words]
    # Retrieve only the words that aren't in the stopwords.
    output = [word.lower() for word in lem if word.lower() not in sw]
    return output

##--CODE--##
# Print the processed book as a set. 
print(set(process_adv_lemmatizaton(moby_dick)))

##--CODE--##



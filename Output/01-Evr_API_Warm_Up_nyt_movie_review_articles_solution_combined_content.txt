##--CODE--##
# Import dependencies
import requests
import json
import pandas as pd
from dotenv import load_dotenv
import os
import time

# Load environment variables and New York Times API key
load_dotenv()
api_key = os.getenv("NYT_API_KEY")

# New York Times Article API URL
url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"

# Filter for movie reviews
# section_name should be "Movies"
# type_of_material should be "Review"
filter_query = 'section_name:"Movies" AND type_of_material:"Review"'

# Use a sort filter, sort by newest
sort = "newest"

# Select the following fields to return:
# headline, web_url, snippet, source, keywords, pub_date, byline, word_count
field_list = "headline,web_url,snippet,source,keywords,pub_date,byline,word_count"

# Search for movie reviews published between a begin and end date
# Select reviews published in July 2023
begin_date = "20230701"
end_date = "20230731"

# Build URL
query_url = (
    f"{url}api-key={api_key}&begin_date={begin_date}&end_date={end_date}"
    + f'&fq={filter_query}&sort={sort}&fl={field_list}'
)

##--CODE--##
# Get the response
response = requests.get(query_url)
response

##--CODE--##
# Retrieve reviews
reviews = response.json()

# Print results in JSON format
print(json.dumps(reviews, indent=4))

##--CODE--##
# Convert results list to JSON normalized Pandas DataFrame
reviews_df = pd.json_normalize(reviews["response"]["docs"])
reviews_df

##--CODE--##
# Get the unique writer bylines as a Series
writers = reviews_df["byline.original"].drop_duplicates()
writers

##--CODE--##
# Convert the writers Series to a list
writers_list = writers.to_list()
writers_list

##--CODE--##
# Use the writers list to find the most recent articles by the same writer

# Empty list for results
results_list = []

# loop through the writers_list
for writer in writers_list:
    # Set up the query
    query_url = f"{url}api-key={api_key}&byline:{writer}&sort{sort}&fl={field_list}"
    
    # Get the results
    results = requests.get(query_url).json()
    
    # Add a 12 second interval between queries to stay within API query limits
    time.sleep(12)

    # Use a try-except clause to collect results
    try:
        # Loop through the "docs"
        for doc in results["response"]["docs"]:
            # Save byline.original, headline.main, snippet,
            # and web_url
            results_list.append({
                "byline": doc["byline"]["original"],
                "headline": doc["headline"]["main"],
                "snippet": doc["snippet"],
                "web_url": doc["web_url"]
            })
        print(f"Found articles {writer}")
    except:
        print(f"No articles {writer} found")

##--CODE--##
# Convert the results to a Pandas DataFrame
results_df = pd.DataFrame(results_list)
results_df

##--CODE--##



WEBVTT

1
00:00:10.710 --> 00:00:15.079
Anthony Taylor: Well, that's the perfect segue into ethics.

2
00:00:15.230 --> 00:00:24.810
Anthony Taylor: Not really. But you know it's the only segue I come up with. They were talking during break someone can't say the name, at least. Where was scammed

3
00:00:25.130 --> 00:00:31.730
Anthony Taylor: by an Etsy person claiming to be an Etsy person. They weren't actually an Etsy person that

4
00:00:32.960 --> 00:00:37.550
Anthony Taylor: would be illegal for me to say it was an Etsy person. Was not an Etsy person

5
00:00:39.060 --> 00:00:52.130
Anthony Taylor: so welcome back, Anthony. I had a great day, thank you for asking those that did  I did thank you. and I am back hoping you guys had a great time.

6
00:00:52.360 --> 00:00:54.179
sonja baro: We missed you, though

7
00:00:54.690 --> 00:00:58.329
Anthony Taylor: I heard you know he's all of energy, this guy

8
00:00:58.580 --> 00:01:00.229
Masarirambi, Rodney: we're we're glad.

9
00:01:01.100 --> 00:01:06.900
Anthony Taylor: I did not bring you anything I did get homemade for me. I would share

10
00:01:07.410 --> 00:01:08.430
Anthony Taylor: if you want?

11
00:01:09.480 --> 00:01:14.030
Anthony Taylor: I could bring a homemade cookie in here?  yeah.

12
00:01:14.440 --> 00:01:16.730
Anthony Taylor: So let's continue.

13
00:01:17.280 --> 00:01:24.729
Anthony Taylor:  So what are we gonna do today? Well, today, we're gonna talk about AI laws and regulations. Now.

14
00:01:25.710 --> 00:01:34.460
Anthony Taylor: this is one of those sections that could be really long. But we're not going to let it. Okay. We're gonna get through it.

15
00:01:34.550 --> 00:01:46.010
Anthony Taylor: We're gonna talk about it. It's funny, me and the Tas, you know, those guys, Kevin James, we were talking pretty extensively about the whole ethics thing at the beginning of class.

16
00:01:46.040 --> 00:01:54.380
Anthony Taylor: And and I'm going to stick to my guns. What I said when Clayton asked last or day, one great topic. okay

17
00:01:54.880 --> 00:02:00.340
Anthony Taylor: in the workplace very unlikely to gonna come up more than

18
00:02:00.350 --> 00:02:12.000
Anthony Taylor: you're saying something like. hey? Wouldn't this be protected information? And then that gets sent off to legal, or whoever is in charge

19
00:02:12.220 --> 00:02:15.719
Anthony Taylor: of that topic, and they take care of it.

20
00:02:15.770 --> 00:02:17.750
Anthony Taylor: and then they come back, and they say, Yep.

21
00:02:17.900 --> 00:02:26.980
Anthony Taylor: you need to do something with that. And then real work for you guys be how to handle that data properly, something we don't talk about much.

22
00:02:27.260 --> 00:02:28.970
Anthony Taylor: Rodney. Question

23
00:02:31.550 --> 00:02:36.630
Masarirambi, Rodney: statement. Did you think that this wouldn't go long with us?

24
00:02:36.660 --> 00:02:42.160
Masarirambi, Rodney: And how we discuss things like, you know, this is gonna go the whole way. Right?

25
00:02:42.760 --> 00:02:49.270
Anthony Taylor: What? Oh, well, maybe. Well, okay. But I have a plan. Okay, I got it. You're right.

26
00:02:49.290 --> 00:02:51.469
Anthony Taylor: Maybe I don't know. I don't know.

27
00:02:51.480 --> 00:02:54.840
Anthony Taylor: I don't know. I think he even commented on.

28
00:02:54.880 --> 00:02:57.100
Raugewitz, Tania: He was taken aback. How vocal we are!

29
00:02:57.640 --> 00:03:09.009
Anthony Taylor: I love that. Oh, your, your new instructor, your tip instructor. Yeah, your your sub. I will have a sub. March 28. Also, I don't know if it'll be that guy. but I don't. I don't know. I'm gonna

30
00:03:09.020 --> 00:03:10.870
Anthony Taylor: take a week off for Easter.

31
00:03:11.430 --> 00:03:17.640
Anthony Taylor: Everybody just don't show up on March 20, eighth, and we'll all watch the record

32
00:03:18.020 --> 00:03:24.550
Anthony Taylor: transformers, too. It's actually a big AI duck. So it's a good one. You're gonna you're not gonna want this

33
00:03:24.760 --> 00:03:29.190
Anthony Taylor:  anyway.

34
00:03:30.110 --> 00:03:37.190
Anthony Taylor: So so just to be clear before we get into this. So we don't get all bored. And then I tell you all this stuff all right, hipaa

35
00:03:37.540 --> 00:03:38.710
Anthony Taylor: socks.

36
00:03:39.320 --> 00:03:45.050
Anthony Taylor: And now the AI wants gdpr, the whatever the heck they call a California one.

37
00:03:45.220 --> 00:03:54.020
Anthony Taylor:  all of these are are the kind of things that when you're working with the data, you may come across data and go.

38
00:03:55.350 --> 00:03:58.739
Anthony Taylor: And I mean, it's kind of easy notices data. First.

39
00:03:59.110 --> 00:04:04.030
Anthony Taylor: let me give you some hints. This has people's names in it. Yeah.

40
00:04:05.500 --> 00:04:13.559
Anthony Taylor: this is people's social security numbers. Yeah. phone numbers even sometimes can be worth looking into

41
00:04:13.580 --> 00:04:17.640
Anthony Taylor: addresses if it's in the same row as their name

42
00:04:17.790 --> 00:04:20.040
Anthony Taylor: could be something you need to look into.

43
00:04:20.690 --> 00:04:26.550
Anthony Taylor: But you're going to get accustomed to that. The real challenge is going to be when Legal says.

44
00:04:26.700 --> 00:04:27.530
Anthony Taylor: Hey.

45
00:04:28.190 --> 00:04:31.439
Anthony Taylor: can't have that in there. You got to do something with it.

46
00:04:31.930 --> 00:04:32.980
Anthony Taylor: Okay.

47
00:04:33.300 --> 00:04:42.360
Anthony Taylor: that's the real challenge. Now. I will try to get us to something. We're not getting to it this week. but masking data.

48
00:04:42.640 --> 00:04:45.850
Anthony Taylor: masking is a big one. Encoding data is a big one.

49
00:04:45.980 --> 00:04:48.020
Anthony Taylor: things like that. So

50
00:04:48.810 --> 00:04:59.109
Anthony Taylor: here's the both the good and the bad is anonymizing data. There's libraries that will help you with that. So we're not going to worry too much. But understand, that's what's going to come of this.

51
00:05:00.240 --> 00:05:13.749
Anthony Taylor: Now. The other thing that could come out the last 3 days of lessons is you might get asked some of these questions at a job interview. Okay, if you're applying at a healthcare It company.

52
00:05:13.780 --> 00:05:16.860
Anthony Taylor: they will probably ask you if you've ever heard of hipaa

53
00:05:17.230 --> 00:05:20.920
Anthony Taylor: guarantee. Everyone in this room has heard of hipaa.

54
00:05:21.410 --> 00:05:25.320
Anthony Taylor: You've even signed documents that say, you understand what it means.

55
00:05:25.550 --> 00:05:27.419
Anthony Taylor: Okay, you probably don't.

56
00:05:28.320 --> 00:05:29.270
Anthony Taylor: But

57
00:05:29.500 --> 00:05:33.039
Anthony Taylor: that's okay. Unless you're like a medical person like sign.

58
00:05:33.080 --> 00:05:35.830
Anthony Taylor: She probably gets it. Okay. But

59
00:05:35.870 --> 00:05:39.139
Anthony Taylor:  it's okay. You just need to know

60
00:05:39.320 --> 00:05:43.570
Anthony Taylor: basically what it is and what to look for. To make sure you're not violating it

61
00:05:43.900 --> 00:05:48.149
Anthony Taylor: for your company. Bring it to their attention. Life moves on.

62
00:05:48.330 --> 00:05:52.949
Anthony Taylor: Today, we're going to talk about some legal oh.

63
00:05:55.190 --> 00:06:05.690
Anthony Taylor: okay, some legal stuff that's going on all over the world. But these are actual laws or regulations that are either in process

64
00:06:05.870 --> 00:06:11.109
Anthony Taylor: or already set. Okay, and we're gonna talk. I have opinions about these.

65
00:06:11.530 --> 00:06:16.380
Anthony Taylor: Some of them are very strongly for, and some of them are very strongly like whatever.

66
00:06:17.090 --> 00:06:18.859
Anthony Taylor: Okay? And I'd love to hear y'all.

67
00:06:20.540 --> 00:06:27.180
Anthony Taylor: So we're going to summarize. Why, different types of organizations, such as governance tech companies and research institutions want to regulate AI.

68
00:06:28.180 --> 00:06:41.160
Anthony Taylor: It's gonna be a question coming up. We're gonna review Us. Regulations for data collection and uses list key regional laws. He, like 3 other. describe major regional law, can impact

69
00:06:41.560 --> 00:06:51.530
Anthony Taylor: system. summarize how and why different organizations are developing regulations that specifically targeted AI and discuss the importance of AI

70
00:06:51.800 --> 00:06:58.369
Anthony Taylor:  and see how that talk about how that may affect our futures in AI.

71
00:06:58.930 --> 00:07:06.050
Anthony Taylor: Okay? And this demo is like what demo there's no demo. It's just words. So

72
00:07:07.110 --> 00:07:09.359
Anthony Taylor: laws and regulations impacting. So

73
00:07:10.600 --> 00:07:13.720
Anthony Taylor: the laws and regulations that we're going to run into

74
00:07:14.140 --> 00:07:19.540
Anthony Taylor: are all about achieving the following goals, okay, protecting privacy.

75
00:07:19.590 --> 00:07:22.509
Anthony Taylor: We got to secure your data,

76
00:07:22.720 --> 00:07:27.690
Anthony Taylor: and and this one's crazy transparency and algorithmic decision making

77
00:07:28.120 --> 00:07:32.390
Anthony Taylor: and requiring human intervention.

78
00:07:34.600 --> 00:07:36.230
Anthony Taylor: Anybody have thoughts on that.

79
00:07:42.700 --> 00:07:45.839
Anthony Taylor: On that last one on any.

80
00:07:46.510 --> 00:07:50.300
sonja baro: I mean, these 2 seem pretty. They're pretty broad. They don't mean anything

81
00:07:50.650 --> 00:07:56.879
Anthony Taylor: right. I mean, they do. They mean a lot, but they're so broad it could be a billion things right? These 2 are interesting to me.

82
00:07:57.000 --> 00:08:02.109
Anthony Taylor: But even with what you guys know now is this something that we can accomplish

83
00:08:03.560 --> 00:08:05.449
sonja baro: not a hundred percent.

84
00:08:05.920 --> 00:08:07.700
Anthony Taylor: And and as you do.

85
00:08:09.960 --> 00:08:11.509
Anthony Taylor: I know there's some.

86
00:08:12.130 --> 00:08:13.749
sonja baro: Go ahead. I think you could.

87
00:08:14.540 --> 00:08:20.870
Masarirambi, Rodney: What do they mean by the option of human intervention. Like, I think that's like so broad.

88
00:08:22.230 --> 00:08:25.959
sonja baro: So they're basically somebody going in and saying that it's wrong.

89
00:08:26.220 --> 00:08:47.789
sonja baro: right? So I can give an example of that, Rodney, where? So in healthcare, especially, right, there's been a lot of focus on how can you drive improvement and care and outcomes through the use of technology. AI is one of them. The challenges is, AI will go off of all of the data that's in the system. And

90
00:08:47.790 --> 00:09:13.199
sonja baro: guess what the data in the system doesn't have specific information on. For instance, certain population sets, or it may make a recommendation that is not a good fit for you and for your based on what you need, and you really need at least to have some kind of checkpoint before something gets prescribed to you that shouldn't, or a procedure or something like that.

91
00:09:15.140 --> 00:09:18.299
sonja baro: So that's that's one I brought would bring up.

92
00:09:18.320 --> 00:09:19.370
Anthony Taylor: I liked it.

93
00:09:19.480 --> 00:09:20.950
Anthony Taylor: I like it, Meredith.

94
00:09:21.990 --> 00:09:24.760
Meredith McCanse (she/her): Is this strictly related to.

95
00:09:24.930 --> 00:09:35.699
Meredith McCanse (she/her): Well, okay, so I'm wondering if it's data. At least you don't have to stick to AI stick to data. Okay? Well, I guess I'm a little. The first thing that came to mind to me was.

96
00:09:36.080 --> 00:09:50.930
Meredith McCanse (she/her): I thought of it as the option of human intervention, not the or, anyway, I thought of that was some sort of whatever Max Jumbo, Jet 7, 37, or whatever that had a thing that a certain thing created it, or it created a

97
00:09:51.030 --> 00:09:57.880
Meredith McCanse (she/her): a bunch of them crashed and a whole bunch of people died. And they realized it was part of this automated system. And

98
00:09:58.310 --> 00:10:11.699
Meredith McCanse (she/her): it made me think of like systems that are have some sort of autopilot with humans are not able to override it. And then something goes wrong. And things like that crash like is this saying things like autopilot or self-driving cars?

99
00:10:11.810 --> 00:10:15.400
Meredith McCanse (she/her): have to have the option for the human to step in and intervene.

100
00:10:15.450 --> 00:10:23.659
Anthony Taylor: and that is, that is required. By the way, there, there's one car. They tried to leave release without a steering wheel, and it didn't go. Well.

101
00:10:23.800 --> 00:10:38.090
Meredith McCanse (she/her): yeah. And I know the self driving cars are fairly new, but when they and they are sort of like a it's like a hybrid self driving like you can let go of the wheel, and then you can use it again. But if they get to a point where, like you said, there's no steering wheel at all, or there's no human intervention.

102
00:10:38.330 --> 00:10:40.049
Meredith McCanse (she/her): I think that's something different.

103
00:10:40.570 --> 00:10:47.680
Meredith McCanse (she/her): And I would argue for the I would argue in that case for the validity of requiring the option for human intervention.

104
00:10:48.290 --> 00:10:54.079
Anthony Taylor: And I, and I think, when it comes to things like automated cars or auto pilot things like that absolutely.

105
00:10:55.210 --> 00:10:56.610
Anthony Taylor: I believe

106
00:10:58.380 --> 00:11:01.550
Anthony Taylor: this statement is intended to be towards

107
00:11:02.670 --> 00:11:16.349
Anthony Taylor:  the Llms and stuff like that. But they're saying, that we need a way to be able to go in there and add human intervention to it

108
00:11:16.660 --> 00:11:20.200
Anthony Taylor: which they've tried, and there's ways they work on that

109
00:11:20.440 --> 00:11:23.450
Anthony Taylor:  boom!

110
00:11:25.020 --> 00:11:34.680
Anthony Taylor: Figures. I finish my break, and then I get all choked up. So all of this stuff affects everybody and let me and none of it affects anybody.

111
00:11:34.840 --> 00:11:46.110
Anthony Taylor: Okay, all of these different areas can be applied to the bill. The main thing. So your data industry, the location as in what your graphical location you're in

112
00:11:46.150 --> 00:11:49.690
Anthony Taylor: and who you work with.

113
00:11:50.080 --> 00:12:00.700
Anthony Taylor: So just because you're in Florida. If you do things with people in Europe. You now have to follow the regulations in Florida and regulations in Europe.

114
00:12:01.870 --> 00:12:05.249
Anthony Taylor: Okay? Which is why a lot of people ate California.

115
00:12:06.720 --> 00:12:12.309
Anthony Taylor: California has the strictest privacy laws in the United States

116
00:12:12.880 --> 00:12:15.219
Anthony Taylor: companies. Let California

117
00:12:16.200 --> 00:12:30.149
Anthony Taylor: because it was just too much. It's like, it's just not worth it. Okay? And as long as you don't directly interact, you can have California customers and not have to worry about interacting. Okay. But if you do business on California

118
00:12:30.470 --> 00:12:31.330
Anthony Taylor: soil.

119
00:12:31.730 --> 00:12:38.750
Anthony Taylor: Okay, you are geographically in California. You have to hear all these additional regulations.

120
00:12:38.900 --> 00:12:39.760
Anthony Taylor: Period.

121
00:12:40.700 --> 00:12:43.889
Anthony Taylor: Okay? Which is why everybody moves to Texas, Texas. They go.

122
00:12:43.990 --> 00:12:48.860
Anthony Taylor: There's a plot of land built. So that's what they they'll charge you a dime. Just give it to you.

123
00:12:49.390 --> 00:12:50.120
Okay.

124
00:12:50.690 --> 00:12:52.460
Anthony Taylor: probably Colorado, too, right?

125
00:12:54.170 --> 00:12:54.910
Anthony Taylor: No.

126
00:12:55.780 --> 00:12:57.700
Raugewitz, Tania: not near boulder. Anyways.

127
00:12:57.740 --> 00:13:10.620
Anthony Taylor: we don't. We don't have nearly as I had a guy. I'll take the story on regulations and how regulations can hurt you. Right. This guy sat next to me on airplane, and he was, Oh, we're gonna do

128
00:13:11.050 --> 00:13:15.920
Anthony Taylor: Well, I build these. Stop! Stop! I go.

129
00:13:16.210 --> 00:13:19.629
Anthony Taylor: But because the fuel tanks on this plane. I built them

130
00:13:19.650 --> 00:13:20.700
Anthony Taylor: all of them.

131
00:13:20.870 --> 00:13:25.109
Anthony Taylor: every plane you see, as far as you can see, I built them all my company.

132
00:13:25.750 --> 00:13:29.790
Anthony Taylor: I go. Oh, that's cool. And I said, So who?

133
00:13:29.860 --> 00:13:32.510
Anthony Taylor: Since you're heading home, we're going to Orange County.

134
00:13:32.680 --> 00:13:36.450
Anthony Taylor: and he goes, yeah, but Texas is my new home.

135
00:13:37.280 --> 00:13:43.409
Anthony Taylor: Why is that he goes. They've literally changed the regulations to the point where I can note longer.

136
00:13:43.640 --> 00:13:51.459
Anthony Taylor: I've been in that and in State of California for 40 years, building these days. and I can no longer build a single tank in state health.

137
00:13:52.340 --> 00:13:59.449
Anthony Taylor: So I have no choice. I have to come. And then we got talking about it. And they did. They basically paid in both the coming

138
00:13:59.630 --> 00:14:02.019
Anthony Taylor: coming. Come on, it's okay.

139
00:14:02.830 --> 00:14:05.600
Anthony Taylor: all of his employees, all of his business.

140
00:14:06.290 --> 00:14:11.130
Anthony Taylor: much text. And then suddenly he was making more money. Figure it out.

141
00:14:11.480 --> 00:14:14.669
Anthony Taylor: So yeah, regulations are good.

142
00:14:14.820 --> 00:14:21.959
Anthony Taylor: but they also have to make sense for the common good of everybody. Now you could say, well, California was making regulations for the common good of the universe

143
00:14:22.180 --> 00:14:23.550
Anthony Taylor: of the globe.

144
00:14:24.630 --> 00:14:25.390
Anthony Taylor: True.

145
00:14:26.110 --> 00:14:30.439
Anthony Taylor: okay, we're not the number one creator of smog and pollution, are we?

146
00:14:31.970 --> 00:14:33.400
Anthony Taylor: There's another country

147
00:14:33.810 --> 00:14:41.340
Anthony Taylor: way, more like a hundred times more, with no regulations and no intentions of making any regulations

148
00:14:42.870 --> 00:14:45.910
Clayton Graves: that doesn't give us free license to

149
00:14:45.960 --> 00:14:57.050
sonja baro: relax our restrictions. Maybe we're not number one because of our restrictions. I thought I thought we had a really good presentation on this.

150
00:14:57.320 --> 00:15:03.039
Anthony Taylor: I love it. Yes, I believe we did

151
00:15:03.130 --> 00:15:07.339
michael mcpherson: that. Look at what that country's done in the same time. Look at what we haven't.

152
00:15:08.970 --> 00:15:10.100
Anthony Taylor: That's true, too.

153
00:15:10.170 --> 00:15:15.820
Anthony Taylor: Everybody knows what country we're talking about right over regulated China is the one China has more and than like

154
00:15:15.910 --> 00:15:19.450
Anthony Taylor: the US. By like like 1020 times as much.

155
00:15:19.850 --> 00:15:26.200
Anthony Taylor: Okay and they did. And there's as far as I know, I don't know. I've heard anything and everything I've ever read

156
00:15:26.250 --> 00:15:32.129
Anthony Taylor: says that they have no intention of putting any kind of regulations in place. so they're going to continue doing it.

157
00:15:32.320 --> 00:15:38.099
Anthony Taylor: And you know, so whatever. But that doesn't mean to me that's not like I'm not trying to condemn them

158
00:15:38.190 --> 00:15:45.239
Anthony Taylor: right. I'm simply pointing out they opted to to make their country.

159
00:15:45.510 --> 00:15:48.190
Anthony Taylor: Manufacturer-friendly

160
00:15:48.570 --> 00:15:55.819
Anthony Taylor: business friendly if you will. Okay. I don't know. Is that right? Is that wrong? Not my place. To say

161
00:15:55.910 --> 00:16:07.250
michael mcpherson: their end goal is to become the senior currency to which all foreign trade is is set up. Economics. That's right, economic man. He's gonna tell us.

162
00:16:07.550 --> 00:16:09.930
Anthony Taylor: that's true, though. I mean, I'm there.

163
00:16:10.080 --> 00:16:16.680
Anthony Taylor: I'm all II. You know my my kids, Mike. I'm too old to have to worry about ramifications, so I'm good.

164
00:16:16.920 --> 00:16:18.819
Anthony Taylor: I'll keep shopping at T.

165
00:16:19.300 --> 00:16:27.710
Masarirambi, Rodney: Alright. So the other thing, also that China doesn't necessarily like them. They don't necessarily want to worry about

166
00:16:27.880 --> 00:16:35.170
Masarirambi, Rodney: like introducing regulation, because they also need to be able to spy on own people. So it's like, kind of

167
00:16:35.190 --> 00:16:39.889
Anthony Taylor: now, a whole nother area brandy. I'm just staying away from that for the moment.

168
00:16:40.080 --> 00:16:44.229
Masarirambi, Rodney: Make a stand.

169
00:16:46.110 --> 00:16:51.630
Anthony Taylor: Okay? Oh, I should go get my little. I bought little hands.

170
00:16:51.880 --> 00:16:54.459
Anthony Taylor: little teeny, bitchy hands. They're so fun.

171
00:16:54.520 --> 00:17:04.710
Anthony Taylor: Do trump with the little games, anyway. Okay, anyway, major area regulation deal outdated by potentially shared.

172
00:17:04.920 --> 00:17:17.380
Anthony Taylor: Okay, so this is your data security. And and I'll tell you how this manifests who we is when I'm architecting a system. I have to ensure that not only

173
00:17:17.589 --> 00:17:23.350
Anthony Taylor: is this system functional and high performing, but that only the people who I know

174
00:17:23.569 --> 00:17:34.110
Anthony Taylor: are supposed to have access to the data can, in fact, get to the data. If anyone comes to me on any given day and says who can see this.

175
00:17:34.570 --> 00:17:40.729
Anthony Taylor: and I either need to know or be able to bring up a very quick report.

176
00:17:40.970 --> 00:17:44.590
Anthony Taylor: That says this is, who can see it? And if I'm wrong.

177
00:17:44.600 --> 00:17:45.639
Anthony Taylor: I fail.

178
00:17:46.920 --> 00:17:47.860
Anthony Taylor: Okay.

179
00:17:48.140 --> 00:17:55.429
Anthony Taylor:  so it's it's important that you're able to do that. And an organization is able to do that.

180
00:17:55.810 --> 00:18:06.079
Anthony Taylor: The other thing is is. And this is particularly true for public companies or any companies dealing with private data is, if you have any kind of rage.

181
00:18:06.250 --> 00:18:09.160
Anthony Taylor: even if it's a suspected breach.

182
00:18:10.310 --> 00:18:14.299
Anthony Taylor: it's like, well, we accidentally left port open to the entire world.

183
00:18:14.620 --> 00:18:18.290
Anthony Taylor: Okay, was only open for 5 min, but

184
00:18:19.130 --> 00:18:20.490
Anthony Taylor: someone

185
00:18:20.590 --> 00:18:23.849
Anthony Taylor: could in that 5 min have have got to our date.

186
00:18:24.200 --> 00:18:26.769
Anthony Taylor: Okay, if that happens.

187
00:18:26.780 --> 00:18:36.169
Anthony Taylor: you have to report it does all of it go to the news. Only only if you're you know, not not getting along with the news media, does it show up?

188
00:18:36.720 --> 00:18:39.920
Anthony Taylor: And I know that sounds political. Kind is

189
00:18:40.000 --> 00:18:41.220
Anthony Taylor: kind of isn't

190
00:18:41.730 --> 00:18:52.900
Anthony Taylor: okay because it's true. It doesn't matter how big or small it is. If you pissed off the media you will be reporting and you will see it on the news. Okay, but I've heard of big ones

191
00:18:53.120 --> 00:18:58.540
Anthony Taylor: that you've never heard of. I've heard of small ones. It's like, why would they report that? Who cares?

192
00:18:59.470 --> 00:19:03.049
Anthony Taylor: It just depends. So unfortunately, it's used as a weapon.

193
00:19:03.520 --> 00:19:08.369
Anthony Taylor:  The good news is how many of you could tell me?

194
00:19:10.320 --> 00:19:13.229
Anthony Taylor: I don't know. 2 breaches in the last 5 years.

195
00:19:17.060 --> 00:19:18.259
Anthony Taylor: Clayton. Can

196
00:19:19.800 --> 00:19:25.509
Anthony Taylor: Sonya ken Ronnie can. Okay. Good. Rodney. Tell me a breach

197
00:19:27.500 --> 00:19:28.980
Anthony Taylor: in the last 5 years?

198
00:19:29.700 --> 00:19:35.630
Masarirambi, Rodney: I mean, technically. Well, would you count the Cambridge analytics one as a breach.

199
00:19:36.440 --> 00:19:38.790
Anthony Taylor: maybe what was released?

200
00:19:39.560 --> 00:19:41.319
Masarirambi, Rodney: It was.

201
00:19:42.810 --> 00:19:48.109
Masarirambi, Rodney: Was it privacy, data, or was it intellectual data

202
00:19:48.200 --> 00:19:55.590
Masarirambi, Rodney: privacy data like? Well, it was the the thing the scraping of like people's.

203
00:19:55.700 --> 00:20:07.429
Masarirambi, Rodney: I mean, it wasn't really a breach is was targeted, cause it was like to try get like find out about the voting the the database of voting voters for the for the trump campaign

204
00:20:07.530 --> 00:20:08.910
Masarirambi, Rodney: and stuff. Okay?

205
00:20:09.320 --> 00:20:11.080
Masarirambi, Rodney: Hi, oh, that's good.

206
00:20:11.380 --> 00:20:19.210
Anthony Taylor: No, that's good. That's that's that's exactly what I was looking for. Okay, and well, we'll do all 3 of the people that volunteer, Clayton.

207
00:20:23.080 --> 00:20:24.380
Clayton Graves: Well.

208
00:20:24.660 --> 00:20:32.370
Clayton Graves: there was a breach at Rockstar, where they lost a whole bunch of data pertaining to Grand Theft auto 6.

209
00:20:33.430 --> 00:20:39.840
Clayton Graves: Okay. But that's not a was that a privacy breach? I guess it was, it was data that right?

210
00:20:40.310 --> 00:20:42.679
Clayton Graves: The data breach?

211
00:20:43.750 --> 00:20:44.970
Oh.

212
00:20:45.790 --> 00:20:49.739
Anthony Taylor: alright, that's good. I uploaded

213
00:20:50.070 --> 00:20:59.680
Clayton Graves: I uploaded a file to get that contained a unencrypted password and

214
00:21:00.860 --> 00:21:05.899
Clayton Graves: expose the password to anybody that had access to our get repository.

215
00:21:06.510 --> 00:21:09.219
Anthony Taylor: Did you have to report it? Did the company have to report it?

216
00:21:09.290 --> 00:21:11.189
Clayton Graves: I report it. That's not fun.

217
00:21:11.200 --> 00:21:18.959
Anthony Taylor: Well, I mean, like to the public, to the government agencies. It was a government agency that I worked for. Ouch.

218
00:21:19.470 --> 00:21:24.519
Clayton Graves: okay, did it get reported outside of the government agency. I assume it did.

219
00:21:25.760 --> 00:21:28.109
Clayton Graves: but never made the news. Huh!

220
00:21:28.580 --> 00:21:31.189
Anthony Taylor: Darn it! Hi, Sonya, your turn?

221
00:21:31.730 --> 00:21:39.940
sonja baro: Yeah, I have a local one and then a national local. My health system had to report a data breach. Which

222
00:21:40.020 --> 00:21:52.369
sonja baro: wasn't. You know? What's what are you gonna do? And then the second one. I thought Equifax had a big ole a couple of years ago.

223
00:21:53.720 --> 00:22:01.059
sonja baro: Somebody big had one cause like target and others target was one, too. They had the data breach as well.

224
00:22:04.210 --> 00:22:05.989
Anthony Taylor: Okay, so

225
00:22:06.400 --> 00:22:07.220
sonja baro: be

226
00:22:07.870 --> 00:22:10.230
Anthony Taylor: I love that. You guys volunteered. Thank you.

227
00:22:10.700 --> 00:22:18.110
Anthony Taylor: Didn't mean to put you out the spot. But just so you guys know, statistic statista com

228
00:22:18.500 --> 00:22:23.760
Anthony Taylor: reports. There were a few data breaches reported last year.

229
00:22:25.000 --> 00:22:28.159
Anthony Taylor: We almost had 3 people. Remember 2.

230
00:22:29.900 --> 00:22:32.410
Anthony Taylor: There were 3 1,000,

231
00:22:32.730 --> 00:22:36.810
Anthony Taylor: 205 data breaches last year, reported.

232
00:22:38.520 --> 00:22:43.680
Anthony Taylor: okay, this is why the data breach thing is an interesting topic to me. Right?

233
00:22:43.970 --> 00:22:52.470
Anthony Taylor: It's only lasts as long as it does on the news. Oh, my God, sell and sell it in a database unless it directly affected you.

234
00:22:53.740 --> 00:22:55.500
Anthony Taylor: he probably already forgot about.

235
00:22:56.120 --> 00:22:58.669
Anthony Taylor: Okay. Now I will tell you.

236
00:22:58.960 --> 00:23:02.659
Anthony Taylor: most likely, how many of you guys have dark web monitoring

237
00:23:04.460 --> 00:23:07.159
Anthony Taylor: anybody. Tanya does. That's smart.

238
00:23:07.650 --> 00:23:13.820
Anthony Taylor: Okay, do you ever get pinged, Tanya, that your stuff's out there. Yeah. Happens to all of us. I guarantee most of your stuff.

239
00:23:14.390 --> 00:23:20.780
Anthony Taylor: Okay, but that's where it ends up. And only the people out there buying it.

240
00:23:21.000 --> 00:23:23.149
Anthony Taylor: you know, or they really really want it. Yeah.

241
00:23:23.380 --> 00:23:27.629
Anthony Taylor: which kind of goes back to what we were talking about the other day. The expectation of privacy.

242
00:23:29.020 --> 00:23:30.330
Anthony Taylor: we.

243
00:23:31.090 --> 00:23:35.519
Anthony Taylor: And from the what generation am I for baby boomers? I guess

244
00:23:35.840 --> 00:23:40.419
Anthony Taylor: I don't know. 60 s. Whatever that was. That's I was born late 60. S,

245
00:23:40.580 --> 00:23:47.500
Anthony Taylor: yeah, whatever it was, there we go. You're genetics. And from 1960 I'm Genet

246
00:23:47.540 --> 00:23:53.609
Anthony Taylor: 1,966. You said late sixties. You're a boomer.

247
00:23:54.000 --> 00:24:00.939
sonja baro: I don't wanna be Jenx. I'm gonna quit. Oh, we're so fun. We're the best.

248
00:24:01.580 --> 00:24:03.799
Anthony Taylor: anyway. A lot of us

249
00:24:03.820 --> 00:24:08.110
Anthony Taylor: still have expectations, privacy. We were brought up with privacy.

250
00:24:08.520 --> 00:24:15.910
Anthony Taylor: Okay, we were brought up where we didn't know people in the next neighborhood unless we happen to wander over there

251
00:24:16.700 --> 00:24:20.349
Anthony Taylor: as opposed to the next Gen. Or 2.

252
00:24:20.440 --> 00:24:26.470
Anthony Taylor: They knew everybody everywhere today. It's very likely every single person

253
00:24:27.480 --> 00:24:29.600
Anthony Taylor: in this room, and that, you know.

254
00:24:30.120 --> 00:24:32.570
Anthony Taylor: know somebody in another country.

255
00:24:33.690 --> 00:24:35.769
Anthony Taylor: Imagine that when we were kids.

256
00:24:36.530 --> 00:24:43.179
Anthony Taylor: if we didn't travel or someone didn't travel to us, what were the odds? We knew somebody in another country

257
00:24:44.610 --> 00:24:48.840
Anthony Taylor: pen pals items. I'm sure they were a real thing, not for me.

258
00:24:49.160 --> 00:24:51.750
Anthony Taylor: because Chat Gpt wasn't around to write the letters.

259
00:24:52.870 --> 00:24:54.840
Anthony Taylor: So

260
00:24:55.210 --> 00:24:57.750
Anthony Taylor: anyway, okay, so enough of that.

261
00:24:57.820 --> 00:25:01.959
Anthony Taylor: Let's keep going. We said we'd be done early, you guys, because you guys talked

262
00:25:02.530 --> 00:25:07.010
Anthony Taylor: privacy. So this kind of goes back to what we just talked right?

263
00:25:07.690 --> 00:25:15.870
Anthony Taylor: the the idea behind regulations. Privacy is to say, how can this data be shared? Not just did it get released by accident.

264
00:25:16.720 --> 00:25:25.529
Anthony Taylor: But how can it be shared if you put your name or your photo or anything on the Internet

265
00:25:26.170 --> 00:25:29.700
Anthony Taylor: who is allowed to share it? If it's on Facebook.

266
00:25:31.470 --> 00:25:36.789
Anthony Taylor: that's tough one. Okay, Facebook has all kinds of levels of security in it.

267
00:25:37.110 --> 00:25:41.250
Anthony Taylor: But if you post, if you when you go to do that post, if you quit public.

268
00:25:42.780 --> 00:25:51.010
Anthony Taylor: You've lost all rights to whatever you just posted. There is no privacy. Once you push public. If you push friends.

269
00:25:51.570 --> 00:25:54.700
Anthony Taylor: you can argue that there's some. But if one of them posts public

270
00:25:56.000 --> 00:25:57.270
Anthony Taylor: changes things.

271
00:25:57.310 --> 00:26:08.129
Anthony Taylor: Okay, now, I do know. And you guys probably have seen this. If you like, post something to your friends, and then they post it public, and somebody who's not your friend goes to look at it. They won't see your part of the post.

272
00:26:08.770 --> 00:26:14.920
Anthony Taylor: Okay, that does happen. That's how they address this. which they do. A pretty good job.

273
00:26:15.670 --> 00:26:19.059
Anthony Taylor: Anything that you put on the Internet

274
00:26:19.370 --> 00:26:20.830
Anthony Taylor: in a

275
00:26:21.910 --> 00:26:29.309
Anthony Taylor: like when you put your name and stuff you, it requires consent. Well, the scenario, I just say, did you give consent?

276
00:26:30.560 --> 00:26:37.359
Anthony Taylor: You posted a picture. and you set it to public when you said, This is me and my

277
00:26:37.410 --> 00:26:43.470
Anthony Taylor: child Susie, who at Disneyland on Valentine's day

278
00:26:46.660 --> 00:26:49.809
Anthony Taylor: did you give it? Did you give Facebook consent

279
00:26:49.940 --> 00:26:54.339
Anthony Taylor: to use your child's picture and your picture.

280
00:26:56.790 --> 00:26:59.310
Anthony Taylor: What do you guys think officially

281
00:27:00.200 --> 00:27:04.490
Clayton Graves: by posting it by uploading it to

282
00:27:04.750 --> 00:27:06.989
Clayton Graves: Facebook, you're giving consent.

283
00:27:07.280 --> 00:27:21.609
Anthony Taylor: Well, by opting it to Facebook and setting it to public. You have given them consent. If you would have did friends or or private, then technically, no. But you have given them a hundred percent consent to share any way they feel like

284
00:27:22.190 --> 00:27:23.989
Anthony Taylor: if you post the public.

285
00:27:24.330 --> 00:27:32.379
Clayton Graves: that's true. But if you look at the the user agreement, they? They re, they basically say, if you upload a picture.

286
00:27:33.180 --> 00:27:34.610
Clayton Graves: it's ours.

287
00:27:34.670 --> 00:27:40.109
Anthony Taylor: They would get killed if they if they did that. But you're right. It's I'm sure. Well, you know what

288
00:27:40.420 --> 00:27:45.560
Anthony Taylor: I don't know right on that, because I have read it. How many of you have read it other than Clayton Clayton's? Read it

289
00:27:45.690 --> 00:27:46.719
Anthony Taylor: anybody else?

290
00:27:47.850 --> 00:27:51.469
Anthony Taylor: How many of you have ever read? You remit that? You went click

291
00:27:51.810 --> 00:27:58.590
Anthony Taylor: on the Internet? Let's keep it limited to the Internet on the Internet. Curry has curry is a hardcore

292
00:27:58.690 --> 00:28:07.799
Anthony Taylor: Jennifer. See, I can see Jennifer doing it, too, because she's a statistics person. But yeah, right, it's just not very common that we're gonna be adults.

293
00:28:08.040 --> 00:28:10.290
Anthony Taylor: anyway. So it's partly our own fault.

294
00:28:10.420 --> 00:28:18.550
Anthony Taylor:  yeah. And so so let's stick to this data regulations. They have to set standards so that customers understand

295
00:28:19.310 --> 00:28:20.830
Anthony Taylor: how their data will be used

296
00:28:21.420 --> 00:28:24.600
Anthony Taylor: again. It's one of those big, long

297
00:28:24.730 --> 00:28:29.200
Anthony Taylor: takes you 20 min of legal ease to read it.

298
00:28:29.580 --> 00:28:32.069
Anthony Taylor: but it's there somewhere

299
00:28:33.420 --> 00:28:34.650
Anthony Taylor: on that website.

300
00:28:35.040 --> 00:28:40.599
Anthony Taylor: The cool thing is you can actually use with co-pilot. You can actually go in there and ask it to summarize it for you.

301
00:28:41.160 --> 00:28:42.110
Anthony Taylor: it would.

302
00:28:43.850 --> 00:28:53.039
Anthony Taylor: So here's where it starts getting even more interesting automated decision making this. So the the biggest one ironing process is one everyone loves to use.

303
00:28:53.210 --> 00:28:59.630
Anthony Taylor: Okay, loan processing. They love to use it there, too. Hiring processing seems a little more interesting.

304
00:28:59.760 --> 00:29:03.680
Anthony Taylor: But I don't know if we talked about the one famous case.

305
00:29:04.020 --> 00:29:07.770
Anthony Taylor: where they created like, a screener.

306
00:29:08.270 --> 00:29:13.579
Anthony Taylor: And it was based on. you know, hundreds of thousands of resumes that were submitted.

307
00:29:13.640 --> 00:29:18.380
Anthony Taylor: And then it said, Okay, next resume comes in. Tell me if it's any good

308
00:29:19.060 --> 00:29:30.549
Anthony Taylor: tested pretty well. problem was of those hundreds of thousands of accepted resumes like 98% of them. We're male

309
00:29:31.760 --> 00:29:37.789
Anthony Taylor: of that 98% like 80% of them were Caucasian males.

310
00:29:38.520 --> 00:29:46.229
Anthony Taylor: So guess which resumes always got through and which ones didn't, even for very qualified

311
00:29:46.340 --> 00:29:48.470
Anthony Taylor: candidates women

312
00:29:48.810 --> 00:29:49.920
Anthony Taylor: forget about

313
00:29:49.990 --> 00:29:54.809
Anthony Taylor: unless you had every single keyword on the job description. You aren't coming through

314
00:29:55.930 --> 00:30:00.899
Anthony Taylor: okay, Caucasian man. They could have like 2 words on the job description, and they got through

315
00:30:01.130 --> 00:30:03.319
Anthony Taylor: pretty much any other race.

316
00:30:05.100 --> 00:30:08.409
Anthony Taylor: just not as not as equally as the Caucasian.

317
00:30:08.690 --> 00:30:13.330
Anthony Taylor: So this was a great example of automated decision making going

318
00:30:13.410 --> 00:30:14.910
Anthony Taylor: terribly warm.

319
00:30:17.170 --> 00:30:24.500
Anthony Taylor: Okay, terribly, though. This happens with loan processing with the automated loan process happens with

320
00:30:24.820 --> 00:30:36.340
Anthony Taylor: freaking, dating sites. It's a recommendation engine. right? It it it does have. The recommendations are based on initially

321
00:30:36.360 --> 00:30:44.040
Anthony Taylor: what other people have said, what depends on their user base. If the original user base was

322
00:30:44.560 --> 00:30:57.450
Anthony Taylor: teenage girls from Russia. Okay. they're gonna be a set style is going to start off as the recommendation. And that seed

323
00:30:57.870 --> 00:31:02.720
Anthony Taylor: will grow to everybody. Yeah, why would that see grow? Well, let's say.

324
00:31:04.550 --> 00:31:09.390
Anthony Taylor: my daughter. say, I was gonna use bursting class. I'm like not gonna

325
00:31:09.410 --> 00:31:17.709
Anthony Taylor: my daughter signs up onto this dating site and she gets all of these dudes that appeal to Russian teenage girl.

326
00:31:18.310 --> 00:31:23.729
Anthony Taylor: She's like, Yeah, okay, we'll eat syrupies, or Oh, that's not bad now. She just said she likes somebody.

327
00:31:25.690 --> 00:31:28.289
Anthony Taylor: but she only, though in choices she was given

328
00:31:28.630 --> 00:31:40.300
Anthony Taylor: were those recommendations from the original algorithm. So what has she just done? Reinforced the algorithm for the bias that was presented to her.

329
00:31:42.060 --> 00:31:48.330
Anthony Taylor: You see how this just gets crazy. This is why we need the regulations. This is why we need to do it. Bottom line, though, is.

330
00:31:48.490 --> 00:31:53.110
Anthony Taylor: And and I'm going to say this again. I said it the other day. If if you leave with nothing else

331
00:31:53.300 --> 00:31:57.000
Anthony Taylor: out of data ethics, this is what you would lead with. Okay.

332
00:31:57.230 --> 00:32:05.169
Anthony Taylor: we as not not consumers. Consumers can try, and consumers will do okay if they make enough noise.

333
00:32:05.260 --> 00:32:13.649
Anthony Taylor: But we, as data professionals are the ones that have to identify and bring this to people's attention, not the media.

334
00:32:14.320 --> 00:32:15.819
Anthony Taylor: not if you like, working

335
00:32:16.580 --> 00:32:25.569
Anthony Taylor: okay, if you like working, you bring it up to your leadership. and you try to get them to understand. And the friend and and I'm going to tell you guys

336
00:32:25.780 --> 00:32:27.519
Anthony Taylor: my feeling on this.

337
00:32:27.730 --> 00:32:31.340
Anthony Taylor: and and you don't have to go with this. It's up to you.

338
00:32:31.600 --> 00:32:32.560
Anthony Taylor: Okay.

339
00:32:32.960 --> 00:32:37.519
Anthony Taylor: There are times when I see things and I say something and and

340
00:32:38.110 --> 00:32:40.989
Anthony Taylor: what, and and then I and and nothing happens.

341
00:32:43.320 --> 00:32:46.530
Anthony Taylor: And the reality is at that point

342
00:32:46.620 --> 00:32:50.710
Anthony Taylor: you have to make a decision. Am I just going to live with it

343
00:32:51.190 --> 00:32:56.030
Anthony Taylor: and stick this out because I like my job or my entire career.

344
00:32:56.390 --> 00:33:02.089
Anthony Taylor: right? Or you know, or and just assume that the people above me are doing the right thing.

345
00:33:03.790 --> 00:33:07.770
Anthony Taylor: Let me tell you what secret to this is documents.

346
00:33:09.600 --> 00:33:12.739
Anthony Taylor: Send an email. Save the email.

347
00:33:13.860 --> 00:33:16.059
Anthony Taylor: Okay? At least that

348
00:33:16.340 --> 00:33:18.169
Anthony Taylor: if it does come back

349
00:33:19.130 --> 00:33:23.100
Anthony Taylor: right, you you are protecting yourself

350
00:33:23.330 --> 00:33:37.360
Anthony Taylor: and maybe even your own job if you can say you know what I saw this coming, and I informed people. And here's my proof. Okay. be careful with that

351
00:33:37.560 --> 00:33:45.000
Anthony Taylor: lot of companies nowadays are purging your email within like 30 days. or you're and one that really kills you. Team chat.

352
00:33:45.590 --> 00:33:53.749
Anthony Taylor: My God! My company purges chat in in 2 weeks. Whatever you said in team chat, it's gone in 2 weeks.

353
00:33:54.450 --> 00:33:55.280
Anthony Taylor: it's like.

354
00:33:55.610 --> 00:33:57.950
Anthony Taylor: but I use that for storage.

355
00:33:58.130 --> 00:33:59.020
Anthony Taylor: But anyway.

356
00:34:00.230 --> 00:34:04.069
Anthony Taylor: so anyway, so decision making.

357
00:34:05.500 --> 00:34:06.410
Anthony Taylor: what are you gonna do?

358
00:34:06.540 --> 00:34:11.940
Anthony Taylor:  the the ability to contest you? Yeah, that's important.

359
00:34:12.520 --> 00:34:16.009
Anthony Taylor: So I'm gonna go back to something. I think Sonya was saying about medicine.

360
00:34:16.120 --> 00:34:22.959
Anthony Taylor: Okay, in the medical field. If you ever see an automated system that is making diagnoses

361
00:34:23.070 --> 00:34:34.059
Anthony Taylor: without follow-up. that's a red flap. Okay, if you ever go to a doctor's office and are, are there these new medical atms? Have you guys seen these things yet?

362
00:34:34.250 --> 00:34:44.870
Anthony Taylor: Hey? You literally could put your finger in, get your blood tested right right there, I'm thinking sheep and it will diagnose you. But they don't go. Oh, well, here's some medicine

363
00:34:45.070 --> 00:34:49.389
Clayton Graves: that sounds an awful lot like theranos. No, thanks.

364
00:34:49.889 --> 00:34:53.949
Anthony Taylor: though they're not. Gonna say, Oh, well, here's your medicine. You have, Covid.

365
00:34:54.110 --> 00:34:57.240
Anthony Taylor: Here's some beloved. Isn't that the name of that stuff

366
00:34:58.140 --> 00:35:00.509
Anthony Taylor: I've got Covid take the loaded

367
00:35:00.680 --> 00:35:03.320
Meredith McCanse (she/her): hack. Paxlva.

368
00:35:03.550 --> 00:35:16.800
Anthony Taylor: I mean, I get it. I get it. But anyway, they're not gonna do that. What they're gonna do is, they're gonna send that information to a real doctor

369
00:35:16.930 --> 00:35:19.980
Anthony Taylor: who's going to look at it and go. Oh, that guy's got go

370
00:35:21.180 --> 00:35:26.820
Anthony Taylor: right. That's the human intervention. Now for medical field. I think that's gonna be around for a long time.

371
00:35:26.980 --> 00:35:29.139
Anthony Taylor: There are a lot of other fields where that may not happen.

372
00:35:30.920 --> 00:35:35.070
Anthony Taylor: Okay, cause they don't need it. It's like I have a wrong rock. So what? What did we do?

373
00:35:35.570 --> 00:35:40.339
Anthony Taylor: Right? We charge them 40 bucks we use, you know, 10 s of electricity.

374
00:35:40.730 --> 00:35:53.570
Anthony Taylor: and we gave them a flyer to something that we could care less cost us a nickel to make. Okay. do they need it? No. So these regulations are needed for that. Okay?

375
00:35:53.970 --> 00:35:58.560
Anthony Taylor: Oh, that's that was so much. Oh, I think I skip one.

376
00:35:58.850 --> 00:35:59.620
Anthony Taylor: Nope.

377
00:35:59.910 --> 00:36:04.610
Anthony Taylor: yeah. Okay, so now we get to do a fun at 2.

378
00:36:04.850 --> 00:36:14.960
Anthony Taylor: This activity. You're going to be researching US. Data regulations as a group. and we'll respond to a series of questions. Now let me tell you as group

379
00:36:16.170 --> 00:36:19.700
Anthony Taylor: what I have here, let's see if they gave it to you. They did not.

380
00:36:19.920 --> 00:36:21.200
Anthony Taylor: So don't move.

381
00:36:21.260 --> 00:36:29.430
Anthony Taylor:  there are 5 topics.

382
00:36:29.480 --> 00:36:33.070
sonja baro: Yeah, I was, gonna say, it's in our activity file. I think.

383
00:36:33.510 --> 00:36:40.010
Anthony Taylor: good. Okay, it's in your activity file. But here's what you guys are. Gonna do. I am going to create 5 breakout rooms.

384
00:36:40.030 --> 00:36:44.260
Anthony Taylor: one for, is there? There's so much chat. Is any other important thing I need to look at.

385
00:36:45.060 --> 00:36:49.629
Anthony Taylor: It's not the same. We have no plans. Oh, okay.

386
00:36:49.890 --> 00:36:55.690
Anthony Taylor: alright. You guys know I did not look at that chat most of the time. When I see 27 of them.

387
00:36:56.450 --> 00:36:58.390
sonja baro: I like kids texting

388
00:36:58.620 --> 00:37:06.079
Anthony Taylor: well. And you, you know, you guys better than the problem. It's me. I've had classes where

389
00:37:06.150 --> 00:37:14.090
Anthony Taylor: I would see it scrolling on the right. And they're just not talking about like TV shows and crap while I'm talking. I'm like, Nope.

390
00:37:14.130 --> 00:37:17.300
Anthony Taylor: I just turn off chat. You just don't get chat anymore during class.

391
00:37:17.350 --> 00:37:23.280
Anthony Taylor: But you guys have never. I've seen gone too far down that path. So maybe Natalie, anyway.

392
00:37:23.390 --> 00:37:28.920
Anthony Taylor: you mean. so I'm gonna create 5 rooms.

393
00:37:29.160 --> 00:37:37.099
Anthony Taylor: What I would like to see is everybody equally split themselves up among 5. But I don't care. Okay, we don't have that big a group.

394
00:37:37.330 --> 00:37:46.099
Anthony Taylor: So if you guys all end up in one or 2 groups, that's fine, but you'll go. You'll pick which room you want to go to, and you will do what it says.

395
00:37:46.570 --> 00:37:59.420
Anthony Taylor: Okay. so you'll go to. If you do, children. you're gonna go here and you're going to search for it. And then you're going to read about it. And you're going to answer the questions. Pretty simple, really.

396
00:38:00.170 --> 00:38:03.870
Anthony Taylor: Okay. So let me give me a second to create the rooms.

397
00:38:05.270 --> 00:38:06.480
Anthony Taylor: A, what

398
00:38:06.610 --> 00:38:22.239
Sihong Zhou: chrome. So you mean, TV, right? Not like the voice control. Yeah, chrome. Just your, your, your inserted back of your TV and your control everything you want to see from

399
00:38:22.460 --> 00:38:31.380
Sihong Zhou: your phone, and you just project it. Project chromecast. Okay, yeah.

400
00:38:32.120 --> 00:38:36.920
I have a TV. Now, the room that has a roku, a fire stick and chromecast on.

401
00:38:37.090 --> 00:38:40.610
Anthony Taylor: Only thing we don't allow in this house is anything with an apple lock?

402
00:38:41.270 --> 00:38:45.810
Clayton Graves: Differential privacy, though.

403
00:38:45.970 --> 00:38:49.210
Clayton Graves: Okay, can I ask why you have 3 on one TV?

404
00:38:49.790 --> 00:38:58.759
Anthony Taylor: Well, the chromecast, because I like to to cast my video, and the Roku mirror cast, and all that stuff works badly.

405
00:38:58.960 --> 00:39:01.080
Anthony Taylor: Roku.

406
00:39:01.490 --> 00:39:07.320
Anthony Taylor: because I like Roku. The TV's default is roku and fire stick, just because

407
00:39:07.550 --> 00:39:08.390
Anthony Taylor: a

408
00:39:09.320 --> 00:39:13.860
Anthony Taylor: we just oh, because I use it for like, well, I used to have the Amazon game

409
00:39:14.200 --> 00:39:19.179
Anthony Taylor: that you used on the Internet. Yeah, I had to have a fire stick in. But that's it.

410
00:39:19.440 --> 00:39:22.150
Anthony Taylor: I know I also have an antenna.

411
00:39:23.160 --> 00:39:27.360
Masarirambi, Rodney: You're the one who makes my job difficult. you apenish

412
00:39:27.560 --> 00:39:39.520
Masarirambi, Rodney: for the screeners, and like for the awards, the people who are trying to airplay to a Roku device and a Roku TV airplay. You just can't do it.

413
00:39:39.760 --> 00:39:45.760
Anthony Taylor: I like my VR. I want to be able to freaking watch myself on the TV while I'm on. VR,

414
00:39:47.280 --> 00:39:51.320
Anthony Taylor: so just by the way, never do that unless you were in quote.

415
00:39:51.370 --> 00:39:52.790
Anthony Taylor: Okay, so

416
00:39:52.900 --> 00:39:54.360
Anthony Taylor: who's this?

417
00:39:54.950 --> 00:39:56.110
Anthony Taylor: That's too funny.

418
00:39:56.190 --> 00:40:00.020
Anthony Taylor:  in Android you can turn on. Oh, that's cool.

419
00:40:00.420 --> 00:40:03.730
Anthony Taylor: Okay. What was the next one

420
00:40:06.410 --> 00:40:10.570
Anthony Taylor: breakout rooms? What was it? Communications? Matt? Deborah Brandon?

421
00:40:10.970 --> 00:40:11.940
Dipinto, Matt: Wow.

422
00:40:12.370 --> 00:40:19.869
Dipinto, Matt: So we did a really bad job finding any example of violations of the Ecpa. There weren't a lot of

423
00:40:20.190 --> 00:40:23.040
Dipinto, Matt: explicitly listed cases. Maybe because

424
00:40:23.560 --> 00:40:30.229
Dipinto, Matt: once your personal information gets violated. You don't want to reprocess that. But anyway, the short summary of the Ecpas, it's like

425
00:40:30.410 --> 00:40:37.199
Dipinto, Matt: counter communications snooping so no wiretaps, no text, message, email, etc. Monitoring

426
00:40:37.360 --> 00:40:45.409
Dipinto, Matt:  and requires, you know, agencies to get warrant subpoenas or court orders to access your data.

427
00:40:45.920 --> 00:40:47.749
Dipinto, Matt: That's that's about what we learned.

428
00:40:49.420 --> 00:40:57.349
Clayton Graves: Yeah, I. The FBI was crazy about illegally wiretapping people back in the

429
00:40:57.450 --> 00:40:58.940
Clayton Graves: the Mccarthy era.

430
00:40:59.090 --> 00:41:04.179
Clayton Graves: So it'd be illegal wiretaps on Oppenheimer.

431
00:41:04.410 --> 00:41:17.429
Dipinto, Matt: We were chatting a little bit about like Watergate as well. And then the other thing is, it does mention that there was like modifications to the rule based on the Patriot Act, Aka. We cancelled most of the stipulations in the rule. For 20

432
00:41:18.040 --> 00:41:22.650
Dipinto, Matt: 18 years. They look at, buy cigarettes, and then we put the rule away.

433
00:41:25.500 --> 00:41:33.360
Anthony Taylor: Good. excellent. Okay. Next one was education. Nobody showed up

434
00:41:34.060 --> 00:41:36.789
finance. Derek, Michael Cindy.

435
00:41:41.420 --> 00:41:48.350
michael mcpherson: you can sue the Government now for violations of the Fair Credit Republic Reporting act

436
00:41:49.560 --> 00:41:54.019
michael mcpherson: according to sprint.

437
00:41:54.190 --> 00:41:56.090
Mason, Natalie: he said. Tell me more.

438
00:41:56.810 --> 00:42:10.630
michael mcpherson:  basically, a guy used a Usda rural housing loan. got a loan, built a house, paid off the loan somewhere along the line. The Usda started reporting it as delinquent

439
00:42:10.680 --> 00:42:14.670
michael mcpherson: even after was after the loan paid off, and then they refused to fix it.

440
00:42:15.150 --> 00:42:19.459
michael mcpherson: So he soon Nick made it all the way to the Supreme Court, and they're like, yep, you're liable

441
00:42:20.670 --> 00:42:24.390
michael mcpherson: but it didn't say what the any punishments are there were going to be

442
00:42:26.090 --> 00:42:27.399
Anthony Taylor: that's interesting.

443
00:42:28.650 --> 00:42:36.390
sonja baro: Any other who was who was liable because I thought I got because he missed. He used the loan in the first place.

444
00:42:36.800 --> 00:42:47.640
michael mcpherson: The Supreme Court said that that since the Usda it was a Usda sponsored loan. the Us. Government was, in fact, the lender.

445
00:42:47.810 --> 00:42:58.670
michael mcpherson: So it is treated the same as the lender. So this would apply to any loans or Va loan. Fha, Fannie, Freddy

446
00:42:58.900 --> 00:43:01.510
michael mcpherson: Usda. I'd

447
00:43:01.790 --> 00:43:02.790
sonja baro: yeah.

448
00:43:03.220 --> 00:43:08.550
Anthony Taylor: What are the others. There's a whole bunch of we don't need them all. We don't need too many letters.

449
00:43:08.590 --> 00:43:09.799
Anthony Taylor: But thank you, Mike.

450
00:43:10.210 --> 00:43:12.120
Mason, Natalie: yeah. Too many letters.

451
00:43:13.880 --> 00:43:18.049
Anthony Taylor: Alright. Any is there? Was that it on that one moving right along?

452
00:43:18.280 --> 00:43:23.600
michael mcpherson: Yeah. Last one, maybe has anything, or Eric has anything. Oh.

453
00:43:24.030 --> 00:43:24.910
Anthony Taylor: Derek.

454
00:43:25.030 --> 00:43:27.919
Anthony Taylor: Cindy, anything, any anything?

455
00:43:29.170 --> 00:43:30.550
Sihong Zhou: No, I don't.

456
00:43:33.400 --> 00:43:42.989
Anthony Taylor: Okay. Last group, this is a big old group. Holy, Moly. I can't even imagine who's gonna talk on this group. This is a big group

457
00:43:43.110 --> 00:43:48.450
Anthony Taylor: healthcare, Hampton, Rodney Meredith, Tanya, Sonya, Gabriel.

458
00:43:49.270 --> 00:43:52.859
Raugewitz, Tania: and I think we you know we should do. We should have half them and Gabriel all the time.

459
00:43:54.730 --> 00:43:59.490
Mason, Natalie: They do. Great. I know they would do great, but they don't drop much.

460
00:43:59.510 --> 00:44:06.020
Anthony Taylor: We're gonna kick it to Halfton. Then, just to show there you go.

461
00:44:06.640 --> 00:44:17.149
Raugewitz, Tania: II think Tanya Tanya. Tanya is about to say something. Yeah, no, because he's so well versed in this particular topic.

462
00:44:17.580 --> 00:44:21.090
Anthony Taylor:  he's not there. Go ahead. Tuck.

463
00:44:21.660 --> 00:44:24.580
Raugewitz, Tania: Oh, then I defer to Sonya.

464
00:44:25.890 --> 00:44:31.829
sonja baro: No, I wanted you didn't know as much. So

465
00:44:31.990 --> 00:44:34.770
Meredith McCanse (she/her): rolling out like 60 more seconds.

466
00:44:34.830 --> 00:44:56.099
sonja baro: Yeah. So we did. Healthcare and hipaa violation. And did a couple of articles actually. And the first one a couple of security guards, actually had access to personal data. Somehow they're found out. I'm not sure I don't remember how exactly, but it was.

467
00:44:56.870 --> 00:45:00.529
Raugewitz, Tania: the where's my answers?

468
00:45:01.970 --> 00:45:06.940
Raugewitz, Tania: The compliance officers role to

469
00:45:07.170 --> 00:45:24.920
Raugewitz, Tania: Moderate that, or make sure that we were in compliance. They were in compliance. But there's a small fine of maybe around $250,000, so we don't feel like that was enough. And then they had to beef up their security to the patients. And anybody feel free to jump in if I'm not summarizing this correctly.

470
00:45:25.340 --> 00:45:52.079
Raugewitz, Tania: So it was like a slap on the wrist. We didn't agree with it. The second article, though, was about this California hospital that had kind of gross violations. I'd say that no offense to anybody on this team. And meaning that it was pretty large scale, and the data wasn't protected, really at all. And and so access to and that was like a multi-million dollar

471
00:45:52.130 --> 00:45:53.270
Raugewitz, Tania: find.

472
00:45:53.280 --> 00:45:56.820
Raugewitz, Tania: So that's the

473
00:45:56.880 --> 00:46:26.620
sonja baro: jump in for some important details, Meredith.

474
00:46:26.820 --> 00:46:38.559
Raugewitz, Tania: Well, II didn't read that article, but I was gonna say he hasn't yet. I haven't. I don't. I don't wanna read it.

475
00:46:38.670 --> 00:46:45.330
Gebrekristos, Hafton: But II think we're we're working on, you know, protecting this PHIN pi data. Now.

476
00:46:45.630 --> 00:46:51.290
Gebrekristos, Hafton: we, we, we're working on a project called Re Deidentification or data. I'm asking

477
00:46:52.210 --> 00:46:55.809
Gebrekristos, Hafton:  So what what is happening is

478
00:46:56.500 --> 00:47:03.880
Gebrekristos, Hafton: business are allowed to download Csv file to their desktop. Right? And they do analysis from it. And this

479
00:47:04.100 --> 00:47:16.319
Gebrekristos, Hafton: CSV. File will contain a lot of PH. And Npi data. And we have so many employees are doing. That is kinda hard to track the files where they coming from and

480
00:47:16.710 --> 00:47:18.940
Gebrekristos, Hafton: who is using it for what

481
00:47:19.520 --> 00:47:25.399
Gebrekristos, Hafton: it's still, it is still a struggle to kind of manage that. But I

482
00:47:25.660 --> 00:47:33.549
Gebrekristos, Hafton: don't want to say about that. You know. Yeah, don't. Don't incriminate yourself. Never knows who's like government agent in here.

483
00:47:35.090 --> 00:47:36.439
Anthony Taylor: You never know.

484
00:47:36.590 --> 00:47:40.909
michael mcpherson: Natalie, I will. Secret agent man

485
00:47:41.230 --> 00:47:56.390
Anthony Taylor: happens a lot.

486
00:47:56.430 --> 00:48:05.120
Anthony Taylor: And identifying information problem is you'd be surprised. What's considered identifying information right? Is birthday identifying.

487
00:48:07.540 --> 00:48:11.109
Anthony Taylor: It's not. It's just, you know, it can be.

488
00:48:11.650 --> 00:48:14.429
Anthony Taylor: How about? How about a medical record number.

489
00:48:15.260 --> 00:48:20.670
Anthony Taylor: the number on your phone or somewhere in hospital? Yeah.

490
00:48:21.730 --> 00:48:23.320
Anthony Taylor: the number is definite.

491
00:48:23.540 --> 00:48:28.760
sonja baro: It's considered like a social. I mean in that hospital. It's unique, identifier to you. Only

492
00:48:29.500 --> 00:48:32.419
birthday. No hair color.

493
00:48:33.320 --> 00:48:34.100
sonja baro: no.

494
00:48:34.310 --> 00:48:35.560
Anthony Taylor: no.

495
00:48:35.600 --> 00:48:44.639
Masarirambi, Rodney: Depends on how much of the information they're getting is if they're figuring out a way to get it. And if they get all these different pieces of information

496
00:48:45.300 --> 00:48:50.330
Gebrekristos, Hafton: combination of data that identifies people.

497
00:48:50.780 --> 00:48:55.390
Gebrekristos, Hafton: But mainly we protect. You know, member Member Member Id address names.

498
00:48:56.000 --> 00:49:00.420
Anthony Taylor: phone is not always phone numbers, but sometimes phone numbers follow that, too.

499
00:49:01.850 --> 00:49:08.060
Anthony Taylor: There's a phone number typically belongs to one family at the very, most, very least. So

500
00:49:08.220 --> 00:49:14.080
Anthony Taylor: yeah, Meredith, question or comment. The whole notion of

501
00:49:14.430 --> 00:49:34.800
Meredith McCanse (she/her): the downloading downloading data into a spreadsheet like that's a big issue in the accounting world, too, because you have to manipulate the data and accountants at least use excel. That's, you know. So you have to get it into a place where you can use it. But I mean, we had an example, last year we were trying to troubleshoot some stuff, and it was part of an audit, and we had to trace payments

502
00:49:34.800 --> 00:49:45.569
Meredith McCanse (she/her): and a whole data file of all these payments the company had made, and there was a vendor id number, and it turned out that their payroll information was included in that. And so people's

503
00:49:45.570 --> 00:50:09.120
Meredith McCanse (she/her):  People like the vendor, like my name, was listed as a vendor, and under the vendor Id number was my checking account number, and they basically had the entire, all the employees included in this list of data cause. They were receiving payments in the form of payroll. And then that data got all passed around for everyone to do all the stuff with. And it's like everybody's personal checking account numbers

504
00:50:09.180 --> 00:50:18.939
Meredith McCanse (she/her): and it's just it like I think it happens all the time in so many ways, shapes and forms. and it just yeah. You're right.

505
00:50:19.530 --> 00:50:20.510
Anthony Taylor: Jennifer.

506
00:50:22.800 --> 00:50:31.410
Jennifer Dahlgren: This might get us off track. Yes, yes, my, my first job in grad school was working for the Cdc.

507
00:50:31.550 --> 00:50:40.020
Jennifer Dahlgren: And it was actually, we're looking at HIV information and I couldn't go to

508
00:50:40.410 --> 00:50:51.959
Jennifer Dahlgren: certain areas because it was identifiable. And the population that I was interviewing was men who have sex with men about their sex lives. In order to understand HIVI lived in Kansas.

509
00:50:51.970 --> 00:51:05.710
Jennifer Dahlgren: And so literally, I could not go to certain areas just because that was identifiable information. I was outing those few individuals in these small rural towns. So I had to go to larger places in order to be able to do my interviews.

510
00:51:06.550 --> 00:51:09.060
Anthony Taylor: That's interesting.

511
00:51:10.000 --> 00:51:11.809
Anthony Taylor: Huh? Yes, I know.

512
00:51:12.490 --> 00:51:21.979
sonja baro: Yeah, I just wanted to add, one of the things about hipaa more than well, not more than it covers a lot of stuff, not just privacy and security.

513
00:51:21.990 --> 00:51:32.229
sonja baro: And one thing I just wanted to make sure everyone knows is that your help? This law made sure that it is known. Your healthcare information is your own.

514
00:51:32.320 --> 00:51:59.120
sonja baro: So whatever and previously used to be, couldn't you know you couldn't get your own record, or people would like, say, No, way, that's your. That's ours. We're everything that about you is ours. You don't get it. This particular law made that flipped it on its head. So I just raise it because a lot of times people may not be aware of that. So if you ever get told, you can't have it. That's this is ours. They're wrong. So

515
00:51:59.680 --> 00:52:02.490
Clayton Graves: so I've got I've got a question for the class.

516
00:52:02.710 --> 00:52:07.480
Clayton Graves: So I was a situation where someone was fired

517
00:52:07.660 --> 00:52:15.620
Clayton Graves: and they were. They filed a complaint, and said that that they believe they were fired because they were hard of hearing.

518
00:52:15.650 --> 00:52:16.810
Clayton Graves: The disability

519
00:52:17.270 --> 00:52:27.610
Clayton Graves: responded back and said, Well, that person never disclosed it. Therefore we couldn't have known about it that therefore we couldn't have been, you know.

520
00:52:27.910 --> 00:52:29.490
Clayton Graves: prejudicial about it.

521
00:52:30.110 --> 00:52:33.840
Clayton Graves: Now, when the the company sent

522
00:52:34.290 --> 00:52:42.549
Clayton Graves: over the list of documents pertaining to the employees. Application, and things like that.

523
00:52:42.880 --> 00:52:51.819
Clayton Graves: One of the things that they submitted was a document showing all the disabilities claimed by every employee in the company.

524
00:52:52.250 --> 00:52:55.080
Clayton Graves: and they sent it to this person

525
00:52:55.280 --> 00:52:59.049
Clayton Graves: who was filing the complaint, and nothing came of it.

526
00:52:59.210 --> 00:53:04.809
Clayton Graves: Nothing came of it. They did not. That sounds like a breach to me.

527
00:53:05.870 --> 00:53:07.720
Anthony Taylor: Yeah, I mean.

528
00:53:07.910 --> 00:53:12.289
Anthony Taylor: yeah, the the first one you were. Somehow they didn't know I've been like

529
00:53:12.820 --> 00:53:25.850
Anthony Taylor: that's reasonable. And I mean if he would ever let him know, and they could prove they don't know to the point of. There's no documentation of it, and I could see them getting away with that, but sending the document with all the disabilities

530
00:53:26.510 --> 00:53:28.850
Anthony Taylor: like date that that

531
00:53:29.160 --> 00:53:32.919
Anthony Taylor: yeah, somebody can make. Some

532
00:53:33.340 --> 00:53:41.910
michael mcpherson: be closed up in a sealed sealed file sealed record and everybody involved at the end of settlement would

533
00:53:42.040 --> 00:53:43.329
michael mcpherson: sign in Nda.

534
00:53:45.720 --> 00:53:48.709
Anthony Taylor: all right. Good stuff, everybody. Good stuff.

535
00:53:49.090 --> 00:53:59.110
Anthony Taylor: Okay. Some big laws that are going around the world. We're not gonna spend a lot of time on these. If you're really interested in them, feel free to look them up.

536
00:53:59.160 --> 00:54:03.069
Anthony Taylor: You have the slideshow, you can google it. You can co-pilot it

537
00:54:03.200 --> 00:54:04.610
Anthony Taylor: can do whatever you want.

538
00:54:04.670 --> 00:54:08.429
Anthony Taylor: But some of the big ones, though I always

539
00:54:08.700 --> 00:54:10.390
Anthony Taylor: privacy framework.

540
00:54:10.620 --> 00:54:13.800
Anthony Taylor: Okay. So I'm just gonna touch on each of these

541
00:54:13.840 --> 00:54:21.199
Anthony Taylor: relatively quickly. The collection limitation principle is basically, if you don't have a mixing to have it, shouldn't have it.

542
00:54:21.250 --> 00:54:23.059
sonja baro: Are you showing something?

543
00:54:23.620 --> 00:54:26.060
Anthony Taylor: I am not showing it. Very well.

544
00:54:26.120 --> 00:54:28.569
sonja baro: Okay, I can see. Second chance.

545
00:54:29.270 --> 00:54:30.780
sonja baro: Thank you. There you go.

546
00:54:31.190 --> 00:54:36.930
Anthony Taylor: Okay. So I mean, it was wasn't much. II think I described. But

547
00:54:37.100 --> 00:54:52.999
Anthony Taylor: the data quality principle, okay, you should only collect data that you have a reason to be collecting. Okay? And it should be maintained and always accurate and up to date. That's a tough one.

548
00:54:53.270 --> 00:55:00.350
Anthony Taylor: My credit report can't even get that right. hey? Oh. great story for you.

549
00:55:00.640 --> 00:55:07.540
Anthony Taylor: 1967, June twenty-seventh. My birthday. I can tell you that because it won't be in this class. Right?

550
00:55:07.600 --> 00:55:16.129
Anthony Taylor: Anthony Taylor. There's another one. Anthony Taylor, born June 20, seventh, 1967.

551
00:55:16.650 --> 00:55:24.569
Anthony Taylor: I know this because I got pulled over by the Irvine police one time and drug out of the car and sat on her

552
00:55:25.660 --> 00:55:28.479
Anthony Taylor: because Anthony S. Taylor

553
00:55:29.480 --> 00:55:32.530
Anthony Taylor: was in prison in Georgia

554
00:55:32.600 --> 00:55:40.310
Anthony Taylor: for assault. Sexual assault. Okay? And they wanted to know how I was in Irvine while I was supposed to be in prison.

555
00:55:41.620 --> 00:55:53.359
Anthony Taylor: It was a very interesting thing. Tell you truth, I had to deal with it for years. I finally called the prison and said, What can I do? And they said, Well, it's social security number ends with this 4 digits.

556
00:55:53.700 --> 00:55:58.150
Anthony Taylor: I went. Well, that's not me, and they go. Well, we know that you're in prison. Wow.

557
00:55:59.320 --> 00:56:04.649
Anthony Taylor: anyway. So then for a while, every time I got this came up I had to say, check social.

558
00:56:05.020 --> 00:56:07.739
Anthony Taylor: and they would they would also. I don't have a minute.

559
00:56:09.130 --> 00:56:12.330
Anthony Taylor: The other problem was this, guy was 6 or 5

560
00:56:12.490 --> 00:56:14.819
Anthony Taylor: 10 Caucasian

561
00:56:14.870 --> 00:56:16.519
Anthony Taylor: brown hair, brown eyes.

562
00:56:17.860 --> 00:56:27.909
Anthony Taylor: So it was. I mean, I'm not 5, 10, I'm 6 foot. But you know that that could be wrong. Anyway. What does that have to do with this discussion. Accurate data.

563
00:56:28.600 --> 00:56:32.140
Anthony Taylor: All they really were looking at was name birthday.

564
00:56:32.880 --> 00:56:39.829
Anthony Taylor: They weren't looking at the rest of the data. Why didn't they find me and have it until I started bringing up. Eventually it went off.

565
00:56:39.940 --> 00:56:43.360
Anthony Taylor: But that was a rough time. Every once in a while.

566
00:56:43.520 --> 00:56:56.119
Anthony Taylor:  purpose specification organizations need to determine why they're collecting data, and they need to document why they are correct collecting the data

567
00:56:56.250 --> 00:56:59.179
Anthony Taylor: and it can only be used

568
00:56:59.680 --> 00:57:12.370
Anthony Taylor: for the things that they say it they're using it for. So that's what this framework is saying. If you say you're gonna use it for this, this is what you could use it, for you want to use it for something else. You have to get additional

569
00:57:12.750 --> 00:57:15.100
Anthony Taylor: consent.

570
00:57:15.310 --> 00:57:18.410
Anthony Taylor: Use limitation. Same idea.

571
00:57:18.780 --> 00:57:20.730
Anthony Taylor: the wrong wrong one

572
00:57:20.940 --> 00:57:27.460
Anthony Taylor:  security safeguards. You have to do everything you can to prevent unauthorized access.

573
00:57:27.510 --> 00:57:36.689
Anthony Taylor: You have to destroy it through a set, you know, a documented policy. And if you modify it, that also has to be docked

574
00:57:38.240 --> 00:57:45.770
Anthony Taylor: open this principle. So basically, this just says that you should be able to find out what it is they got

575
00:57:47.300 --> 00:57:48.250
Anthony Taylor: alright.

576
00:57:48.500 --> 00:57:54.200
Anthony Taylor: and then last, but not not last, individual participates. But this one I'm gonna read

577
00:57:54.430 --> 00:58:05.870
Anthony Taylor: people should be able to confirm. If they have information, request all of that information, receive all of that information in a reasonable timeframe

578
00:58:05.930 --> 00:58:11.030
Anthony Taylor: at a minimal cost in a format they can understand. That's a lot

579
00:58:11.430 --> 00:58:20.330
Anthony Taylor: request changes to the organization's information that relates to them. Request personal information, be deleted, and be provided with reasons. If any

580
00:58:20.500 --> 00:58:29.089
Anthony Taylor: of the proceeding requests are denied a lot. Accountability are organizations that break any of these rules will be

581
00:58:29.310 --> 00:58:30.500
Anthony Taylor: health count.

582
00:58:31.310 --> 00:58:37.410
Anthony Taylor: That's the organization, economic cooperation and development. It's huge.

583
00:58:37.640 --> 00:58:44.710
Anthony Taylor: hey lot of people. A lot of countries are involved with it in this alright.

584
00:58:44.820 --> 00:58:48.780
Anthony Taylor: Oh, just basically what I just told you just in works.

585
00:58:49.880 --> 00:58:53.740
Anthony Taylor: Gdpr, so this is one that causes me nightmares.

586
00:58:53.820 --> 00:58:59.099
Anthony Taylor: My company is in Florida, but we have. We have factories in Germany, Japan.

587
00:58:59.110 --> 00:59:03.290
Anthony Taylor: We have employees in Croatia, Germany, Japan, China.

588
00:59:04.220 --> 00:59:05.230
Anthony Taylor: Brazil.

589
00:59:05.990 --> 00:59:10.909
Anthony Taylor: Okay, we deal with customers and surgeons all over the world.

590
00:59:11.120 --> 00:59:14.129
Anthony Taylor: So Gdpr applies to everything we do.

591
00:59:14.690 --> 00:59:32.360
Anthony Taylor:  So yeah, so in in. Basically, all of the stuff from Oecd is in there.  The the EU is very strict on companies that don't follow this, and they will find the geezers out of

592
00:59:33.090 --> 00:59:37.349
Anthony Taylor:  probably the most. The biggest difference in all of this

593
00:59:37.450 --> 00:59:38.580
Anthony Taylor: is

594
00:59:39.470 --> 00:59:49.260
Anthony Taylor: the privacy framework. Gdpr has rules about automated decision making. These require transparency around the decisions we've already talked about this.

595
00:59:49.820 --> 00:59:56.540
Anthony Taylor: Okay, th, it's really hard to do this with LON, are they gonna win this? They're not gonna win this

596
00:59:57.330 --> 01:00:01.899
alright California consumer privacy act. This is that one I told you about

597
01:00:02.230 --> 01:00:05.570
Anthony Taylor: ticks off the rest of the country. Okay?

598
01:00:05.870 --> 01:00:12.210
Anthony Taylor: And I love. I could say that in this class, because you guys aren't usually a uci.

599
01:00:12.320 --> 01:00:14.549
Anthony Taylor: But right, you guys.

600
01:00:14.970 --> 01:00:18.510
Anthony Taylor: yeah, stupid California. I know. That's how you feel

601
01:00:18.730 --> 01:00:33.060
sonja baro: our job. California

602
01:00:33.410 --> 01:00:35.209
Mason, Natalie: on like every box.

603
01:00:35.990 --> 01:00:44.589
Anthony Taylor: No, but II could tell you first story on that one, too. No, this will basically that you have to know if someone using your data.

604
01:00:44.720 --> 01:00:52.720
Anthony Taylor: The difference is as opposed to Gdpr. Is that they don't have to tell you as much

605
01:00:53.790 --> 01:00:55.800
Anthony Taylor: hold on the core difference.

606
01:00:55.840 --> 01:01:09.679
Anthony Taylor: It does not require customer consent for collection. Okay, it does require notice of collection. So in other words, if you're dealing in California, you have to have somewhere on your site that. Yep.

607
01:01:09.970 --> 01:01:11.429
Anthony Taylor: we're taking your data.

608
01:01:11.460 --> 01:01:18.210
Anthony Taylor: but we're going to use it. But you don't have to get consent to do so. The rest of the country. They could just take your data. And that's it.

609
01:01:18.770 --> 01:01:24.490
Anthony Taylor: If you're on their website. then so that's what Ccpa basically is.

610
01:01:25.240 --> 01:01:32.010
Anthony Taylor:  okay, so this is really, this might be the most important slide of the day.

611
01:01:32.510 --> 01:01:35.400
Anthony Taylor: Okay, when it comes to data regulations.

612
01:01:36.410 --> 01:01:39.899
Anthony Taylor: these are the things I want you to remember as a data professional.

613
01:01:40.150 --> 01:01:43.889
Anthony Taylor: Consider your industry. Boom right there. That's a big.

614
01:01:44.250 --> 01:01:45.140
Anthony Taylor: hey.

615
01:01:45.530 --> 01:01:48.060
Anthony Taylor: Sonya, healthcare hippo!

616
01:01:48.570 --> 01:01:53.079
Anthony Taylor: Actually, are there others? Yes. hipaa, that's one of the biggest

617
01:01:53.360 --> 01:01:59.800
Anthony Taylor: you better know anything with finance or publicly traded companies.

618
01:02:00.240 --> 01:02:01.600
Anthony Taylor: What is it, Meredith?

619
01:02:04.620 --> 01:02:06.720
Meredith McCanse (she/her): I mean, there's lots

620
01:02:06.810 --> 01:02:10.710
Meredith McCanse (she/her): alright. The big one. Darby's Oxley.

621
01:02:10.770 --> 01:02:12.720
Meredith McCanse (she/her): also known as socks.

622
01:02:12.920 --> 01:02:13.990
Anthony Taylor: Yeah.

623
01:02:14.040 --> 01:02:26.530
Anthony Taylor: that's the big one that every publicly traded company has to be aware of. Alright. But if you deal with finance at all with that  education.

624
01:02:26.650 --> 01:02:34.629
Anthony Taylor: I don't know the big regulations around education. But I'm certain you can't share like information without consent.

625
01:02:34.710 --> 01:02:36.620
Anthony Taylor:  yeah.

626
01:02:36.790 --> 01:02:44.710
Anthony Taylor: Okay, so lots of stuff like that. So consider industry. Be aware of the regulations that affect your industry. Again.

627
01:02:44.750 --> 01:02:49.709
Anthony Taylor: very unlikely. You, as data professionals are going to be the ones enforcing them.

628
01:02:49.840 --> 01:02:52.700
Anthony Taylor: But you might be the one reporting them.

629
01:02:53.600 --> 01:02:56.680
Anthony Taylor: and if you're lucky you're not the one

630
01:02:56.720 --> 01:02:57.980
Anthony Taylor: pausing them.

631
01:02:59.850 --> 01:03:03.009
Anthony Taylor: Okay, trust me, I've done it.

632
01:03:03.390 --> 01:03:07.440
Anthony Taylor: I've done things that definitely would have violated some kind of regulation.

633
01:03:07.490 --> 01:03:09.929
Anthony Taylor: and then caught it before I told anybody.

634
01:03:11.400 --> 01:03:12.380
Anthony Taylor: I always tell.

635
01:03:12.960 --> 01:03:18.059
Anthony Taylor: but that's not the point point is, and it could happen. But you should be aware, all right.

636
01:03:18.090 --> 01:03:27.900
Anthony Taylor: Research, the data and technology. So not only is the type business, but where you are. what state, what country? Okay.

637
01:03:28.160 --> 01:03:32.430
Anthony Taylor: where your company is, you have to follow those regulations.

638
01:03:32.550 --> 01:03:34.649
Anthony Taylor: Figure out where your customers are.

639
01:03:35.070 --> 01:03:39.780
Anthony Taylor: If you're in Texas, where you could pretty much carry a gun to school.

640
01:03:40.670 --> 01:03:41.740
Anthony Taylor: Okay.

641
01:03:42.850 --> 01:03:47.970
Anthony Taylor: that's great. But if your customers are in California used to follow those out on the

642
01:03:49.420 --> 01:03:52.300
Anthony Taylor: okay, yeah, I don't show it. Same with Gdpr.

643
01:03:52.700 --> 01:03:54.540
Anthony Taylor: how to follow the European rules.

644
01:03:56.220 --> 01:04:02.309
Anthony Taylor: and last, but the least determine if your process the data of children, or if children will likely, what

645
01:04:03.320 --> 01:04:08.539
Anthony Taylor: I don't understand, why, that? I mean, yes, it's obviously important. But it's really specific.

646
01:04:09.250 --> 01:04:12.270
Anthony Taylor:  okay, we'll just go with it.

647
01:04:12.340 --> 01:04:17.680
Anthony Taylor: So yeah, if your stuff affects children. raise a red flag. Okay.

648
01:04:19.110 --> 01:04:21.100
Anthony Taylor: cool questions.

649
01:04:22.200 --> 01:04:24.210
Anthony Taylor: We're not done, but we are

650
01:04:24.390 --> 01:04:27.999
Anthony Taylor: on break. So I'll see you guys in quarter after the hour.

651
01:04:32.290 --> 01:04:34.730
Anthony Taylor: Welcome back, everybody.

652
01:04:36.270 --> 01:04:41.190
Anthony Taylor: Okay, I don't know, I think you guys are actually having fun doing these article things right?

653
01:04:42.570 --> 01:04:44.270
Anthony Taylor: Okay, all right.

654
01:04:44.590 --> 01:04:54.650
Anthony Taylor: So here's what we're gonna do. Well. we we have another article one. A,

655
01:04:55.730 --> 01:05:00.460
Anthony Taylor: this one's specific to open. AI and Gdpr.

656
01:05:03.250 --> 01:05:05.959
Anthony Taylor: and I'm not sharing. Let me just share this.

657
01:05:06.810 --> 01:05:13.730
Anthony Taylor: So it's in your activities. All of that. This cool article has some questions for you.

658
01:05:13.960 --> 01:05:18.299
Anthony Taylor: let's do. I'm just gonna just just

659
01:05:18.430 --> 01:05:22.310
Anthony Taylor: put you guys in random groups so that we don't have to all

660
01:05:22.350 --> 01:05:23.679
Anthony Taylor: do it separately.

661
01:05:24.060 --> 01:05:29.709
Anthony Taylor: I'll put you out like in 3 groups. And you guys can read and then talk about sound good.

662
01:05:31.450 --> 01:05:34.130
Anthony Taylor: So let me. Thank you.

663
01:05:34.490 --> 01:05:35.300
Anthony Taylor: Yeah.

664
01:05:36.940 --> 01:05:46.290
Anthony Taylor: I've tried to get it to work like 3 times. It creates a video and it gives you a link. And it takes you to the same video every time, and it's some guy it's called brick rolled.

665
01:05:46.840 --> 01:05:49.760
Anthony Taylor: which I have. A feeling is like, you're a sucker.

666
01:05:50.010 --> 01:05:52.819
michael mcpherson: He's trolling us.

667
01:05:52.840 --> 01:05:59.439
michael mcpherson: He's totally trolling. I don't think that.

668
01:06:00.280 --> 01:06:03.620
Anthony Taylor: No, I got II went to chat Tpt.

669
01:06:04.240 --> 01:06:07.750
Anthony Taylor: and there is a Gp. T. Called Sora.

670
01:06:08.310 --> 01:06:13.230
Derek Rikke: Hold on! Let me look at some. I don't think it's real. So it's like Sam Allman was on Twitter today.

671
01:06:13.280 --> 01:06:18.710
Derek Rikke: He was like taking requests and then making the videos. So it's like he was doing it for people.

672
01:06:19.110 --> 01:06:24.500
Derek Rikke: So I don't think anyone could just do it. I think it's like.

673
01:06:25.010 --> 01:06:29.150
Anthony Taylor: that's a bummer I got suckered into. I'm gonna I'm gonna

674
01:06:29.720 --> 01:06:30.989
Anthony Taylor: eat that guy.

675
01:06:33.460 --> 01:06:36.189
Anthony Taylor: So yeah, generalmind.com.

676
01:06:36.720 --> 01:06:38.629
sonja baro: are you talking about what he posted?

677
01:06:39.310 --> 01:06:50.919
Anthony Taylor: The video thing is that not real cause? They look real. Yeah. Well, I just went tried it. Every time you run it, it takes you the same video. And now that I look.

678
01:06:51.010 --> 01:06:54.629
Anthony Taylor: there's like a hundred of them called Sora.

679
01:06:54.800 --> 01:07:02.280
Anthony Taylor: and I'll bet you every one of them are exactly the same.

680
01:07:02.350 --> 01:07:13.749
sonja baro: It's got a couple of videos on the page, and it says everything on that page was generated by sorry. And then it talks about how like today, it became available to certain people.

681
01:07:13.940 --> 01:07:21.730
Meredith McCanse (she/her): I don't think it's like publicly available is insane, like.

682
01:07:21.900 --> 01:07:23.360
sonja baro: Oh, my God.

683
01:07:23.610 --> 01:07:28.359
Anthony Taylor: it's so cool! I think it's I mean, it is. It is on the open AI website.

684
01:07:29.620 --> 01:07:37.980
Meredith McCanse (she/her): I think it's text. I don't think it's image to video. I think it's scroll down. There's a whole nother block that is image to video.

685
01:07:38.730 --> 01:07:39.480
Anthony Taylor: Hello!

686
01:07:39.760 --> 01:07:47.879
sonja baro: Like the the girl with the pearl earring it. It paintings start. She starts laughing. And then

687
01:07:48.010 --> 01:07:49.850
sonja baro: I mean, it's really cool.

688
01:07:50.890 --> 01:07:59.710
Anthony Taylor: Okay, all right. Maybe it's not fake, but it's the the ones that are there now, currently are. That's interesting. Okay.

689
01:08:01.210 --> 01:08:03.640
Anthony Taylor: that's pretty cool. I'm excited about that.

690
01:08:04.810 --> 01:08:06.930
Anthony Taylor: Alright. So

691
01:08:07.380 --> 01:08:10.959
Anthony Taylor: let me get the recording started. Oh, it is going.

692
01:08:12.250 --> 01:08:16.040
Anthony Taylor: I guess I started it. I don't know. Hopefully, it wasn't on that whole time you guys were gone

693
01:08:16.140 --> 01:08:17.519
Anthony Taylor: that'd be no fun at all.

694
01:08:19.840 --> 01:08:20.689
Anthony Taylor: Okay.

695
01:08:22.620 --> 01:08:23.899
Anthony Taylor: group one.

696
01:08:25.560 --> 01:08:26.890
Anthony Taylor: somebody talked to me.

697
01:08:27.189 --> 01:08:31.139
Anthony Taylor: actually, anybody. We all looked at the same thing. Everybody just stopped.

698
01:08:31.160 --> 01:08:36.600
Anthony Taylor: What do you guys think? Is there a solution? Well, what's the problem? What is it talking about?

699
01:08:39.840 --> 01:08:41.450
Anthony Taylor: Did did nobody read it?

700
01:08:42.260 --> 01:08:53.350
Clayton Graves: I feel like some of the complaints that were made. And we we talked at length about this, some of the complaint complaints that they were made about, like using personal data

701
01:08:53.479 --> 01:08:58.760
Clayton Graves: to train the model. I'm not seeing any evidence of that.

702
01:08:59.250 --> 01:09:03.189
Clayton Graves:  So I'm I'm not quite sure

703
01:09:03.439 --> 01:09:15.610
Clayton Graves: how to approach that. The other thing that they talked about was wanting more transparency out of open AI. And and the way that I thought of it thought of it as was like

704
01:09:15.729 --> 01:09:21.330
Clayton Graves: this sounds like an unmovable object meeting an irresistible force because you've got.

705
01:09:21.410 --> 01:09:25.039
Clayton Graves: you know this, this law, this is this is the law

706
01:09:25.359 --> 01:09:38.709
Clayton Graves: on one side. But you're also talking about asking for something that I don't think we're technologically capable of providing yet which is transparency into how these things work.

707
01:09:40.229 --> 01:09:41.790
Anthony Taylor: That's a very nice.

708
01:09:41.899 --> 01:09:49.940
Anthony Taylor: this discussion. There, let me give you the one piece that you don't have

709
01:09:50.010 --> 01:09:57.260
Anthony Taylor: right. What we do know about Openai is that it scraped the Internet right? Any accessible website

710
01:09:57.350 --> 01:10:03.259
Anthony Taylor: as part of its consumption. And they admitted that back in Gp, d. 3.

711
01:10:03.680 --> 01:10:13.080
Anthony Taylor:  which what this dude is trying to claim is well, they scraped everything. They definitely got personal information off of people

712
01:10:13.420 --> 01:10:19.699
Clayton Graves: as as of April 2023, that that's when it was last updated and trained.

713
01:10:19.810 --> 01:10:21.280
Clayton Graves: GPT. 4.

714
01:10:21.490 --> 01:10:24.740
Clayton Graves: It can't tell me a thing about myself.

715
01:10:24.780 --> 01:10:26.320
Anthony Taylor: There you go.

716
01:10:27.980 --> 01:10:29.040
Anthony Taylor: So there you go

717
01:10:29.140 --> 01:10:35.119
Anthony Taylor: now. Did they anonymize? Did they do something? Well, that's probably their argument. Back to them.

718
01:10:35.360 --> 01:10:46.289
Anthony Taylor:  the the hard part of this whole thing is. And and I told you guys this before, these models are designed to do things that we.

719
01:10:47.320 --> 01:10:51.179
Anthony Taylor: while we may be capable of, we aren't seeing.

720
01:10:52.800 --> 01:10:53.800
Anthony Taylor: naturally.

721
01:10:54.330 --> 01:11:04.099
Anthony Taylor: And I'm not saying we can't see that it's like I said, they, when we and you guys are going to find out all this works later, but effectively, it finds relationships in

722
01:11:04.190 --> 01:11:05.460
Anthony Taylor: context

723
01:11:06.140 --> 01:11:15.389
Anthony Taylor: of praises or objects, basically words. Okay? And it finds relationships

724
01:11:15.670 --> 01:11:24.710
Anthony Taylor: based on a prompt, you give it a prompt it goes, oh, well, let me go find relationships and all of the stuff I've ever looked at. It finds those things and it returns them.

725
01:11:25.220 --> 01:11:26.160
Anthony Taylor: Okay?

726
01:11:26.340 --> 01:11:34.629
Anthony Taylor:  yeah. I mean, that is really hard. The the problem is, it's finding those relationships. Many of us.

727
01:11:34.640 --> 01:11:41.090
Anthony Taylor: you know, have never found them. Which is why we're discovering new things with these Lls. The.

728
01:11:42.520 --> 01:11:50.489
Anthony Taylor: it's just. It's just so hard now. Anybody else or anybody else want to comment on this before we I read this little thing I brought up

729
01:11:51.840 --> 01:11:55.690
Masarirambi, Rodney: so like I thought that I thought it was interesting, cause

730
01:11:56.430 --> 01:12:17.219
Masarirambi, Rodney: I didn't think that the there was a basis for the guy to say like, Oh, write me the thing about like a biography about myself, because unless if they were like a famous person, I don't expect it to. And I think maybe he was trying to figure out like, Oh, did he actually scrape my information? I think that's where the the research was probably coming from.

731
01:12:17.340 --> 01:12:20.660
Masarirambi, Rodney: But I do think that if

732
01:12:20.910 --> 01:12:24.790
Masarirambi, Rodney: he comes back and says he wants to have that information corrected.

733
01:12:26.420 --> 01:12:29.570
Masarirambi, Rodney: if he's putting in that request. I'm actually okay with it.

734
01:12:30.030 --> 01:12:40.649
Masarirambi, Rodney: but not the other way around. And I thought that was like interesting like. And we're starting to have that debate us to is that when we when we got when we got cut off. But I thought that was like an interesting thing, because

735
01:12:40.680 --> 01:12:48.220
Masarirambi, Rodney: it's it's very rare that. You see. somebody wanting to have their data put into

736
01:12:48.700 --> 01:12:53.830
Masarirambi, Rodney: an Llm. II guess, is like, go from the first. It's the first time I've seen it. So, yeah.

737
01:12:54.070 --> 01:12:59.560
Clayton Graves: But how would you go about correcting it? I mean that the way that this is the way this works

738
01:12:59.830 --> 01:13:04.680
Clayton Graves: this is not an AI that updates itself in real time. So

739
01:13:04.800 --> 01:13:08.610
Clayton Graves: it it's you'd you'd basically have to submit a new model.

740
01:13:09.670 --> 01:13:12.969
Anthony Taylor: Well, train a new model which, by the way takes months

741
01:13:13.390 --> 01:13:19.659
Anthony Taylor: to do so. There's it's not something that simple. Now, I will say like, if you look at copilot

742
01:13:19.760 --> 01:13:24.759
Anthony Taylor: like being co-pilot, I mean, it's actually searching the live Internet, getting answers

743
01:13:25.030 --> 01:13:30.570
Anthony Taylor: right? Which actually, in my opinion, brings up a whole nother set of potential

744
01:13:30.770 --> 01:13:35.280
Anthony Taylor: issues. Because it's I mean, you could put up a website now.

745
01:13:35.450 --> 01:13:46.010
Anthony Taylor: set up proper SEO, so that it gets picked up by a search engine and then asking a question. And it might go find your website and use your website to answer questions.

746
01:13:46.930 --> 01:13:57.789
Anthony Taylor: Okay? Which goes back to them talking about well, misinformation. And all of this kind there. But guys, Google's been doing this for for freaking 40 years?

747
01:13:59.240 --> 01:14:00.190
Anthony Taylor: Right?

748
01:14:00.310 --> 01:14:07.980
Anthony Taylor: Do they give you like? The the only difference between Google and Chat Gp is the way they answer the question.

749
01:14:08.850 --> 01:14:13.750
Anthony Taylor: Google just gives you a bunch of articles and says, Go find the answer in these articles.

750
01:14:14.430 --> 01:14:21.060
Anthony Taylor: But we're gonna pick which articles you get to see first. and which articles you get to see. Period

751
01:14:22.390 --> 01:14:25.369
Anthony Taylor: chat Tbt's doing the same thing. It's just aggregate.

752
01:14:26.180 --> 01:14:37.260
Anthony Taylor: It's finding relationships based on your prompt aggregating it into a readable format that you then read that readable format was determined by what, when determined by you.

753
01:14:39.550 --> 01:14:47.510
Anthony Taylor: Okay, it was determined by the programmers, and how they train the model. Now I will tell you. It is very easy

754
01:14:48.350 --> 01:14:51.070
Anthony Taylor: to change the way Chat Cbt answers.

755
01:14:52.800 --> 01:14:56.800
Anthony Taylor: okay, you can make it answer you in almost any format you want.

756
01:14:56.930 --> 01:15:09.889
Anthony Taylor: but and which is cool because the developers are like, we're cool with that. Go ahead. But if you leave it as default. You're gonna get the answer, the way that they decide to give it to you. Okay? Which I you know, I'd be interested in knowing is like

757
01:15:10.000 --> 01:15:11.040
Anthony Taylor: where?

758
01:15:11.420 --> 01:15:16.520
Anthony Taylor: Because it's basically written in a style, it's like, Well, what style is it written as a default?

759
01:15:17.080 --> 01:15:23.630
Anthony Taylor: Because you can have it right in like Mark Twain or Dr. Seuss, you know, whatever you want, and he'll do it.

760
01:15:23.830 --> 01:15:27.610
Anthony Taylor: But it's interesting to think that. I wonder what the default style actually is.

761
01:15:28.160 --> 01:15:34.590
Clayton Graves: II would like to point out that even co-pilot, which is supposedly a live aye.

762
01:15:34.690 --> 01:15:40.089
Clayton Graves: the most, most, the most they can tell me about myself is my location

763
01:15:40.270 --> 01:15:45.509
Clayton Graves: and my language. Let's let's ask you

764
01:15:45.570 --> 01:15:49.989
Raugewitz, Tania: the most. It will tell you about yourself versus what it can tell you about yourself.

765
01:15:50.060 --> 01:15:52.069
Anthony Taylor: Yeah, that's a good point, too, Tony.

766
01:15:52.510 --> 01:15:57.339
Anthony Taylor: But what about Google? When you Google, Google. how many links come up about you?

767
01:15:57.940 --> 01:15:59.329
Clayton Graves: Quite a few more?

768
01:15:59.750 --> 01:16:05.820
Anthony Taylor: Yeah. So it's a. So again, a lot of the stuff is very specific to people just being afraid of the unknown.

769
01:16:06.440 --> 01:16:12.289
Anthony Taylor: Right? We're all worried about Terminator, I mean, I can't tell you how many times I always like Terminator Skynet.

770
01:16:12.530 --> 01:16:14.470
Anthony Taylor: Right? No, it's not.

771
01:16:14.640 --> 01:16:18.250
Anthony Taylor: But that's when a lot of people are associating these AI Su.

772
01:16:18.560 --> 01:16:23.469
Anthony Taylor: right? And and I mean, Google's been doing the same thing for years.

773
01:16:23.850 --> 01:16:27.880
Anthony Taylor: They just don't output the result as nicely as Chat Gp does.

774
01:16:28.510 --> 01:16:32.710
Anthony Taylor: or give us the ability to customize, and it's up to us

775
01:16:33.350 --> 01:16:38.609
Anthony Taylor: as a I professionals and data professionals, one to educate.

776
01:16:38.960 --> 01:16:46.960
Anthony Taylor: I probably give the speech that Chad Gpt is not going to replace your job once a week. Minimal.

777
01:16:48.070 --> 01:16:52.910
Anthony Taylor: Okay? And if it does, you have a crappy job you need to get. You know they'll take my bootcamp.

778
01:16:53.730 --> 01:17:00.779
Anthony Taylor: Okay? You know that we have to educate. And 2, we need to, you know,

779
01:17:02.700 --> 01:17:07.739
Anthony Taylor: we need to understand how it works. So we can make it work better for the people who

780
01:17:07.970 --> 01:17:14.900
Anthony Taylor: are afraid of it. Make them understand. What can this do for you? I I've had a number. I

781
01:17:15.550 --> 01:17:19.599
Anthony Taylor: okay, I'm not. Gonna so I had this friend with a nurse

782
01:17:19.750 --> 01:17:28.549
Anthony Taylor: right? I showed her how we could type in information and get like a template for her notes every day that she was spending hours

783
01:17:28.640 --> 01:17:29.830
Anthony Taylor: putting together.

784
01:17:30.100 --> 01:17:32.150
Anthony Taylor: Okay, without

785
01:17:32.460 --> 01:17:34.810
Anthony Taylor: patient, identifiable information.

786
01:17:35.040 --> 01:17:39.759
Anthony Taylor: I'm II have friends. She's a special education teacher.

787
01:17:39.870 --> 01:17:50.750
Anthony Taylor: We were talking about like the the like, a lot of the things that they have to do, the paperwork they have to document, how you could type in just the base information and Bam

788
01:17:50.880 --> 01:17:56.380
Anthony Taylor: get this beautiful flipping document that was as good as any teacher has ever done

789
01:17:57.000 --> 01:18:08.310
Anthony Taylor: in seconds. But it was just because I know how to make the tool work for us. Right? And that's our job as AI professionals. I did. By the way, just for fun.

790
01:18:08.410 --> 01:18:15.549
Anthony Taylor: I summarized that article with copilot, and it did a decent job. Then I said, because it's dead

791
01:18:15.990 --> 01:18:21.929
Anthony Taylor: broader implications. The complaint raises the question about the compatibility of dinner. AI with Pdpr.

792
01:18:22.310 --> 01:18:28.840
Anthony Taylor: And such technologies. Blah blah blah. And I said, you know what address those broader implications for me

793
01:18:29.600 --> 01:18:31.120
Anthony Taylor: and did

794
01:18:31.200 --> 01:18:42.070
Anthony Taylor: came up with suggestions on how we could address, and and it mostly mostly makes sense transparency. Engage with the use professionals on what you're doing, how you're doing it.

795
01:18:42.150 --> 01:18:50.470
Anthony Taylor:  they should. They should be assessing the impact of Chat Gbt on individuals data. How much is they really gonna do?

796
01:18:50.920 --> 01:18:55.110
Anthony Taylor: And you know. But but this is a way they could

797
01:18:55.380 --> 01:18:59.350
Anthony Taylor: try to limit Gdpr's impact. Yes, ron.

798
01:19:00.350 --> 01:19:05.620
Masarirambi, Rodney: so I suspect this person is actually trying to mobilize Openai.

799
01:19:05.810 --> 01:19:14.290
Anthony Taylor: Because, yeah, well, yeah. But in in in, I thought, and I think it actually really clever way, because

800
01:19:15.360 --> 01:19:16.990
because of the way that

801
01:19:17.090 --> 01:19:19.720
Masarirambi, Rodney: they specifically targeted this.

802
01:19:19.730 --> 01:19:23.860
Masarirambi, Rodney: They knew enough about EU law and Gd. Gdpr. That

803
01:19:23.930 --> 01:19:43.720
Masarirambi, Rodney: request, like the request that they did, and also being able to ask it to a ask the company to edit information about themselves again. I don't think it's reasonable that this person thinks that they, like a biograph for them, could be done like a true one. So therefore, if you ask them to adjust this.

804
01:19:43.860 --> 01:19:47.610
Masarirambi, Rodney: then, that opened the floodgates. Or let's just say.

805
01:19:48.040 --> 01:19:53.130
Masarirambi, Rodney: a million people in in in in the European Union start start to request this

806
01:19:53.390 --> 01:20:04.320
Masarirambi, Rodney: at that point in time. Each time that's requested, a new model has to be trained, which is gonna start raising up the costs. And it's it's gonna make it. And I think you could slow it down till all. And I think it's

807
01:20:05.130 --> 01:20:16.599
Anthony Taylor: I think it's kind of genius do I agree with? That's the way to go. Maybe I don't know about that, but I think it's actually a genius.

808
01:20:16.750 --> 01:20:21.069
Anthony Taylor: I mean, and then I hate to be the you know, the money's gonna win.

809
01:20:21.110 --> 01:20:27.460
Anthony Taylor: But in this case not only will money win. If you took Chat, gpt away from people today.

810
01:20:29.470 --> 01:20:31.139
Anthony Taylor: there would be.

811
01:20:31.660 --> 01:20:40.010
Anthony Taylor: I mean, the loudest outcry we've heard in a very long time. Alright, it is big enough now that it's own beasts.

812
01:20:40.080 --> 01:20:42.969
Anthony Taylor: and and people are already depending on it

813
01:20:43.070 --> 01:20:52.570
Anthony Taylor: pretty consistently. you know, across the board. But that being said, I mean. look at who's building these things and who's selling them?

814
01:20:52.720 --> 01:20:54.920
Anthony Taylor: Who's making money off of these things?

815
01:20:55.120 --> 01:20:57.739
Anthony Taylor: The biggest companies in the world.

816
01:20:58.000 --> 01:21:02.079
Clayton Graves: It is making money off of this amazingly scary.

817
01:21:02.470 --> 01:21:06.089
Clayton Graves: How much I've come to depend on it, just in 4 months.

818
01:21:06.580 --> 01:21:23.949
Clayton Graves: I didn't even have a Chat Gp account before this. Something else out here, too, Pilot actually is based on data that is older than Chat Gvt. 4.

819
01:21:24.160 --> 01:21:27.710
Clayton Graves: It does not. No, it is chatty. P. 3. Boarding.

820
01:21:28.140 --> 01:21:39.340
Clayton Graves: It does not have direct access to personal information or external databases. It's based on publicly available text data up to a certain point in time around 2,021.

821
01:21:41.110 --> 01:21:44.920
Anthony Taylor: Wait. Well, hold on. You're talking about this co-pilot.

822
01:21:45.500 --> 01:21:55.430
Clayton Graves: Yeah, this is this is the way I test this. This is Microsoftcom.

823
01:21:56.000 --> 01:21:59.139
Anthony Taylor: Well, I'm on one that's built into Bing, and

824
01:21:59.820 --> 01:22:03.490
Anthony Taylor: I just asked who won the super bowl in 2,024.

825
01:22:04.770 --> 01:22:12.909
Clayton Graves: Well, you're right. My right? My understanding is that it'll search the Internet. It can search and Internet publicly available. Internet. Yes.

826
01:22:13.400 --> 01:22:26.820
Clayton Graves: but actual data that it that it was. It was trained on it. It it it's just like Chat G. Ppt is something. I tell you. It is chat Ppt, and it's got to be Chat GPT. 3.

827
01:22:27.070 --> 01:22:30.589
Anthony Taylor: It could be an older version, because it wouldn't benefit.

828
01:22:30.620 --> 01:22:40.009
Anthony Taylor: It wouldn't benefit the people of Chat Cpt to allow Microsoft to give it away for Free and Microsoft has a huge interest in opening up.

829
01:22:40.250 --> 01:22:44.810
Anthony Taylor: so it is very likely that open AI will always be slightly better

830
01:22:44.850 --> 01:22:53.300
Anthony Taylor: then, being co-pilot unless Microsoft outright buys them. But I'm gonna tell you guys, co pilots, really.

831
01:22:53.600 --> 01:23:01.390
Anthony Taylor: And not only that. If you ask the same question. Actually, you know what I haven't done this a long time. Let's find out

832
01:23:03.260 --> 01:23:06.849
Anthony Taylor: if you ask this same question to Openai.

833
01:23:07.990 --> 01:23:11.519
Anthony Taylor: It should tell me. No, oh, no, it's gonna it searches bing.

834
01:23:12.360 --> 01:23:13.140
Anthony Taylor: So.

835
01:23:13.300 --> 01:23:14.400
Raugewitz, Tania: But okay.

836
01:23:14.730 --> 01:23:22.530
Raugewitz, Tania: so to that point, Anthony, like I was wondered why you always did chat gpt versus copilot.

837
01:23:22.620 --> 01:23:31.660
Raugewitz, Tania: because I like, well, cause copilot. Wasn't that good until recently? My opinion it wasn't. I still like Chat Pbt, a little better.

838
01:23:31.720 --> 01:23:33.220
Anthony Taylor: But

839
01:23:34.070 --> 01:23:46.550
Anthony Taylor: yeah, yeah, II I'll be honest. I've been using copilot more, more, partly because, like, I'll be in here and I'll be like, Oh, well, who's this pinball guy? And you could just go in here

840
01:23:46.650 --> 01:23:49.029
Anthony Taylor: and ask copilot and it just

841
01:23:49.220 --> 01:23:54.339
Anthony Taylor: if you're in bing, I mean, it's that easy to get co-pilot to answer a question.

842
01:23:54.550 --> 01:23:57.660
Meredith McCanse (she/her): Do you, Anthony, have a question?

843
01:23:58.140 --> 01:24:11.099
Anthony Taylor: Do you have to? Are you use what browser are using? Can you only do co-pilot in that sidebar in edge? I think you only get the sidebar in edge. Sorry edge

844
01:24:11.190 --> 01:24:14.549
Anthony Taylor: but I don't know. Has anybody tried it in chrome or safari?

845
01:24:14.690 --> 01:24:25.889
Anthony Taylor: I've tried it in chrome. I can't get it opens a new tab, but I can't get it to do what you have on your screen now where it

846
01:24:25.930 --> 01:24:32.200
Raugewitz, Tania: yeah, go ahead. Sorry. No, I'm in chrome and I have copilot on my side. View.

847
01:24:32.710 --> 01:24:35.519
Anthony Taylor: Oh. you have an extension or something.

848
01:24:36.300 --> 01:24:40.029
I first opened it up in in my Microsoft office.

849
01:24:40.100 --> 01:24:42.300
Raugewitz, Tania: and then I went, and it

850
01:24:42.560 --> 01:24:47.959
Raugewitz, Tania: stayed in my sidebar. I mean, I can show you my screen. and then I but I have.

851
01:24:48.000 --> 01:24:50.489
Raugewitz, Tania: Chrome is my browser.

852
01:24:50.700 --> 01:24:51.890
Raugewitz, Tania: and they're both there.

853
01:24:52.010 --> 01:25:01.100
Meredith McCanse (she/her): Oh, and it shows up on your screen like Anthony screen, where it's like part of it, just on the side. Yeah, I can scroll on my copilot and it goes back.

854
01:25:01.210 --> 01:25:02.529
Raugewitz, Tania: Oh, dear, okay.

855
01:25:03.470 --> 01:25:17.999
Anthony Taylor: Share screen. I'm sure it's an extension of some sort, cause I try. I installed in a chrome extension and tried to do it, and for the life of me couldn't get it to do anything other than just open a separate tab

856
01:25:18.050 --> 01:25:22.039
Anthony Taylor: I love. Okay? So

857
01:25:22.310 --> 01:25:28.039
Anthony Taylor: okay, I see you have it. But it looks like, is that part of the browser? I guess it is. Huh?

858
01:25:28.460 --> 01:25:33.509
Raugewitz, Tania: Maybe I split my screen. May I get a slip on? No, I didn't split my screen.

859
01:25:33.930 --> 01:25:37.640
Anthony Taylor: but hit the little pop out arrow, or how do you get it to come up

860
01:25:37.880 --> 01:25:38.910
Raugewitz, Tania: a

861
01:25:39.430 --> 01:25:42.830
Raugewitz, Tania: I can open it in Microsoft Edge.

862
01:25:42.860 --> 01:25:50.800
Raugewitz, Tania: I did this. Oh, oh, you're doing it from windows. Oh, that's windows, co-fire!

863
01:25:51.110 --> 01:26:02.280
Anthony Taylor: So that's part of windows. 11. Okay? So that explains it. So you're yes, you basically have split your creed. And that's the windows co-pilot, which is the same thing as the one on it.

864
01:26:02.470 --> 01:26:09.829
Anthony Taylor: So that's cool, though. No, I like I actually do that. I like III will say, Guy, I really, really.

865
01:26:10.250 --> 01:26:21.540
Anthony Taylor: if you, as far as the browser wars go. I'm not gonna get into it too much. But I mean Edge has been edging out from for quite a while.

866
01:26:21.700 --> 01:26:29.619
Anthony Taylor: as far as capability and performance and all that kind of stuff, and from a technical perspective, you can still have your preferences. That's fine.

867
01:26:29.980 --> 01:26:34.340
Anthony Taylor: But anyway, and this little sidebar really fun, cause you can get

868
01:26:35.860 --> 01:26:38.630
Anthony Taylor: it has, like the designer stuff and everything in here

869
01:26:38.710 --> 01:26:42.140
Anthony Taylor: as well. So you can like create an image and like a

870
01:26:42.350 --> 01:26:43.520
Anthony Taylor: split second.

871
01:26:43.710 --> 01:26:50.170
Anthony Taylor: Pretty cool, anyway. Okay, enough of that. Let's bought back to our material.

872
01:26:50.820 --> 01:26:52.970
Anthony Taylor: We only have a little bit left.

873
01:26:56.810 --> 01:26:57.619
Anthony Taylor: There we go

874
01:26:59.020 --> 01:27:04.950
Anthony Taylor: alright. So AI specific regulations. This is pretty much it for today.

875
01:27:05.270 --> 01:27:16.590
Anthony Taylor:  so they talk about this one in particular, the EU Europe, the European. AI.

876
01:27:19.470 --> 01:27:29.180
Anthony Taylor: This is an attempt to control the Llms. And what they're allowed to do and what they're allowed to know. And

877
01:27:32.150 --> 01:27:33.210
Anthony Taylor: I mean.

878
01:27:34.350 --> 01:27:40.820
Anthony Taylor: I don't know. What's it? At the time that this was written. It had not asked yet.

879
01:27:41.200 --> 01:27:46.459
Anthony Taylor: I looked it up. It's supposed to pass sometime in 2,024.

880
01:27:47.020 --> 01:27:51.050
Anthony Taylor:  There's so many things here that bother me.

881
01:27:51.470 --> 01:27:52.290
Anthony Taylor: But

882
01:27:52.820 --> 01:28:04.130
Anthony Taylor: does it matter that it bothers me? Oh, really, I'm not gonna have any. Say it, but it's only Europe. So I don't right. They're not gonna do it in the States States is gonna succumb to the money. And we're not worried about. But

883
01:28:04.390 --> 01:28:08.420
Anthony Taylor: what kind of things? Unacceptable risk, what is unacceptable risk?

884
01:28:08.600 --> 01:28:10.839
Anthony Taylor: Well, does the system pose a danger?

885
01:28:12.290 --> 01:28:13.330
Anthony Taylor: I don't. Does it?

886
01:28:13.690 --> 01:28:19.690
Anthony Taylor: To people being smarter, or have more access to data, make, you know, create a danger.

887
01:28:20.170 --> 01:28:23.089
Anthony Taylor: Well. some people might say, Yeah.

888
01:28:25.060 --> 01:28:34.929
Anthony Taylor: alright. Even if it's not dangerous, quote dangerous data, right? You know, we know that these ais know how to create pump, but it won't tell you how to do it.

889
01:28:36.420 --> 01:28:41.320
Anthony Taylor: Alright. So is that unacceptable risk. Just the fact that it knows how to do.

890
01:28:42.810 --> 01:28:46.549
Anthony Taylor: Is it high risk that it knows how to do?

891
01:28:48.600 --> 01:28:51.100
Raugewitz, Tania: But that that data is out there?

892
01:28:51.870 --> 01:28:56.340
Mason, Natalie: Oh, yeah. Well, the data is on the Internet. Anyway, you can Google it.

893
01:28:56.580 --> 01:29:04.550
Anthony Taylor: You can go. You could Google it. You could go to the dark web and Google it. You could go. I mean the the irony to the whole thing is, it just feels like

894
01:29:04.910 --> 01:29:05.790
Anthony Taylor: to me.

895
01:29:06.830 --> 01:29:11.010
Anthony Taylor: And this and this is one of those my opinion. This is not

896
01:29:11.150 --> 01:29:23.200
Anthony Taylor: even based on anything other than being in the business for years. Okay. it. It feels like to me that they just want to stop it because it is. It's a disruption.

897
01:29:25.170 --> 01:29:32.850
Anthony Taylor: Okay, it's like the first motorized vehicle. It's like the first automated factory machines.

898
01:29:33.970 --> 01:29:35.409
Anthony Taylor: It's like Google.

899
01:29:37.950 --> 01:29:41.980
Anthony Taylor: when it first came out the Internet when it first came out.

900
01:29:42.000 --> 01:29:51.900
Anthony Taylor: Everything was a disruptor, and there will always be people that want to stop the disruptors. And and maybe they're right.

901
01:29:52.510 --> 01:29:57.459
Anthony Taylor: And maybe they're only kind. Right? This is definitely one of those moments where

902
01:29:57.470 --> 01:30:07.400
Anthony Taylor: there's absolutely right. And there's absolutely wrong. And these guys are almost definitely somewhere in the middle. And in the end, my opinion.

903
01:30:07.740 --> 01:30:12.740
Anthony Taylor: we need to weigh more towards advancing period.

904
01:30:13.740 --> 01:30:20.539
Anthony Taylor: I was gonna say, advancing as a as the human beings. But let's advance intelligence

905
01:30:22.050 --> 01:30:27.890
Anthony Taylor: right? This is advancing our intelligence. It's allowing us to

906
01:30:30.450 --> 01:30:40.770
Anthony Taylor: offload the easy stuff. and even some of the hard stuff so that we can do harder stuff.

907
01:30:42.800 --> 01:30:46.259
Anthony Taylor: And in the end, that's really what these things are going to give us.

908
01:30:47.470 --> 01:30:51.640
Anthony Taylor: right? The easy stuff, the easy jobs they're gonna go away.

909
01:30:52.540 --> 01:30:55.319
Anthony Taylor: They're going to fade away. There is no question.

910
01:30:55.440 --> 01:31:04.369
Anthony Taylor: The jobs that require technology or knowledge that are simple are going to fade away or change drastically.

911
01:31:05.370 --> 01:31:12.060
Anthony Taylor: but that means we just have to. We. We are as as a people are going to have to take a step up

912
01:31:13.670 --> 01:31:15.919
Anthony Taylor: and do more be smarter.

913
01:31:17.220 --> 01:31:21.820
Anthony Taylor: Okay. that's my opinion. Take it or leave it.

914
01:31:21.870 --> 01:31:29.749
Masarirambi, Rodney: I mean, I think you're right. I think you're. I think you're right, because I think so. My, so I believe that there's

915
01:31:29.820 --> 01:31:35.600
Masarirambi, Rodney: truly an ex existential threat that lies in AI somewhere. However.

916
01:31:35.760 --> 01:31:41.539
Masarirambi, Rodney: what everybody's been saying, everybody's like worried about. I don't think that's it. I don't think we've seen it.

917
01:31:41.750 --> 01:32:02.699
Masarirambi, Rodney: And and and to resume that it's not there is like just poly. I'm not saying it's not there. But but it's not what everybody is freaking out about right now, like that's, I think, right now it's a growth, and we need to grow and and and maybe we'll get smart enough to stop if it's going to get. I don't even know what out of control is, gonna be

918
01:32:03.700 --> 01:32:07.499
Anthony Taylor: right, because at this point. It has no control over anything

919
01:32:07.590 --> 01:32:18.359
Anthony Taylor: autonomously right everything is, I mean, at least to this point. Now, I mean, they're showing there's robots. Oh, there's a robot that could play basketball.

920
01:32:18.900 --> 01:32:20.040
Okay.

921
01:32:20.350 --> 01:32:23.400
Anthony Taylor: I'm ready to see robot basketball. I paid for it.

922
01:32:23.920 --> 01:32:25.870
Anthony Taylor: Okay,

923
01:32:26.390 --> 01:32:27.110
Anthony Taylor: but

924
01:32:27.790 --> 01:32:30.740
Anthony Taylor: I mean, I don't know. You know, we just have to.

925
01:32:30.790 --> 01:32:35.770
Anthony Taylor: You know, we go to a robot basketball game and they start killing everybody in the audience. That would be bad.

926
01:32:36.410 --> 01:32:40.820
Anthony Taylor: right? Be like it'd be like celebrating super bowl in Kansas City.

927
01:32:41.030 --> 01:32:45.140
Anthony Taylor: Oh, too soon. Okay, anyway.

928
01:32:45.370 --> 01:32:50.030
Anthony Taylor: The point is. passionately.

929
01:32:50.370 --> 01:32:53.679
Anthony Taylor: but it's I that happens every year.

930
01:32:53.740 --> 01:32:57.350
Anthony Taylor: Every time a dang championship is won anywhere.

931
01:32:57.360 --> 01:33:04.600
Anthony Taylor: People get hurt. It's the stupidest thing I've ever san Antonio solved that problem. We just don't have a winning team ever.

932
01:33:05.370 --> 01:33:07.730
Raugewitz, Tania: That's why we can't have nice things, isn't it?

933
01:33:07.870 --> 01:33:09.499
Anthony Taylor: That's exactly right.

934
01:33:10.050 --> 01:33:16.289
Anthony Taylor: Duck on it. I don't need the Lombardi trophy. Yeah, Derek, Derek's gonna yell at me.

935
01:33:16.480 --> 01:33:18.490
Anthony Taylor: Like. I live in Kansas City.

936
01:33:18.540 --> 01:33:30.060
Derek Rikke: That's my people, though that's my people. I don't know. There's something I heard from Mark Andrewson, but more responding to what Rodney said. I think that

937
01:33:30.850 --> 01:33:33.939
Derek Rikke: that's like a big assumption. Everyone.

938
01:33:34.430 --> 01:33:49.680
Derek Rikke: It's just like everyone is making a big assumption that there's something like nuclear bomb level that can happen. And I don't. I just think that's just a big assumption that everyone thinks is true. And there's not really

939
01:33:49.980 --> 01:33:54.950
Derek Rikke: proof that that is true or nobody can understand it well enough.

940
01:33:55.110 --> 01:34:01.369
Derek Rikke: And it's just like that's a big assumption that everyone is just making and basing all this legislation and stuff on.

941
01:34:01.570 --> 01:34:13.490
Derek Rikke: And it's like people should have to have some amount of proof to say that I don't know. It's like trying to prove that God is real or something. It's like such an impossible conversation to have.

942
01:34:14.790 --> 01:34:31.660
Anthony Taylor: Somebody should have proof to say, stuff.

943
01:34:32.200 --> 01:34:42.649
Clayton Graves: You know, nuclear technology is a fair and valid comparison. They're both game, changing technologies, life changing technologies

944
01:34:43.080 --> 01:34:49.819
Clayton Graves: that will forever alter the way human beings interact with the world around them

945
01:34:50.090 --> 01:34:51.910
Clayton Graves: forever. I agree.

946
01:34:53.690 --> 01:35:01.669
Anthony Taylor: I agree. I agree. I agree with that. II would thank God you'd go down the other path. Oh, I just did, anyway.

947
01:35:02.040 --> 01:35:11.310
Anthony Taylor: yeah, it. I mean, I said it to you guys, early on the best quote I heard at the AI summit this year is my child's life will never be the same.

948
01:35:12.410 --> 01:35:17.289
Anthony Taylor: Once I show them this. AI, and they learn how to use it. They will never have a day without

949
01:35:17.380 --> 01:35:24.030
Anthony Taylor: none of us in this room. None of our children, our children's children will know life without an AI,

950
01:35:24.990 --> 01:35:47.679
Anthony Taylor: and it's you're right. 100. Yeah, I don't know. I've got nuclear. Seems more like it's destructive thing and a deterrent, while AI seems for like, it's gonna help us all. Pro is a race is the people you look at nuclear reactors that provide power for thing. Okay, it's no different than fire.

951
01:35:48.320 --> 01:35:57.879
Anthony Taylor: No, II agree. I agree with you. No, you're right. I didn't. I didn't. II was going nuclear bombs. But you're right, your nuclear technology in general definitely is

952
01:35:58.260 --> 01:35:59.929
Anthony Taylor: affected us. So anyway.

953
01:36:00.130 --> 01:36:05.290
Anthony Taylor: okay, so you asked AI legislation, there is a lot of legislation.

954
01:36:05.680 --> 01:36:13.180
Anthony Taylor:  I think I think in the Us. More than the rest of the world, because Gdpr got passed in Europe.

955
01:36:13.500 --> 01:36:16.829
Anthony Taylor: Which tells me the lobbyists are strong in Europe.

956
01:36:17.440 --> 01:36:25.589
Anthony Taylor:  I think I personally think that anything that it comes up in the US. Will either be

957
01:36:25.600 --> 01:36:27.250
Anthony Taylor: slaps on the wrist.

958
01:36:27.510 --> 01:36:29.799
Anthony Taylor: There are no teeth at all.

959
01:36:30.410 --> 01:36:31.709
Anthony Taylor: Right again.

960
01:36:31.750 --> 01:36:35.109
Anthony Taylor: Biggest companies in the world

961
01:36:35.780 --> 01:36:44.040
Anthony Taylor: are the ones making money off of this. Alright. So you've got to get past Microsoft Apple Ivm.

962
01:36:44.180 --> 01:36:48.720
Anthony Taylor: you know, a Mic or Microsoft again. And you know, Amazon.

963
01:36:48.820 --> 01:36:51.610
Anthony Taylor: these are all people making big money.

964
01:36:51.720 --> 01:36:56.740
Clayton Graves: and America at its heart is a capitalistic society.

965
01:36:57.910 --> 01:37:07.310
Anthony Taylor: Well, and we all know that those that have the money are gonna control most of the legislation. So like I said, I do believe there will be legislation.

966
01:37:07.980 --> 01:37:12.200
Anthony Taylor: I do believe there will be loved. I just don't believe they're going to be big enough

967
01:37:12.350 --> 01:37:18.479
Anthony Taylor: that anyone's gonna care 25 million to Amazon because they recorded children's voices, but

968
01:37:18.610 --> 01:37:23.660
Anthony Taylor: they also got enough data to do what they needed to do. They're like, Whoa, damn!

969
01:37:23.810 --> 01:37:24.970
Anthony Taylor: We got off cheap.

970
01:37:26.320 --> 01:37:30.900
Anthony Taylor: right? And I mean they don't care, or they care. They made that yesterday.

971
01:37:33.270 --> 01:37:45.619
Anthony Taylor: you know, it's like, Oh, okay, okay. So anyway, I mean, and I'm not. I'm not against capitalism. I'm a hundred percent for it. I'm a hundred percent thinking that these products will move us forward.

972
01:37:45.730 --> 01:37:48.790
Anthony Taylor: I'm also a hundred percent aware that

973
01:37:49.500 --> 01:37:51.480
Anthony Taylor: there will always be a bad element.

974
01:37:53.550 --> 01:37:59.810
Anthony Taylor: Okay, no matter what we make, no matter what we create. someone will find a way to misuse them.

975
01:38:02.060 --> 01:38:03.990
Anthony Taylor: And when they do.

976
01:38:04.650 --> 01:38:07.200
Anthony Taylor: I mean, that's just the way it is. We have to stop the people.

977
01:38:07.460 --> 01:38:08.720
Anthony Taylor: not the product

978
01:38:08.790 --> 01:38:15.159
Clayton Graves: I know. Get into what that sounds like.

979
01:38:16.440 --> 01:38:22.859
Anthony Taylor: You're right, you know, but I mean, the dark web is such a perfect example. I know it's like second time in a week dark web's come.

980
01:38:22.890 --> 01:38:26.320
Anthony Taylor: But Dark Web is a perfect example. We took the Internet.

981
01:38:26.360 --> 01:38:40.549
Anthony Taylor: we found a way to hide it. make it completely anonymous. pretty much untraceable. And now look at what's going up. And I didn't just discovered a dark web recently. I've known about it for long.

982
01:38:40.690 --> 01:38:41.460
Anthony Taylor: Yeah.

983
01:38:42.230 --> 01:38:45.640
Anthony Taylor: okay. And it's not a pretty place to be

984
01:38:47.610 --> 01:38:51.270
Anthony Taylor: okay. If you ever want to go there. Just side note.

985
01:38:51.410 --> 01:39:01.229
Clayton Graves: it, isn't it true that the the the part of the Internet that we're actually aware of and interact with every day is really small

986
01:39:01.240 --> 01:39:12.579
Clayton Graves: small percentage of the entire Internet is, is that something? That's I would say. Most people only deal with a small percentage of the Internet every day, you know, cause it's so freaking massive.

987
01:39:12.750 --> 01:39:13.570
Anthony Taylor: But

988
01:39:13.880 --> 01:39:22.599
Anthony Taylor: you know I mean I do. I think the dark web makes up more than the regular web. No, not even close. Do I think there are stuff in the regular web that are

989
01:39:22.880 --> 01:39:26.659
Anthony Taylor: unethical or dangerous or illegal 100%? Yes.

990
01:39:27.060 --> 01:39:36.789
Anthony Taylor: you know. But it's pretty easy to cycle through IP numbers. Right? I can. You know, I've shown you guys my VPN, that'll make it appear on anywhere in the world.

991
01:39:39.120 --> 01:39:44.369
Anthony Taylor: Okay? And then I can go browse as if I'm in Brazil can go browse as if I'm in

992
01:39:44.700 --> 01:39:56.280
Anthony Taylor: Croatia. Nobody anywhere will know the difference. Just looks like I'm there. So there's a lot of ways to do that kind of stuff.  but

993
01:39:57.660 --> 01:40:02.409
Anthony Taylor: like, I said, if they all ams get used in that way and they will. We're already seeing.

994
01:40:03.100 --> 01:40:09.780
Anthony Taylor: I already told you guys about the Youtube sites uncensored, Llama, uncensored, stable diffusion

995
01:40:11.010 --> 01:40:13.770
Anthony Taylor: easy to do. Any one of you could do it on your laptops.

996
01:40:15.960 --> 01:40:24.630
Anthony Taylor: Okay? But we as AI professionals, that data professionals have a responsibility to educate.

997
01:40:25.450 --> 01:40:29.569
Anthony Taylor: to ensure that the companies we work for are using it properly

998
01:40:30.290 --> 01:40:33.100
Anthony Taylor: as much as we can, much as we have within our.

999
01:40:33.370 --> 01:40:34.300
Anthony Taylor: Okay.

1000
01:40:34.640 --> 01:40:41.980
Anthony Taylor: Anyway, there's some academic research and regulations. Oxford has a great one.

1001
01:40:42.400 --> 01:40:44.650
Anthony Taylor: it's another one that you can.

1002
01:40:45.300 --> 01:40:49.790
Anthony Taylor: You know a lot of these? If you do. II mean honestly go to the site

1003
01:40:49.880 --> 01:41:01.210
Anthony Taylor: and and do like the copilot summarize. It'll take this really, really long documents.

1004
01:41:01.610 --> 01:41:02.950
There's actually

1005
01:41:03.380 --> 01:41:09.470
Anthony Taylor: one of these has, like a straight up link. Think it? I don't know. It just shows you the whole document.

1006
01:41:09.670 --> 01:41:14.429
Anthony Taylor: And you can just like, do summarize with big chat and or chat. Gp, and

1007
01:41:14.730 --> 01:41:20.049
Anthony Taylor: you'll you'll get a nice summary, or you can read all thing if you really want to. It's really cool. If you want to.

1008
01:41:20.360 --> 01:41:23.240
Anthony Taylor: Anybody know about the tureen test.

1009
01:41:25.050 --> 01:41:29.090
Clayton Graves: it's basically a way to test. If artificial intelligence!

1010
01:41:29.390 --> 01:41:30.560
Acts like a human

1011
01:41:31.230 --> 01:41:35.950
Anthony Taylor: it's actually incapable of of

1012
01:41:36.550 --> 01:41:41.679
Clayton Graves: actually it would be incapable of accurately determining

1013
01:41:41.710 --> 01:41:54.109
Clayton Graves: this one. And there's another one I forget what it's called, but both of that and the other ones newer. But there, neither one of them are capable of accurately determining.

1014
01:41:56.190 --> 01:41:56.900
Good.

1015
01:41:57.270 --> 01:42:06.330
Anthony Taylor: Yeah, I mean, that's that. Yeah, a lot of keynote. But that's a great movie. I love that movie. A keynote machina. What if?

1016
01:42:06.690 --> 01:42:13.080
Anthony Taylor: Okay? So purpose self regulation. Yeah, that's practically an oxymoron.

1017
01:42:13.580 --> 01:42:16.820
Anthony Taylor:  but yeah.

1018
01:42:17.140 --> 01:42:20.620
Anthony Taylor: they they have to do something they have to at least appear

1019
01:42:20.740 --> 01:42:24.060
Anthony Taylor: to be. And I mean a censors. It's weird that Accenture's in here

1020
01:42:24.560 --> 01:42:27.180
Anthony Taylor: because that's a consulting company. So maybe.

1021
01:42:27.360 --> 01:42:35.869
Anthony Taylor: But Ivy at Google Samsung, Sony Microsoft. They're all saying they're doing it. I'm sure they are, because they want to limit

1022
01:42:36.110 --> 01:42:40.160
Anthony Taylor: the the risk and the the fines as much as possible.

1023
01:42:40.410 --> 01:42:44.640
Anthony Taylor: But again, I mean, they're making millions a day off of this stuff.

1024
01:42:45.340 --> 01:42:46.240
Anthony Taylor: right?

1025
01:42:46.360 --> 01:42:50.780
Anthony Taylor: What you know. Okay, you want to find me a hundred $1,000. Okay.

1026
01:42:51.150 --> 01:42:53.040
Anthony Taylor: damn it! Oh.

1027
01:42:55.080 --> 01:42:56.010
Anthony Taylor: it's fine.

1028
01:42:56.120 --> 01:42:59.440
Anthony Taylor: it said. As long as the legislation has no teens.

1029
01:42:59.630 --> 01:43:01.929
Anthony Taylor: They're just gonna keep doing what they're doing. And

1030
01:43:01.960 --> 01:43:12.040
Anthony Taylor: do. I have a problem with that, as I think I've made very, very clear. I think these tools are necessary. Do I think they have a bad sight? Yeah, you can kill somebody with a wrench.

1031
01:43:14.220 --> 01:43:19.490
Anthony Taylor: Okay, but you need a wrench to fix a call. You need a wrench to build a building.

1032
01:43:20.250 --> 01:43:20.940
maybe

1033
01:43:21.270 --> 01:43:22.279
Anthony Taylor: depends on the building.

1034
01:43:23.290 --> 01:43:27.470
Anthony Taylor: Alright. I mean, you know, tools can be used a whole bunch of ways.

1035
01:43:27.830 --> 01:43:31.179
Anthony Taylor: And these tools, I think, have enough value

1036
01:43:31.220 --> 01:43:35.419
Anthony Taylor: that we need to consider that. Okay, I'm done with these stupid activities.

1037
01:43:38.320 --> 01:43:42.010
Clayton Graves: I'm just done with these. Read another article.

1038
01:43:42.360 --> 01:43:59.469
Clayton Graves: One of the things that I was doing while while we were discussing this stuff is, I was asking Chat Gp. You know, if it's possible for AI to someday, evolve to the point where it can regulate itself, detect biases and correct them.

1039
01:43:59.680 --> 01:44:07.149
Clayton Graves: And yes, that that is something that's being researched now. But there are a lot of ethical. And

1040
01:44:07.650 --> 01:44:12.879
Anthony Taylor: you just ask the devil. is it? Okay? If I kill this person, it's like.

1041
01:44:13.340 --> 01:44:15.869
Clayton Graves: what else am I gonna ask.

1042
01:44:15.930 --> 01:44:20.870
Anthony Taylor: I know. Well, I know. Yes, the the bank robber.

1043
01:44:21.100 --> 01:44:24.740
Anthony Taylor: What do you think should make robbery be illegal? Hello!

1044
01:44:25.490 --> 01:44:29.430
Clayton Graves: That that is, that is a fair assessment.

1045
01:44:29.440 --> 01:44:31.960
Clayton Graves: But again welts, am I going to ask?

1046
01:44:32.260 --> 01:44:43.730
Anthony Taylor: I know. I know I mean I mean, I'm supposed to be one of the experts on this topic according to a whole bunch of 2 different advisory councils, and I could tell you I don't have an answer.

1047
01:44:43.980 --> 01:44:47.869
Anthony Taylor: The answer, in my opinion. is, it's a tool.

1048
01:44:48.040 --> 01:44:53.350
Anthony Taylor: and we need to treat it like a tool. And we need to understand that, you know.

1049
01:44:53.540 --> 01:44:57.360
Anthony Taylor: tools can be misused. So it's it's about education.

1050
01:44:57.750 --> 01:45:00.269
Anthony Taylor: Do. I think regulation is going to solve it? I don't

1051
01:45:00.330 --> 01:45:04.119
Anthony Taylor: do. I think we need to make sure that we don't give.

1052
01:45:04.760 --> 01:45:15.610
Anthony Taylor: you know. Make it as easy as it is right now to get. you know, information that's usually hard to find with just one question. I guess

1053
01:45:16.060 --> 01:45:23.120
Anthony Taylor: right? It's just the problem, is it not. Gonna bring up this. I'm

1054
01:45:23.490 --> 01:45:29.559
Anthony Taylor: there's a lot of things out there. If you want it bad enough you can get it, no matter what the law said

1055
01:45:30.230 --> 01:45:39.019
Anthony Taylor: ll ends same thing. stable diffusion, making images that are inappropriate. Same thing. deep fakes. Shh!

1056
01:45:39.150 --> 01:45:43.180
Anthony Taylor: You could download the the tools to make deep fakes, you know, 2 years ago.

1057
01:45:43.740 --> 01:45:48.170
Anthony Taylor: on your laptop. you can put anybody's face in any

1058
01:45:49.170 --> 01:45:51.010
Anthony Taylor: video you can come up.

1059
01:45:53.550 --> 01:46:01.900
Anthony Taylor: You could change their voice. use their voice and their face. Put them in Star Wars.

1060
01:46:03.920 --> 01:46:05.990
Anthony Taylor: But we want Kenobi. We need your help.

1061
01:46:07.610 --> 01:46:09.950
Anthony Taylor: whatever it is. I know

1062
01:46:10.160 --> 01:46:17.160
Anthony Taylor: some of you know that quote, anyway. So the point is, is, these tools have been around forever. We have to educate

1063
01:46:17.930 --> 01:46:18.790
Anthony Taylor: right?

1064
01:46:19.040 --> 01:46:24.959
Anthony Taylor: We have to educate. We as an industry, have to do what we can but realize.

1065
01:46:25.930 --> 01:46:29.820
Anthony Taylor: I don't know, you know. Well, whose job is it to make sure people don't act like

1066
01:46:30.640 --> 01:46:32.850
Anthony Taylor: pigs or monsters?

1067
01:46:34.170 --> 01:46:38.820
Anthony Taylor: It's not our job. but it's all our job right?

1068
01:46:39.700 --> 01:46:45.969
Anthony Taylor: So I don't. I don't know. I don't have a good answer for you guys. II am. I am probably not the best for all companies.

1069
01:46:46.150 --> 01:46:49.069
Anthony Taylor: So anyway. I work for the money.

1070
01:46:49.470 --> 01:46:51.900
Anthony Taylor: All right. So

1071
01:46:53.960 --> 01:46:55.440
Anthony Taylor: we're done with ethics.

1072
01:46:55.880 --> 01:46:58.010
Anthony Taylor: Okay?

1073
01:46:59.360 --> 01:47:06.509
Anthony Taylor: what I like. I said, most important thing to take away from this week. There's a lot of rules. There's a lot of regulations.

1074
01:47:06.620 --> 01:47:09.760
Anthony Taylor: There's a lot of stuff going on. Know your industry.

1075
01:47:09.810 --> 01:47:12.420
Anthony Taylor: Know you're, you know. Be

1076
01:47:13.210 --> 01:47:18.879
Anthony Taylor: educate yourself on the stuff that industry and your location requires

1077
01:47:18.970 --> 01:47:31.190
Anthony Taylor: at least enough so that you can identify that, hey? You might have risk. You do not need to be the expert unless that is what Job wants. Okay, so identify the risks.

1078
01:47:31.300 --> 01:47:35.160
Anthony Taylor:  also.

1079
01:47:36.070 --> 01:47:50.459
Anthony Taylor: just, you know. I mean, we'll do what we can to get these to keep these tools explainable. But the the funny part is is, we really want to work. and for them to work the way they're working, we may not be able to explain every answer. They come up

1080
01:47:51.260 --> 01:47:57.370
Anthony Taylor: because that's kind of assuming that somebody somewhere could have found that answer without.

1081
01:47:59.580 --> 01:48:04.320
Anthony Taylor: and maybe they could've. But we can't prove that because we found the answer.

1082
01:48:05.720 --> 01:48:06.500
Anthony Taylor: So

1083
01:48:06.620 --> 01:48:17.450
Anthony Taylor: I would rather have new medicines and disease cures and new chemical combinations be created. you know, at the risk of somebody, you know might be able to

1084
01:48:18.570 --> 01:48:22.769
Anthony Taylor: write a you know, create a bomb or something

1085
01:48:23.110 --> 01:48:24.130
Anthony Taylor: easier.

1086
01:48:25.430 --> 01:48:29.700
Anthony Taylor: I don't know, maybe not not sure. It's not my place.

1087
01:48:29.760 --> 01:48:36.610
Anthony Taylor: My place is too informed. and to educate. not just in this class, but at work, too.

1088
01:48:37.630 --> 01:48:39.710
Anthony Taylor: So anyway.

1089
01:48:40.930 --> 01:48:42.650
Anthony Taylor: data ethics is over.

1090
01:48:43.100 --> 01:48:47.200
Anthony Taylor:  next class is Project 2.

1091
01:48:50.600 --> 01:48:56.800
Anthony Taylor: So when you come to class, start class, this is what you're going to do. I will give you this much. Now.

1092
01:48:56.910 --> 01:49:01.229
Anthony Taylor: project 2 is a whole lot like Project One, except you're going to have an Ml component.

1093
01:49:02.980 --> 01:49:11.989
Anthony Taylor: Alright, you will be required to have an Ml component some kind of clustering or predictive analytics or classification something.

1094
01:49:13.740 --> 01:49:14.700
Anthony Taylor: Okay.

1095
01:49:15.840 --> 01:49:20.899
Anthony Taylor: that's the main change. It does not have to be on the same topic.

1096
01:49:21.290 --> 01:49:24.070
Anthony Taylor:  yeah.

1097
01:49:25.080 --> 01:49:30.440
Anthony Taylor: alright, you can do a whole new topic. If you want, you can continue with what you had before.

1098
01:49:30.740 --> 01:49:35.980
Anthony Taylor: Whatever you want to do. you'll get more details on Monday.

1099
01:49:37.010 --> 01:49:41.410
Anthony Taylor: No, no class on Monday on Tuesday.

1100
01:49:42.230 --> 01:49:45.190
Anthony Taylor: No, on Wednesday.

1101
01:49:46.200 --> 01:49:51.150
Anthony Taylor: Oh, I'm sorry, guys, I get lost.

1102
01:49:51.300 --> 01:49:53.749
Anthony Taylor: It's next week. Dog got it.

1103
01:49:55.830 --> 01:49:58.809
Anthony Taylor: Okay? So that's class.

1104
01:50:00.910 --> 01:50:04.109
Anthony Taylor: I will fill you guys in and all the details. And we will get started

1105
01:50:04.310 --> 01:50:10.729
Anthony Taylor: alright. Same basic schedule. 2 weeks in class, you'll have the entire class.

1106
01:50:10.990 --> 01:50:13.909
Anthony Taylor: The work is, you know, in breakouts

1107
01:50:14.500 --> 01:50:17.060
and then presentation on the last day.

1108
01:50:17.490 --> 01:50:27.049
Anthony Taylor: And then after that, when we come back, we're going to do like really cool data science stuff. We're gonna do neural networks, deep learning vision.

1109
01:50:28.310 --> 01:50:30.260
We're an N lp.

1110
01:50:31.590 --> 01:50:33.040
Anthony Taylor: very cool.

1111
01:50:33.370 --> 01:50:36.380
Anthony Taylor: really, into the AI side stuff.

1112
01:50:36.890 --> 01:50:42.880
Anthony Taylor: Okay. guys have a wonderful weekend. We will be here for 30 min.


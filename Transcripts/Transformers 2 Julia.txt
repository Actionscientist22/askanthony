WEBVTT

1
00:02:16.450 --> 00:02:18.989
Kevin Nguyen: Oh, no! James is just me. And you.

2
00:02:30.800 --> 00:02:31.700
Kevin Nguyen: James.

3
00:02:34.480 --> 00:02:35.120
James Torres: Testing

4
00:02:35.736 --> 00:02:36.243
James Torres: yeah.

5
00:02:40.150 --> 00:02:41.689
Kevin Nguyen: Don't tell me it's just me and you.

6
00:02:43.850 --> 00:02:45.130
James Torres: No, you can hear me. Sorry.

7
00:02:45.130 --> 00:02:46.109
Kevin Nguyen: Yeah, I can hear you.

8
00:02:49.660 --> 00:02:51.410
Kevin Nguyen: I gotta jump.

9
00:02:51.670 --> 00:02:52.520
Kevin Nguyen: Huh!

10
00:02:52.520 --> 00:02:53.300
James Torres: Yeah, job.

11
00:02:53.900 --> 00:02:54.799
Kevin Nguyen: You got a job?

12
00:02:55.580 --> 00:02:56.490
Kevin Nguyen: Oh, nice.

13
00:02:56.880 --> 00:02:58.813
James Torres: Yeah, yeah. Got the offer?

14
00:03:00.710 --> 00:03:03.270
James Torres: yeah. I think it was this morning.

15
00:03:04.097 --> 00:03:06.460
Kevin Nguyen: Which before the teaching role, or before.

16
00:03:07.072 --> 00:03:12.139
James Torres: Well, that also. But I also got another job with the Zeppree foods.

17
00:03:12.970 --> 00:03:13.445
Kevin Nguyen: Okay.

18
00:03:13.920 --> 00:03:16.474
James Torres: So I'll be doing their power. Bi and excel work.

19
00:03:17.300 --> 00:03:19.560
Kevin Nguyen: Nice dude. That's a young.

20
00:03:19.920 --> 00:03:20.520
James Torres: What's up?

21
00:03:20.700 --> 00:03:22.839
Kevin Nguyen: You're on a roll. Are you gonna teach today's class?

22
00:03:23.845 --> 00:03:25.670
James Torres: They didn't ask me to.

23
00:03:26.630 --> 00:03:27.870
Kevin Nguyen: Well, you you should.

24
00:03:27.880 --> 00:03:28.919
Kevin Nguyen: she does.

25
00:03:29.360 --> 00:03:30.880
Kevin Nguyen: If nothing's gonna come.

26
00:03:33.240 --> 00:03:34.680
Kevin Nguyen: Is anybody gonna come.

27
00:03:37.460 --> 00:03:38.900
James Torres: London State.

28
00:03:43.784 --> 00:03:47.205
Julia - SUB INSTRUCTOR: And I've been teaching

29
00:03:48.120 --> 00:03:51.250
Julia - SUB INSTRUCTOR: through various programs since

30
00:03:51.390 --> 00:04:01.500
Julia - SUB INSTRUCTOR: 2019. So it's been a while. But I love teaching, and I love the content, especially this particular boot camp.

31
00:04:01.885 --> 00:04:15.760
Julia - SUB INSTRUCTOR: There's just so many interesting things. So I'm so excited to be here to you know. Kind of carry on while your instructors out and to give you a little bit of background, I actually started my career as a programmer.

32
00:04:16.543 --> 00:04:23.179
Julia - SUB INSTRUCTOR: I have a background in Java and database management and data analytics.

33
00:04:23.602 --> 00:04:30.657
Julia - SUB INSTRUCTOR: Moved into more of an operational role because my person, I have a pretty extra, you know, outgoing

34
00:04:31.070 --> 00:04:52.599
Julia - SUB INSTRUCTOR: extrovert type of personality, and so found being on the technical side, you know, a little bit challenging from a a personality and cultural perspective. So I currently head of revenue operations at a startup here in New York and we focus on AI technologies, both

35
00:04:53.023 --> 00:05:12.939
Julia - SUB INSTRUCTOR: within our company itself. And then also you know, I incorporate AI within many different aspects of our tech stack processes and whatnot from an operational perspective, and I am an entrepreneur at heart. I've done a ton with startups, and so

36
00:05:13.303 --> 00:05:31.476
Julia - SUB INSTRUCTOR: advise a number of up and coming. Fintech actually here in New York City and you know, in my off time I love doing the New York Times crossword puzzle. I have been on this 1,000 plus days. Streak I think it's 1037. So I'm

37
00:05:31.940 --> 00:05:42.969
Julia - SUB INSTRUCTOR: I I'm trying to get to 2,000. I don't know if I'll make it but I love doing that. I love puzzles, and I love going to the museums. And you know, here in New York there's a ton of really great

38
00:05:42.990 --> 00:06:04.100
Julia - SUB INSTRUCTOR: museums. Of all types, modern art, antiquities and things like that. So yeah, a little bit about me. But I look forward to getting to know all of you today, and I'm gonna share my screen so we can kind of kick off with some slides. I'll share my entire desktop. So I'll have to like constantly toggle.

39
00:06:04.130 --> 00:06:08.189
Julia - SUB INSTRUCTOR: But today is really all about pre-chained

40
00:06:08.200 --> 00:06:32.920
Julia - SUB INSTRUCTOR: transformer models. You got in, you know, taste of transformers in the last class. But today we're, you know, digging much deeper into this, and we'll learn about the transformer architecture. How to use these pre trained models for specific Nlp, natural language processing tasks like language, translation, text generation and other use cases. So this is actually gonna be a really fun class.

41
00:06:33.210 --> 00:06:44.491
Julia - SUB INSTRUCTOR: And again, you know, so relevant because, chat, Gbt uses transformers. It was actually built on a number of transformers that will kind of get exposed to.

42
00:06:44.880 --> 00:07:06.539
Julia - SUB INSTRUCTOR: You know, transformers are really just neural networks that learn context. So the relationship or connections between separate words are still captured. And that's really important, because language is so complex, and that that context is such an important part of being able to interpret and generate text and summarize text. And, you know, kind of understand what's happening.

43
00:07:06.540 --> 00:07:21.980
Julia - SUB INSTRUCTOR: So you know, that's really where the the magic and the power of transformer models are. And and it's all relatively recent. And there's a couple of resources that I'll share as we kind of move through the course tonight.

44
00:07:22.686 --> 00:07:26.669
Julia - SUB INSTRUCTOR: So what we're gonna do is actually take a look at a sentence

45
00:07:26.700 --> 00:07:37.779
Julia - SUB INSTRUCTOR: to really understand the context, the significance of context. And I just kind of threw it in another. Pdf, so we could actually look at it together.

46
00:07:38.890 --> 00:07:42.599
Julia - SUB INSTRUCTOR: And maybe because there's a lot of

47
00:07:42.740 --> 00:07:52.150
Julia - SUB INSTRUCTOR: information that we're going to talk about tonight, and I always feel bad that I end up speaking so much. I'm going to have someone read us this lovely sentence.

48
00:07:52.482 --> 00:08:02.480
Julia - SUB INSTRUCTOR: Let's see who's first, I guess, Christine, since you were the last person who said something. You're next on my zoom real. So do you mind reading us this lovely sentence?

49
00:08:02.900 --> 00:08:12.169
Kanouff, Christine: The river run past the bustling market square, the nosey, noisy schoolyard, and the acres and acres of farmland, before finding its way into the sea.

50
00:08:12.870 --> 00:08:20.380
Julia - SUB INSTRUCTOR: Okay, so from your perspective, what do you think the sentence is trying to point out or convey.

51
00:08:27.640 --> 00:08:31.829
Kanouff, Christine: The. There was quite a bit of distance that the river

52
00:08:31.930 --> 00:08:35.279
Kanouff, Christine: was passing between where it started, and

53
00:08:35.460 --> 00:08:38.980
Kanouff, Christine: when it found its way into the next body of water.

54
00:08:39.190 --> 00:08:59.020
Julia - SUB INSTRUCTOR: Yeah, exactly. I mean, the really core components of the sentence is the fact that you have this river right? And there's some originating starting point, as Christine is saying, and that it it went through a lot of stuff to get to the C, which is sort of the endpoint. And you know, there's a lot of descriptive text

55
00:08:59.355 --> 00:09:15.939
Julia - SUB INSTRUCTOR: that describes that journey. But you know, the real point is sort of the beginning of the sentence, and the end of the sentence, and there's all the stuff in the middle. So if we're analyzing this particular sentence by parsing it text like from a text, perspective, word by word.

56
00:09:15.960 --> 00:09:21.840
Julia - SUB INSTRUCTOR: and we're kind of breaking it up or tokenizing it, you know, in a sequential process.

57
00:09:21.840 --> 00:09:49.120
Julia - SUB INSTRUCTOR: you know it. You may miss the the point of this sentence, right? And that. And that's really why relationships between or connections between words that are far apart are so important. Right? Because you would then miss the point of the sentence, which is really exactly. You know what Christine had captured, which is, you know you have a Ri river, and is trying to make its journey to an endpoint. And the endpoint is to see. And there's all the stuff that happens. But that's not the critical piece.

58
00:09:49.150 --> 00:09:50.160
Julia - SUB INSTRUCTOR: So

59
00:09:50.590 --> 00:09:52.580
Julia - SUB INSTRUCTOR: to go back to the slides.

60
00:09:53.840 --> 00:09:55.333
Julia - SUB INSTRUCTOR: we have.

61
00:09:56.610 --> 00:09:58.210
Julia - SUB INSTRUCTOR: this concept

62
00:09:58.460 --> 00:10:17.189
Julia - SUB INSTRUCTOR: of transformer architecture that was actually first introduced. In a a paper. Actually, I have a link to the paper which I can slack out. But it was it pulled it up here so we could actually take a look at it. It's actually this paper, and I'll throw this in slack.

63
00:10:17.880 --> 00:10:22.560
Julia - SUB INSTRUCTOR: What channel do you tend to use? The general channel? The local channel.

64
00:10:22.560 --> 00:10:23.550
Clayton Graves: Use, live.

65
00:10:23.550 --> 00:10:24.789
Julia - SUB INSTRUCTOR: Okay. Live. Alright. So.

66
00:10:24.790 --> 00:10:44.280
Clayton Graves: And and I wanted to point something out real quick that the last slide showed English going in and French go coming out. So my question is this, we talked yesterday about how the encoding and the decoding works and and I likened it at the time to one of those old school oval team decoder rings.

67
00:10:44.420 --> 00:10:46.969
Clayton Graves: And so I'm wondering if

68
00:10:47.500 --> 00:10:52.369
Clayton Graves: that is what makes translation so easy for AI, because

69
00:10:52.510 --> 00:10:57.839
Clayton Graves: it's literally doing the same thing regardless. It's just the output of the code is different.

70
00:10:59.200 --> 00:11:05.906
Julia - SUB INSTRUCTOR: Yes, yes, and we'll actually kinda de, you know, dig a little bit deeper into how that encoding happens

71
00:11:06.350 --> 00:11:23.770
Julia - SUB INSTRUCTOR: and how that decoding happens today. So we'll actually take a look at a few code examples. But this particular article that was published by some, you know, computer scientists through Cornell, very recently it was only 2017 that this was actually published.

72
00:11:24.185 --> 00:11:46.874
Julia - SUB INSTRUCTOR: And it's called Attention is all you need. But it really, introduced this concept of this transformer model or transformer architecture, which is what is really driving a lot of what we're talking about today. And a lot of the innovation that we're seeing within these large language models. So if I just kind of go back to the slides,

73
00:11:48.570 --> 00:12:18.519
Julia - SUB INSTRUCTOR: we can kind of dig deeper into the the diagram and then again dig deeper into the encoder piece, as as I think it was. Was it Clayton who was asking about? Yeah. So transformers are really a 2 component system. And I, you know some of this you may have, you know, reviewed last class, but you know, one component is that encoder piece here? And then the second component is that decoder piece and maybe I can have someone read this slide for me. Let's see

74
00:12:18.945 --> 00:12:26.204
Julia - SUB INSTRUCTOR: Rodney, you're next on my zoom rail. Do you wanna take a stab at just kind of reading us. The high level

75
00:12:27.590 --> 00:12:29.330
Julia - SUB INSTRUCTOR: had been bullets.

76
00:12:29.620 --> 00:12:30.420
Masarirambi, Rodney: No problem

77
00:12:31.250 --> 00:12:32.750
Masarirambi, Rodney: transformer architecture

78
00:12:33.060 --> 00:12:37.999
Masarirambi, Rodney: at a high level. The transformer architecture consists of an encoder decoder.

79
00:12:38.280 --> 00:12:41.879
Masarirambi, Rodney: The encoder will process will process an input sentence.

80
00:12:42.530 --> 00:12:44.040
Masarirambi, Rodney: a sentence in English.

81
00:12:44.260 --> 00:12:49.150
Masarirambi, Rodney: The encoded sentence will then pass through the Dakota, which produces the output.

82
00:12:49.490 --> 00:12:53.160
Masarirambi, Rodney: The same sequence is then translated into French.

83
00:12:54.040 --> 00:13:01.260
Julia - SUB INSTRUCTOR: Absolutely. Yeah. And so what you should essentially think of a transformer is is having one and input

84
00:13:01.830 --> 00:13:07.458
Julia - SUB INSTRUCTOR: that is encoded right and then decoded with results in an output. So it's really just this kind of like,

85
00:13:08.010 --> 00:13:11.749
Julia - SUB INSTRUCTOR: you know, pass through system. And now.

86
00:13:11.750 --> 00:13:13.419
Clayton Graves: Into it or not. But

87
00:13:13.911 --> 00:13:21.859
Clayton Graves: one of the things that that can get lost with a direct translation is nuance, nuance, and English is different from nuance, and French

88
00:13:21.960 --> 00:13:25.779
Clayton Graves: does. Does the does the model take that into account?

89
00:13:26.140 --> 00:13:29.170
Julia - SUB INSTRUCTOR: Yes. Well, transformal models do.

90
00:13:29.310 --> 00:13:37.910
Julia - SUB INSTRUCTOR: And the reason being is that context piece that we were talking about earlier because traditionally, and you'll see that these

91
00:13:38.730 --> 00:14:01.709
Julia - SUB INSTRUCTOR: language translator apps have actually improved significantly in the last couple of years. And it's really because it's leveraging these technologies like transformer models and other enhancements to how you process language. And it's really exactly what you're kind of pointing out is not just about saying, oh, the word run is this in French or

92
00:14:01.710 --> 00:14:15.990
Julia - SUB INSTRUCTOR: this in German. It's also the context when you're kind of thinking about it from a holistic perspective of what's being implied in that sentence. And not only that, but we'll also look at an example tonight of how you might have multiple paragraphs

93
00:14:15.990 --> 00:14:39.359
Julia - SUB INSTRUCTOR: of text right? And there might be some context in that first paragraph, you know a lot of these newer models that use this transformer architecture can actually go back and reference and understand the context from previous paragraph. So it's not just within the context of a particular sentence. It's actually within the context of maybe even entire article. And that's really what's making things so much more powerful.

94
00:14:39.360 --> 00:14:50.385
Julia - SUB INSTRUCTOR: and making it seem more human. Right? Because I think you know a lot of times when you might have experienced AI before. It's it's very clearly, you know,

95
00:14:50.730 --> 00:15:04.850
Julia - SUB INSTRUCTOR: almost like robotic right in in way that it it puts responses together. But you'll notice that as these last language models develop, they're becoming better at better about understanding context and that nuance that you were referencing.

96
00:15:06.542 --> 00:15:30.770
Julia - SUB INSTRUCTOR: Okay, so in this slide, what we're focusing on is that encoder part. So, as I was saying, we're gonna dig deeper into these different pieces, you know. It's it's actually made of 2 sections. So if you look at that original encoder box right? Which is just this box here, and then actually kind of zoom in, it's actually made out of 2 layers. One is called the self Attention Layer. And you see this great box here.

97
00:15:30.770 --> 00:15:40.460
Julia - SUB INSTRUCTOR: and that represents that, and we'll explain what that means. And then the second layer is this layer up here, which is a feed forward layer, and again we'll talk a little bit more about that.

98
00:15:40.460 --> 00:16:05.269
Julia - SUB INSTRUCTOR: But in order for our model to be able to process the input which is usually text right? Like that sentence that we were just looking at. We actually have to embed our in inputs which are down here. So you can see, here's an example sentence. It says, Hello, how are you today? Right? And that's text. And that's our input but we do have to embed our inputs into what are called word vectors. And that's what these

99
00:16:05.713 --> 00:16:34.560
Julia - SUB INSTRUCTOR: essentially arrays or lists. Look, you know that these little boxes here represent. And this process of translating our work, you know, our sentence into these word factors is called vectorization. And basically, what it means is, it's just mapping words to numbers. And you know you, you can kind of do it at many different levels. If you have a very complex paragraph, it could be by sentence. So you're kind of breaking things up into smaller chunks, but depending on like

100
00:16:34.720 --> 00:16:55.420
Julia - SUB INSTRUCTOR: what you're trying to accomplish. It may be. If you have a large paragraph, it might be by sentence. If it's a sentence, it might be by word, you know, in some cases there could be applications where you actually take words and break them up into individual letters. But the idea is that you know you're mapping, what is text to something that's more numeric based.

101
00:16:55.450 --> 00:17:10.809
Julia - SUB INSTRUCTOR: And so in this example, right? It says, You know, Hey, you know, Hello, how are you today. And it vectorizes it. So essentially, it takes this line, it this text here puts it in a word, vector and then passes that into the self attention layer.

102
00:17:11.020 --> 00:17:18.389
Julia - SUB INSTRUCTOR: And when we pass those vector embeddings into the self attention layer, it actually captures those

103
00:17:18.725 --> 00:17:41.560
Julia - SUB INSTRUCTOR: connections or relationships between the inputs. I mean this. The sentence isn't the best example. But you know, when we were reading that earlier sentence about the river and the sea, you know, it would be able to capture the fact that there was some relationship between, you know the river, the sea, and probably some other of the the descriptive language that's within that sentence.

104
00:17:41.760 --> 00:18:06.919
Julia - SUB INSTRUCTOR: And so then, once it kind of captures what that relationship is. It actually then passes the import, the inputs further into this encoder layer and feeds it into that second sub layer which is that feed forward layer which takes, you know again the output. So this is this layer. Here's the output of the self attention layer. And it takes that as its input

105
00:18:07.140 --> 00:18:28.359
Julia - SUB INSTRUCTOR: and processes them. And so the idea is that this input here isn't the exact same as what you started out with over here, because this essentially has contextualized embedding. So there's another layer of embedding that happens because you know as you go deeper and deeper into this encoder layer, you know it is kind of getting processed more and more.

106
00:18:29.016 --> 00:18:36.760
Julia - SUB INSTRUCTOR: Is there anything that you kind of notice about this diagram or something that you're like. Huh! That's weird. I wouldn't have thought that was the case.

107
00:18:37.370 --> 00:18:47.450
Julia - SUB INSTRUCTOR: Anything kind of stand out to you, or you notice about you know how the sentence is kind of moving through this encoder.

108
00:18:48.850 --> 00:18:50.179
Julia - SUB INSTRUCTOR: Meredith.

109
00:18:50.300 --> 00:18:51.140
Julia - SUB INSTRUCTOR: thoughts.

110
00:18:52.730 --> 00:18:57.009
Meredith McCanse (she/her): I guess I wouldn't. I'm I didn't expect to, or

111
00:18:57.996 --> 00:19:04.009
Meredith McCanse (she/her): the part that we're the last part. The feed forward is a neural network. That I guess that

112
00:19:05.300 --> 00:19:08.890
Meredith McCanse (she/her): it surprises or doesn't surprise me because I'm not super knowledgeable, but like

113
00:19:09.060 --> 00:19:13.499
Meredith McCanse (she/her): that, there's a neural network buried in just the encoding part of it

114
00:19:13.630 --> 00:19:14.610
Meredith McCanse (she/her): feels

115
00:19:17.060 --> 00:19:20.460
Meredith McCanse (she/her): yeah, complex or something.

116
00:19:20.460 --> 00:19:27.070
Julia - SUB INSTRUCTOR: Yeah, no, that's exactly right. Because not only that. And you're absolutely right in pointing this out. There are.

117
00:19:27.070 --> 00:19:52.040
Julia - SUB INSTRUCTOR: you know, these additional neural networks additional models within this larger model, right? Because there's stuff that's happening here. That's actually a model. And then there's stuff that's happening in here that are models. Right? And so you're, you're absolutely right. And saying, Hey, this is this looks quite complex. And not only that, but if you actually notice, there's multiple neural network models in this feed forward layer. So it's not one single

118
00:19:52.040 --> 00:20:11.819
Julia - SUB INSTRUCTOR: feed forward neural network that is used across the entirety of all the information that's being passed in. But you almost have individual models that are applied to each, you know, sort of input and output that's being generated as you move through this encoder. Sonya.

119
00:20:12.940 --> 00:20:16.499
Baro, Sonja: So is it truly linear like that, or

120
00:20:16.800 --> 00:20:22.480
Baro, Sonja: like, Hello, is gonna go down the first path into those models there, or is it

121
00:20:22.490 --> 00:20:24.989
Baro, Sonja: branching out because.

122
00:20:25.280 --> 00:20:26.060
Baro, Sonja: hop

123
00:20:26.260 --> 00:20:29.210
Baro, Sonja: I wouldn't. I don't know if it would

124
00:20:29.390 --> 00:20:36.770
Baro, Sonja: serve the purpose like, how would it know which neural network is best fit to that particular input.

125
00:20:37.330 --> 00:21:03.679
Julia - SUB INSTRUCTOR: Yes. So you're you're right. It isn't. It's exactly linear. But there is linearity to it. And so that's why it's actually specifically kind of diagrammed out in this way, because the part that is a little bit misleading, and you know and you're kind of right to say, hey it! I I can't believe it just literally kind of passes through in this very straightforward fashion, when when Hello comes into the self attention layer here

126
00:21:03.680 --> 00:21:31.617
Julia - SUB INSTRUCTOR: there is a little bit of munging that happens right? And so the output of the self attention layer that becomes the input for this feed forward layer is again is is conceptualized embedding. So what that means is that there's some additional information that's been added. So the hello that might be in here has some additional context like, oh, you know what? Hello might have to have this type of relationship with you, or you know, there, there's a little bit of

127
00:21:32.290 --> 00:21:42.379
Julia - SUB INSTRUCTOR: additional information. So there is some linearity. But it's also captured that sort of like branching that you're kind of describing. Does that make sense?

128
00:21:43.560 --> 00:22:05.660
Julia - SUB INSTRUCTOR: Yeah, okay. But the reason why there is that aspect of linearity is because this is actually a feature of transformers. So this idea of you know, not exact linearity. But you know this idea of things. Kind of flowing in lanes. Is what is actually called parallelism, and parallelism is what allows for faster trainings.

129
00:22:06.003 --> 00:22:26.276
Julia - SUB INSTRUCTOR: It supports. So a lot of times, your models are highly dependent on how much training data you're able to kind of push through. And so the volume of that is very important. It impacts the quality of your output especially for long, large language models. And so you know, this this type of parallelism. Parallelism.

130
00:22:26.991 --> 00:22:55.589
Julia - SUB INSTRUCTOR: allows you to support a higher volume of training data and actually pre oftentimes produces a higher level of predictive accuracy. So there's a lot that kind of it adds to. And again, there's still one more module after tonight there's a a 21.3 where again you're gonna kind of go even deeper into transformer models. But the takeaway here is really that there's parallelism or parallel kind of processing that's happening.

131
00:22:55.590 --> 00:23:09.320
Julia - SUB INSTRUCTOR: And but there's also this idea that there is some context or additional information that it kind of picks up as it moves through this encoder. That allows it to have a little bit more complexity than just you know. Hello, literally kind of moving.

132
00:23:09.320 --> 00:23:12.810
Julia - SUB INSTRUCTOR: you know, through through this process. Okay?

133
00:23:12.870 --> 00:23:31.439
Julia - SUB INSTRUCTOR: So I I wanna talk a little bit more about the self attention. Layer. And we have this nice little slide. That kind of just gives us a high level overview, and we'll kind of dig a little bit deeper. But who's next on my zoom real Matt, do you wanna go ahead and read this slide?

134
00:23:31.500 --> 00:23:33.927
Julia - SUB INSTRUCTOR: And if there's more than one, Matt, I'm so sorry.

135
00:23:34.600 --> 00:23:35.439
Dipinto, Matt: It's just me.

136
00:23:35.580 --> 00:23:36.680
Julia - SUB INSTRUCTOR: Okay. Awesome.

137
00:23:36.890 --> 00:23:45.380
Dipinto, Matt: Self attention, self attention allows the model to compare each word in the sequence to other words, in the sequence, to determine any useful information for encoding the word.

138
00:23:45.430 --> 00:23:49.959
Dipinto, Matt: the example being the flower that stood in the sun, wilted because it was too bright

139
00:23:50.260 --> 00:24:00.109
Dipinto, Matt: in English. The word it refers to the sun, and not the flowers. When the model is encoding the word it the self attention. Mechanism allows the model to encode useful information.

140
00:24:00.200 --> 00:24:04.239
Dipinto, Matt: such as how strongly the word. It is related to sun and flower.

141
00:24:04.310 --> 00:24:11.350
Dipinto, Matt: This is as close to understanding what it means in the sentence as computational as a computational model. We'll get.

142
00:24:11.900 --> 00:24:21.339
Julia - SUB INSTRUCTOR: Yeah. So that's kind of like the best description of that branching. You're kind of talking about, Sonia, because you, you can see that we are kind of processing it.

143
00:24:21.701 --> 00:24:49.859
Julia - SUB INSTRUCTOR: You know, for word by word. But we're also kind of capturing these weird relationships and references between the different words. Right? Because too bright, you know, even has a relationship with son. Right? And so, like each of these words in the sentence has some kind of relationship or connection to other words in the sentence that give it meaning right? So if we look at the verb stood by itself, we don't necessarily know

144
00:24:49.940 --> 00:25:07.175
Julia - SUB INSTRUCTOR: what stood right. Is it the sun? Is it the flower? But when we kind of like, look out further, we can see. Oh, the flower that stood right? So there's some kind of relationship between these 2 words. And and so the self attention layer essentially captures those different

145
00:25:08.586 --> 00:25:29.579
Julia - SUB INSTRUCTOR: relationships. It it basically embeds that information about context for each word in the text. So it literally will do it for all of these words. And and the way that it actually kind of works behind the scenes is, it's kind of giving it a score. So you know, we know how like, with all you know, neural networks and and just machine learning and general

146
00:25:29.580 --> 00:25:42.390
Julia - SUB INSTRUCTOR: loves numbers. Everything kinda has to be in a in in a number format for it to work optimally. That's why, you know, we we usually have to transform our categorical data. And so same thing it. You know, it captures this

147
00:25:42.779 --> 00:26:06.919
Julia - SUB INSTRUCTOR: information about context by scoring the the strength. Or you know, maybe the the weakness of relationships between different words. And that's kind of how it carries that information through to the next layer, which is that feed forward layer. So you know, the scoring between sun and flower will have a certain amount, whereas, you know sun and it will be

148
00:26:06.920 --> 00:26:21.199
Julia - SUB INSTRUCTOR: higher, or you know so or like sun, and though might be lower. So there's there's different ways to kind of use, numbers or scoring to kind of indicate what these relationships are between the different numbers question. Sonia.

149
00:26:21.390 --> 00:26:28.599
Baro, Sonja: I was more about just connecting to previous lessons. I think we, in natural language processing

150
00:26:28.660 --> 00:26:35.759
Baro, Sonja: last week, talked about that waiting and how it almost diagrams the sentence, If you will.

151
00:26:36.592 --> 00:26:42.220
Baro, Sonja: And I was thinking, is that what is allowing this self attention to happen.

152
00:26:43.080 --> 00:26:55.450
Julia - SUB INSTRUCTOR: Yeah, yeah, it's it's this. The concept is is very, very similar. And we're not gonna get into like how the scoring actually happens today. But you now understand that that

153
00:26:55.450 --> 00:27:25.379
Julia - SUB INSTRUCTOR: because a lot of times people are like, Wait, how does it carry that context like, how does it carry that information forward and it it's it's that waiting or that scoring, and this concept of waiting and scoring. Ha! You'll see it many, many times over in machine language, right? Because it's just kind of a really great way to either demonstrate strength. Or, you know, sort of like weakness, right? It's kind of like when you think about correlation, like, how strongly is something correlated, or how weakly is it correlated? Is that same idea? But it's using

154
00:27:25.380 --> 00:27:26.579
Julia - SUB INSTRUCTOR: a score. Yeah.

155
00:27:26.590 --> 00:27:28.060
Julia - SUB INSTRUCTOR: or wait. Yeah.

156
00:27:28.731 --> 00:27:49.110
Julia - SUB INSTRUCTOR: and then, what did I? Wanna. Oh, right? And so these word. Vector, so now, you can kinda see essentially all of the words that in or in the sentence, right are gonna move through that self attention layer and be passed on to the fast feed forward layer, or they call fns.

157
00:27:49.487 --> 00:28:03.059
Julia - SUB INSTRUCTOR: But th, they're they're gonna now have this additional information around scoring that allows you to understand the context of what's happening in the sentence. And so that I'm gonna go back to this diagram.

158
00:28:03.720 --> 00:28:23.630
Julia - SUB INSTRUCTOR: So that was why I was trying to explain that this layer here, which is the output of the self retention layer and the input into the feed forward. Layer whoops is is not quite the same as this. Right? It's not like these move exactly through. They're they're just a little bit different. But there's enough similarity that you know. There's

159
00:28:23.760 --> 00:28:30.550
Julia - SUB INSTRUCTOR: there's like, just additional context. That's added right. So it's not so vastly different that you wouldn't understand what's happening.

160
00:28:30.907 --> 00:28:38.369
Julia - SUB INSTRUCTOR: The best example I could think of like, if you remember, back to Pca. You know a lot of times when you do. Pca, you kind of lose.

161
00:28:38.670 --> 00:28:51.680
Julia - SUB INSTRUCTOR: you know, contacts and information about what's happening, because it's just kind of compress. Right? But that's not exactly the thing that happens here. You don't lose everything as it kind of moves through you. You still retain some of that information.

162
00:28:52.550 --> 00:28:56.779
Julia - SUB INSTRUCTOR: So okay. Alright. Yes. Oh, question.

163
00:28:58.080 --> 00:29:03.340
Derek Rikke: Yeah, I'm wondering how the self attention layer was like developed like did

164
00:29:04.000 --> 00:29:10.869
Derek Rikke: did like machine learning models process a bunch of text? Or do they like? Train it on like English textbooks

165
00:29:11.020 --> 00:29:13.190
Derek Rikke: to learn the context.

166
00:29:13.190 --> 00:29:40.549
Julia - SUB INSTRUCTOR: Well, we'll we'll see some examples actually, tonight. And then again, I actually highly recommend. If you have, you know, a little bit of time in the morning, or something. I would read. That article is a really great article, and it talks about a lot of these const concepts and more depth than I think, what we're gonna cover tonight. And then again, you're gonna dig even deeper in the next class. But we'll actually see some of the examples of how it kind of

167
00:29:40.880 --> 00:29:44.679
Julia - SUB INSTRUCTOR: uses different sources to train itself.

168
00:29:45.810 --> 00:29:47.050
Julia - SUB INSTRUCTOR: But good question.

169
00:29:47.140 --> 00:29:53.899
Julia - SUB INSTRUCTOR: okay? Oh, yes. So yes. We'll be using transformers to translate.

170
00:29:54.120 --> 00:30:09.790
Julia - SUB INSTRUCTOR: you know, summarize some text today. But you know, I wanted to talk about these pre trained transformers. You know, pre trained transformers are exactly what it sounds like. They they're trained for you, right? So you can focus on more of like the analysis. Or

171
00:30:10.380 --> 00:30:14.060
Julia - SUB INSTRUCTOR: you know, the predictions or the outputs that are coming out of your model.

172
00:30:14.340 --> 00:30:35.989
Julia - SUB INSTRUCTOR: And the Prejud models are really great, because one, you know, you actually might not have a ton of training data, you know, going back to your question. Derek. So you know, if you didn't have all of that data to train a model, you know, and there was a sort of a comparable model that was available through, you know. You know, like, hugging face has a ton. Google has a ton.

173
00:30:35.990 --> 00:30:49.499
Julia - SUB INSTRUCTOR: But there are many different sources for pre trained models. But you know, it's it's it's actually really great. Cause you don't have to worry about trying to figure out. Oh, my, gosh, where am I gonna get all this training data from? You can actually leverage a pre trained model?

174
00:30:49.926 --> 00:31:01.003
Julia - SUB INSTRUCTOR: And this screenshot is just kind of just all the things that for example, the hugging face transformer models can do. And

175
00:31:02.240 --> 00:31:10.190
Julia - SUB INSTRUCTOR: have you. You guys have seen the hugging face. Site already, I think, from last class. Okay, perfect. Alright awesome.

176
00:31:10.867 --> 00:31:17.070
Julia - SUB INSTRUCTOR: Okay. So we're going to be using a pre trained model. Called T, 5.

177
00:31:17.918 --> 00:31:33.449
Julia - SUB INSTRUCTOR: It's a. It's a really powerful text to text pre-trained transformer that we're gonna use for our translation. So we're gonna be actually looking at language translation, that exact scenario where we're taking English and translating it to French.

178
00:31:33.794 --> 00:31:59.965
Julia - SUB INSTRUCTOR: And really, again, you know, we kind of talked about this at the beginning, but with the improvement of these large language models, you know, AI has really evolved these language translation apps that you've seen, and they're much more reliable now. They're more reliable, the more accurate. And they can actually process more text. You know if you're kind of using in the context of maybe not like learning a language. But

179
00:32:00.770 --> 00:32:04.770
Julia - SUB INSTRUCTOR: yeah. Curious. If anyone is currently trying to learn a new language.

180
00:32:05.090 --> 00:32:19.289
Julia - SUB INSTRUCTOR: I I am actually trying to learn French very poorly. But I'm currently using the dual lingo app, and they've added a lot new of new sort of AI features in there. But is anyone else trying to learn a new language.

181
00:32:20.800 --> 00:32:23.070
Baro, Sonja: I'm always trying to learn Spanish.

182
00:32:23.899 --> 00:32:25.019
Julia - SUB INSTRUCTOR: Nice. Yeah.

183
00:32:25.020 --> 00:32:30.329
Baro, Sonja: I say trying cause it's I go for a while, and then I stop, and you know how it is. It's like.

184
00:32:30.540 --> 00:32:35.269
Baro, Sonja: you know. Once you stop, you use it or lose it right? That's so true.

185
00:32:35.680 --> 00:32:36.800
Julia - SUB INSTRUCTOR: Absolutely.

186
00:32:37.565 --> 00:32:55.054
Julia - SUB INSTRUCTOR: So this is actually the type of model used in these types of language translation transformer models. They're called sequence to sequence. So let me have someone else read a slide.

187
00:32:56.320 --> 00:32:59.260
Julia - SUB INSTRUCTOR: Rodney, you are next on my zoom reel.

188
00:32:59.600 --> 00:33:00.679
Julia - SUB INSTRUCTOR: which you.

189
00:33:03.360 --> 00:33:03.980
Masarirambi, Rodney: Good

190
00:33:04.910 --> 00:33:08.529
Masarirambi, Rodney: language, translation, sequence to sequence, lolms.

191
00:33:09.610 --> 00:33:11.750
Masarirambi, Rodney: sequence to sequence, or

192
00:33:11.860 --> 00:33:16.220
Masarirambi, Rodney: sec. To sec. Models consist of an encoder and decoder.

193
00:33:16.500 --> 00:33:23.770
Masarirambi, Rodney: the transformer model used in language. Translation. Toss is essential is sequential to sequential. Like language model.

194
00:33:24.150 --> 00:33:34.179
Masarirambi, Rodney: The encoded takes the source language sentence as input and encodes it into a fixed length vector representation often called a context vector

195
00:33:34.870 --> 00:33:42.190
Masarirambi, Rodney: 2, then the decoded takes the context vector as input and generates the target language sentence, word by word.

196
00:33:43.170 --> 00:33:56.849
Julia - SUB INSTRUCTOR: Perfect. And we're actually gonna look at some code that uses the T 5 model to translate some English to French. And so we'll actually do this exact thing so let me exit out of the slides.

197
00:33:56.890 --> 00:33:59.970
Julia - SUB INSTRUCTOR: And I'm going to be using collab.

198
00:34:01.060 --> 00:34:09.180
Julia - SUB INSTRUCTOR: have a link to. Oh, yeah, I have a link to the T 5 model here, which I'll throw in slack in case you want to take a look at it.

199
00:34:10.728 --> 00:34:12.670
Julia - SUB INSTRUCTOR: Matt, you have a question.

200
00:34:15.350 --> 00:34:24.350
Dipinto, Matt: Yeah, I the one thing that I think is a little interesting that I don't understand is, why is it a fixed length encoding. And so I think we were seeing this with

201
00:34:26.210 --> 00:34:31.219
Dipinto, Matt: whatever model we were using but it was like 384

202
00:34:31.340 --> 00:34:32.530
Dipinto, Matt: floats

203
00:34:32.960 --> 00:34:36.620
Dipinto, Matt: in every single encoding. And so

204
00:34:37.050 --> 00:34:44.489
Dipinto, Matt: I guess my question is, how does a sentence with 5 words need the same number of weights as a sentence with, You know, 45 words.

205
00:34:45.719 --> 00:35:11.009
Julia - SUB INSTRUCTOR: Yeah, it's because, depending it, it depends on the model that you're using. So what happens is a lot of time. These models have sort of a a specific shape to them. I think you've probably used that shape function in the past to look at. You know what are the dimensions of the data I'm inputting? Or what are the dimensions of? Especially in a neural network? Right? As you're kind of moving layer through layer.

206
00:35:11.450 --> 00:35:37.379
Julia - SUB INSTRUCTOR: So it kinda depends on like whoever built that model and why they felt that was the optimal sort of shape that they wanted your data to Co go through. And then also, if you think about how these models process data, right? It's very hard to kind of account for inconsistencies. Right? And so if you know that this is going to be the particular fixed length that you're expecting.

207
00:35:37.379 --> 00:35:50.209
Julia - SUB INSTRUCTOR: and there are spots that are empty that's much easier to handle from a pro programmatic standpoint than to kind of constantly be trying to figure out like what's happening in your input or, you know, like a

208
00:35:50.299 --> 00:35:54.319
Julia - SUB INSTRUCTOR: output, you know, data set. Does that? Does that make sense.

209
00:35:54.800 --> 00:35:55.990
Dipinto, Matt: Yeah, absolutely. Okay.

210
00:35:56.690 --> 00:36:03.019
Julia - SUB INSTRUCTOR: But yeah, you know a a lot of the the models out there are open source. So you can actually take a look and see

211
00:36:03.190 --> 00:36:27.010
Julia - SUB INSTRUCTOR: some of them, especially the hugging face. Models have great documentation. So you can actually see. And then also I have in the past, and that, you know I've encouraged students to. If you know who developed those models a lot of times, they will love to talk to you more about like, what was their, you know, sort of motivation behind it. They usually have published papers about it. And you know, they're they're usually really excited to interact with people who are.

212
00:36:27.010 --> 00:36:41.639
Julia - SUB INSTRUCTOR: you know, generally trying to learn about something that they develop. So you know, encourage you to. You know, if if there's a particular model that you're really curious about reach out to the authors of them model because oftentimes they're very happy to talk to you and share knowledge, too.

213
00:36:43.540 --> 00:36:48.609
Julia - SUB INSTRUCTOR: Okay, so we if you wanna follow along, I'm just gonna pull in

214
00:36:51.010 --> 00:37:10.819
Julia - SUB INSTRUCTOR: I was just looking at this earlier. I ran everything to make sure everything would run. But we're gonna look at the under the first folder. Instructor text translation, the unsolved folder. There should be a text underscore translation notebook file. So if you go ahead and open that again, I'm using Colab

215
00:37:11.211 --> 00:37:21.390
Julia - SUB INSTRUCTOR: but you could always use notebook. Jupiter notebook. And I'm gonna uncomment. This, I can go ahead and do an install of my transformer

216
00:37:21.500 --> 00:37:24.889
Julia - SUB INSTRUCTOR: module so I can access the class that I need

217
00:37:25.464 --> 00:37:41.580
Julia - SUB INSTRUCTOR: and you can see here, as soon as this finish finishes running that we're going to be using this auto tokenizer class, which is within the transformer library. And this is really gonna help us

218
00:37:41.580 --> 00:38:09.020
Julia - SUB INSTRUCTOR: prepare our data. A lot of times. And and you know, you're probably very used to this. Now, a lot of the the issues that come out of your models usually is, because there was an adequate, you know, preparation of your data. And so we're actually gonna be using this auto tokenizer class in order to prepare our inputs so that we can actually fit it into feed it into our translation model. Alright. So finally ran.

219
00:38:09.240 --> 00:38:32.340
Julia - SUB INSTRUCTOR: I'm gonna go ahead and run this section of code which is pre-coded for us. To again import our auto auto tokenizer. But then create an instance of it. Using our T 5 model that I was just referencing, which is a prechained model. And just saving that into that tokenizer variable. So we can reference that instance later. Okay, so it's done.

220
00:38:32.780 --> 00:38:36.580
Julia - SUB INSTRUCTOR: Now, what we want to do is define some text.

221
00:38:37.886 --> 00:38:46.283
Julia - SUB INSTRUCTOR: oh, you know one thing to also note. So there's actually different types of tokenizers that exist.

222
00:38:48.160 --> 00:38:51.450
Julia - SUB INSTRUCTOR: if you have a large amount of data.

223
00:38:51.540 --> 00:39:18.610
Julia - SUB INSTRUCTOR: or you wanna batch your data. There's something called a fast tokenizer. And so fast tokenizers are, you know, faster. That's why they're called fast tokenizers. And they're typically used for batch tokenization, especially if you've a large amount of data that you you need to kind of process but you know most of the time, you know, using a slow or a regular tokenizer is is good enough, but just kind of letting you know that that exists. Sonia, you have a question.

224
00:39:18.610 --> 00:39:24.699
Baro, Sonja: Yeah, I I just wanna confirm. I'm understanding. Is the auto tokenizer, then something that

225
00:39:24.710 --> 00:39:37.219
Baro, Sonja: so we spend a lot of time on tokenizing word, sentence, character, all that kind of stuff. Can we use auto tokenizers rather than go through that detail of of tokenizing.

226
00:39:37.620 --> 00:39:48.169
Julia - SUB INSTRUCTOR: I don't remember the context that you probably use token the whatever you mean like. When you did like, maybe get dummies, or some of those other.

227
00:39:48.170 --> 00:40:01.620
Baro, Sonja: No, maybe I'm misunderstanding. I'm thinking of when we took a sentence, and we had to tokenize parts of the sentence right? Because tokenizing, meaning can getting it separated and also converted into numbers.

228
00:40:01.660 --> 00:40:03.260
Baro, Sonja: Isn't that what I'm doing.

229
00:40:03.260 --> 00:40:07.844
Julia - SUB INSTRUCTOR: Yes, it is. But the thing that you wanna know is, you'll notice that there's

230
00:40:08.190 --> 00:40:35.129
Julia - SUB INSTRUCTOR: this kind of additional code that's added to this auto tokenizer. And what this is doing is it's taking all the kind of information that's been pre trained into this T 5 model, which is specifically for like, you can use it for many different things. But it does a really great job of language translation. So, for example, if you were doing some other type of

231
00:40:35.447 --> 00:40:56.099
Julia - SUB INSTRUCTOR: use case with language that doesn't have to do with language translation. And this particular model might not be the best fit for you. You may want to look at some other method of tokenization or you know, reference, a different model. So it depends. It really depends on the context of what you're trying to accomplish, I guess, is what I'm saying.

232
00:40:56.390 --> 00:40:57.200
Baro, Sonja: Okay.

233
00:40:57.610 --> 00:40:58.639
Julia - SUB INSTRUCTOR: Does that make sense.

234
00:41:00.340 --> 00:41:01.280
Baro, Sonja: Think so.

235
00:41:01.280 --> 00:41:02.000
Julia - SUB INSTRUCTOR: Okay.

236
00:41:02.370 --> 00:41:06.043
Baro, Sonja: I have to read the documentation on the Tokenizer and the model.

237
00:41:06.350 --> 00:41:06.920
Julia - SUB INSTRUCTOR: Yeah.

238
00:41:06.920 --> 00:41:07.600
Baro, Sonja: End of the day.

239
00:41:07.600 --> 00:41:13.910
Julia - SUB INSTRUCTOR: That is the correct answer. That is correct. Answer. But I

240
00:41:14.610 --> 00:41:21.649
Julia - SUB INSTRUCTOR: like. I think I think that's one of the challenges. It's one of the beauties and challenges of this particular

241
00:41:21.977 --> 00:41:45.530
Julia - SUB INSTRUCTOR: area is that there are many ways to approach a problem, and it's highly influenced by a lot of different things. Right? You know. You could have the right you know, algorithm, you could have the right data. But maybe because of the way that you've distributed your weights. It's not gonna work as well right. It's not gonna produce the outcome when you do your trainings, as you expect.

242
00:41:45.530 --> 00:41:49.572
Julia - SUB INSTRUCTOR: you know. And so it's like all these little things, right?

243
00:41:50.290 --> 00:42:19.550
Julia - SUB INSTRUCTOR: you know, depending on like, what activation functions you use and all of that. So there's so much complexity. And how these things can influence your eventual output, that there's probably no wrong answer. But there's probably a better answer. It gives us the better way to say it, so I'm not saying that you couldn't use auto tokenizer every single day, but there's probably some instances where it is more appropriate, and some where it's just not gonna serve you, I guess, is my best answer.

244
00:42:21.273 --> 00:42:31.630
Julia - SUB INSTRUCTOR: Alright. So we're gonna define some text we want to translate. So I'm gonna create a variable called. I don't know text underscore to translate, as I don't feel very creative today.

245
00:42:31.820 --> 00:42:40.770
Julia - SUB INSTRUCTOR: And I'm gonna write some sentence. Thursdays are the best day of the week.

246
00:42:41.150 --> 00:42:43.769
Julia - SUB INSTRUCTOR: Exclamation point. So here we go.

247
00:42:44.383 --> 00:43:10.039
Julia - SUB INSTRUCTOR: Alright. So let me run it. So we get that in our memory, and then, it's it's pre coded for us here. But what we're doing is we're tokenizing our text to generate input. Ids. I'm gonna actually run this. So you can actually see oh, cause they pre coded it for us, and I use it a different, variable name. So they were expecting English underscore text. But I use text to translate.

248
00:43:10.190 --> 00:43:12.650
Julia - SUB INSTRUCTOR: Let me rerun this, and it should run fine.

249
00:43:13.622 --> 00:43:29.739
Julia - SUB INSTRUCTOR: And while that's kind of running what it's doing is it's breaking up that sentence I have the sentence. Thursdays are the best day of the week, right? And it's breaking that sentence into words and converting our our text string to numbers or ids.

250
00:43:29.790 --> 00:43:46.133
Julia - SUB INSTRUCTOR: And one thing to notice is that one of the parameters for like, you'll notice here it says, translate English to French here. It's it's just including, like a prompt in that

251
00:43:46.790 --> 00:43:54.270
Julia - SUB INSTRUCTOR: part of the text. And what it's doing is it's telling our transformer to translate from English to French.

252
00:43:54.270 --> 00:44:17.680
Julia - SUB INSTRUCTOR: And we add this because you know, as I kind of mentioned, the T 5 transformer model can do more than just language translation. But people tend to use it for that. So we're we're essentially telling it what we want it to do. And that's language translation. So again, you can use. There's many different models out there. And some of them can do multiple things, some things better than other things.

253
00:44:17.680 --> 00:44:35.789
Julia - SUB INSTRUCTOR: But in this case, t, the T, 5 model can do more than just language translation. But in this case that we we wanna see it do an actual translation from English to French. So we're giving it a prompt saying, Hey, I wanna translate this text. Thursdays are the best day of the week. From English to French.

254
00:44:36.200 --> 00:44:46.480
Julia - SUB INSTRUCTOR: and just for background. If you wanted to use this for your own like project, your last project, the T, 5 model supports English, French.

255
00:44:46.910 --> 00:45:02.244
Julia - SUB INSTRUCTOR: I think German and Romanian are the 4. The other thing that you'll notice is that there is an additional parameter that we're passing in when we actually tokenize our sentence. And it's

256
00:45:02.670 --> 00:45:09.509
Julia - SUB INSTRUCTOR: this return tensors argument. And specifically, we're saying, Hey, we want

257
00:45:09.510 --> 00:45:33.970
Julia - SUB INSTRUCTOR: tensor flow. That's what the Ts stands for tensors to come back as the output and and you know that that particular flag is is pretty commonly in use. But one thing to note is what whatever output format that we use, you wanna make sure that whatever model you apply on top of this can support that output. So, for example.

258
00:45:34.280 --> 00:46:02.730
Julia - SUB INSTRUCTOR: this sequence to sequence model that we're using down here actually can support tensor tensor flow tensors. So it, it should be fine, but just kind of be aware of that. If you're working on your project, and you're like trying to use multiple different models with each other. Just make sure that you. You read the documentation. Ca, carefully to make sure that whatever the outputs are are aligned with what the expected inputs are for in any other models that you might use downstream.

259
00:46:03.130 --> 00:46:18.105
Julia - SUB INSTRUCTOR: Okay, alright. So we've got that going. And then here, what we're doing again, it's all Pre quoted for us. But we're importing our sequence to sequence. Model to create our translation model. So essentially, this

260
00:46:18.500 --> 00:46:34.669
Julia - SUB INSTRUCTOR: this reference up here was just kind of creating all that prep work. Right? So we're using our tokenizer instance to do all our prep work. And it hasn't done any translation yet. It's really this sequence to sequence model that we're gonna use in order to actually do the translation for us.

261
00:46:34.740 --> 00:46:49.830
Julia - SUB INSTRUCTOR: And so what's happening here? And I'll run this. So you can actually see what the output is. But essentially in this first line. It's taking this instance of our it's taking our class, our sequencies sequence class.

262
00:46:49.830 --> 00:47:09.660
Julia - SUB INSTRUCTOR: And then again, it's passing in the T 5 model saying, Hey, I wanna be consistent here, you know, I wanna use that to train our model in order to be able to actually translate. And so now we have an instance of our translation model here that's referenced by that variable translation underscore model.

263
00:47:09.660 --> 00:47:15.730
Julia - SUB INSTRUCTOR: And then we're using the generate function associated with that model to take the inputs

264
00:47:15.840 --> 00:47:19.589
Julia - SUB INSTRUCTOR: which are the Ids that we tokenized up here.

265
00:47:19.860 --> 00:47:29.819
Julia - SUB INSTRUCTOR: And then we're using that generate function to actually generate or create that translation. And then that translation is actually stored in this outputs.

266
00:47:31.310 --> 00:47:33.660
Julia - SUB INSTRUCTOR: this outputs variable here.

267
00:47:33.880 --> 00:47:39.906
Julia - SUB INSTRUCTOR: And so the one thing that you'll see here is that we do have

268
00:47:40.830 --> 00:47:54.130
Julia - SUB INSTRUCTOR: you know, a couple of different things. We've got our inputs here. But then we have this Max new tokens, parameter here, and that's just limiting the number of output tokens that are generated.

269
00:47:54.460 --> 00:48:22.209
Julia - SUB INSTRUCTOR: And then you can kind of see the output, you know, as it actually ran, and you'll see this reference to pi torch, you know. It's just, you know, another deep learning framework like tensor flow. So this particular model is actually, you know, kind of incorporating that you know you could probably incorporate that in your project if you're interested in using that particular library or module. But yeah, you know, people tend to use

270
00:48:22.230 --> 00:48:31.177
Julia - SUB INSTRUCTOR: tense tensorflow are more for professional settings. And then, you know, pie torch and and and maybe more research and and kind of like

271
00:48:32.210 --> 00:48:48.725
Julia - SUB INSTRUCTOR: training settings. So okay? And then, finally, we're gonna do that second part of that 2 part system that we looked at for transformers, which is decode right? We did the encoding. Now we're translating. And now we're gonna decode. So I'm gonna go ahead and run this

272
00:48:50.030 --> 00:48:57.900
Julia - SUB INSTRUCTOR: pre-coded text for us. And you can see that it's saying, Oh, hey! You know that sentence that I wrote. What did I read?

273
00:48:58.406 --> 00:49:00.860
Julia - SUB INSTRUCTOR: Thursdays are the best day of the week.

274
00:49:01.282 --> 00:49:11.719
Julia - SUB INSTRUCTOR: It's translated that into French here. Because I have not made a ton of progress in learning French. I can't verify that. That is the correct.

275
00:49:11.720 --> 00:49:12.110
Baro, Sonja: Right.

276
00:49:12.110 --> 00:49:12.570
Julia - SUB INSTRUCTOR: Ethan.

277
00:49:13.000 --> 00:49:14.180
Julia - SUB INSTRUCTOR: I think, okay.

278
00:49:14.180 --> 00:49:15.409
Meredith McCanse (she/her): It looks good. Yup!

279
00:49:15.410 --> 00:49:15.730
Baro, Sonja: Yeah.

280
00:49:15.730 --> 00:49:24.999
Julia - SUB INSTRUCTOR: Okay. So we got 2 thumbs up alright. And then again, you know, I think there was a question about you know. Hey? What's the deal with all of you know this sort of like

281
00:49:25.020 --> 00:49:45.960
Julia - SUB INSTRUCTOR: padding or extra text, you know, you can actually see that here. There's some additional text that you see in addition to the actual translation. So if we wanted to just look at the text itself, we can actually use this parameter here that says, Hey, skip any special kind of like text

282
00:49:46.230 --> 00:50:06.269
Julia - SUB INSTRUCTOR: and we set that to true. And then if we do that we get the actual translation text itself. So there's a little bit of work that you have to do there. But you know, you, you get the correct translation at the end. The other thing I do wanna point out as part of this is that you'll notice that here, you know, we didn't actually just call the outputs

283
00:50:06.310 --> 00:50:16.649
Julia - SUB INSTRUCTOR: as as that parameter that we're passing to that decode function. And the reason why is because our outputs up here.

284
00:50:17.180 --> 00:50:30.780
Julia - SUB INSTRUCTOR: And again, this is why it's so important to just really kind of pay attention to what's coming out of, you know, either your model as an output or coming out of functions that you're using. Our outputs are are coming out as a 2 dimensional array.

285
00:50:31.010 --> 00:50:50.880
Julia - SUB INSTRUCTOR: And so this translation, the the actual translation that we're looking for is in the inner array of this 2 dimensional array that comes out when you use this generate function. And so that's why we're saying, Hey, we want the outputs. But we're looking at this inner array. And that's why we're using that index 0 to to reference that.

286
00:50:51.650 --> 00:51:13.710
Julia - SUB INSTRUCTOR: Okay, alright. So to kind of recap what we did. We just started out with using the transformers library to pull in our auto tokenizer, which you know, kind of prepped our data. It, you know, broke up our sentence into different words and translated them into numbers. And then, once we did that

287
00:51:14.010 --> 00:51:19.190
Julia - SUB INSTRUCTOR: we use a different model, this sequence to sequence model

288
00:51:19.660 --> 00:51:33.939
Julia - SUB INSTRUCTOR: to then generate our translation so English to French, and then we actually decoded it and then printed it out. And voila, we actually had French.

289
00:51:35.330 --> 00:51:36.210
Julia - SUB INSTRUCTOR: okay.

290
00:51:36.330 --> 00:51:52.039
Julia - SUB INSTRUCTOR: this code that we ran to translate the text was, you know, relatively easy. It was like one sentence, right? But in a real world settings. A lot of times when you think when you hear about machine learning, you'll hear about something called pipelines

291
00:51:52.060 --> 00:52:03.479
Julia - SUB INSTRUCTOR: and pipelines. Essentially, allow us to automate a lot of our machine learning workflow. And so the second file that's in this particular folder, and I'm gonna go ahead.

292
00:52:04.140 --> 00:52:05.999
Julia - SUB INSTRUCTOR: and open it.

293
00:52:06.460 --> 00:52:08.759
Julia - SUB INSTRUCTOR: Can I have a

294
00:52:11.560 --> 00:52:23.260
Julia - SUB INSTRUCTOR: this one? It's a it's in the same folder, in the same unself. Folder is text underscore translation, underscore pipeline. If you open that up, you'll actually see the same file that I'm going to be looking at.

295
00:52:23.800 --> 00:52:25.599
Julia - SUB INSTRUCTOR: And again, I'll just do a quick

296
00:52:25.870 --> 00:52:32.350
Julia - SUB INSTRUCTOR: install of my fits formers. But essentially there's

297
00:52:33.290 --> 00:52:44.010
Julia - SUB INSTRUCTOR: a lot of time in a more in a business setting or a production setting. You'll actually use this concept of pipelines to automate a lot of the sort of stuff that happens.

298
00:52:44.050 --> 00:52:58.169
Julia - SUB INSTRUCTOR: And so we'll take a look at a sec. An example in a second. If it finishes installing soon. But you'll notice that. What I'm doing here, instead of pulling in

299
00:52:58.190 --> 00:53:08.458
Julia - SUB INSTRUCTOR: all of the we had a lot more steps in that first example, cause we're kind of like going through it looking at the encoding and the, you know translation and the decoding

300
00:53:09.067 --> 00:53:29.010
Julia - SUB INSTRUCTOR: here, because we're gonna be using a pipeline. It's a little bit more straightforward, and it's because the pipelines, you know, almost kind of hide some of the the stuff that happens before, and it's really great. If you have a lot of repetitive work. you can just kind of use, the same technique. But here we are. We've got our transformers library here.

301
00:53:29.180 --> 00:53:33.889
Julia - SUB INSTRUCTOR: We're going to import this pipeline class. So I'm going to go ahead and run that.

302
00:53:34.410 --> 00:53:46.840
Julia - SUB INSTRUCTOR: And then, once that happens, we're going to create an instance of our pipeline here, and we're going to tell it exactly what we want to do with our pipeline, because pipelines can be used, not just in the context of translating text.

303
00:53:46.840 --> 00:54:09.720
Julia - SUB INSTRUCTOR: but it can be used for any sort of again, machine learning workflow. So for this particular example, we're translating text. So we're saying, Hey, we wanna translate some text. And I wanna again use that T 5 model and then create an instance of that pipeline and save it in our translator variable here. So if I go ahead and run that

304
00:54:10.177 --> 00:54:26.640
Julia - SUB INSTRUCTOR: now, what's gonna happen is that I can actually translate a sentence and have it be fairly straightforward. And it's actually then very easy to kind of scale this. Alright. So it's done.

305
00:54:26.690 --> 00:54:48.531
Julia - SUB INSTRUCTOR: Oh, okay. So we wanna translate to German. Okay? So I'm gonna create a variable text to translate was, I think, what I use last time. So I'm gonna say, I don't know. I cannot wait for Friday, because tg, if actually, I'm not sure what it's gonna do with Tjf, we'll see what how it translates

306
00:54:50.210 --> 00:54:59.119
Julia - SUB INSTRUCTOR: And so this is the actual sentence that I want to translate and let me go ahead and run that. Oh, and then we actually have to write some code here.

307
00:54:59.498 --> 00:55:22.620
Julia - SUB INSTRUCTOR: So now that I have my text that I want to translate. I'm going to actually also include that prompt right? Because if you remember the last time we actually had to say, hey? You know, I know you can do all the stuff. T. 5 model. You're so amazing. But I actually just want you to translate from English to French. But in this case I think we're doing German. So I'm gonna say, I'll just create a new variable called text.

308
00:55:22.620 --> 00:55:32.349
Julia - SUB INSTRUCTOR: I can just use an F string to just the F string, notation, to just say, translate English to German.

309
00:55:33.109 --> 00:55:36.899
Julia - SUB INSTRUCTOR: and then we can just kind of put the text

310
00:55:37.250 --> 00:55:40.310
Julia - SUB INSTRUCTOR: to translate variable in there.

311
00:55:40.360 --> 00:55:42.325
Julia - SUB INSTRUCTOR: Translate. Okay, yes,

312
00:55:43.200 --> 00:55:57.170
Julia - SUB INSTRUCTOR: And then all I have to do here, instead of like the extra code that we did last time is just take that text and actually pass that into our translator instance. So I'm gonna and then we'll probably want to capture the output right? So the output

313
00:55:58.298 --> 00:56:00.731
Julia - SUB INSTRUCTOR: so that translator, instance

314
00:56:01.440 --> 00:56:13.770
Julia - SUB INSTRUCTOR: oh, did I spell translator wrong? Yes, I did. Okay, so here it is. This is the pipeline instance that we saved in this translator variable. So I'm gonna fix this because I spell it wrong. Translate or

315
00:56:14.216 --> 00:56:17.869
Julia - SUB INSTRUCTOR: so I'm gonna pass it just that text right?

316
00:56:17.930 --> 00:56:23.550
Julia - SUB INSTRUCTOR: And then I'll just print out output, and we'll see what it looks like. So let's go ahead and run it.

317
00:56:24.340 --> 00:56:25.909
Julia - SUB INSTRUCTOR: Cross our fingers.

318
00:56:26.402 --> 00:56:28.170
Julia - SUB INSTRUCTOR: Okay, so it did.

319
00:56:28.230 --> 00:56:39.381
Julia - SUB INSTRUCTOR: This looks German. It looks interesting, because, I guess there is no German equivalent to Tgif. But you know, if there's anyone in class who's like yes, that is correct.

320
00:56:39.730 --> 00:56:42.499
Baro, Sonja: So what you do, you say on this one.

321
00:56:43.220 --> 00:56:44.260
michael mcpherson: Yeah, that's right.

322
00:56:44.380 --> 00:56:45.310
michael mcpherson: Ashkani.

323
00:56:45.310 --> 00:56:46.629
Baro, Sonja: Good answer.

324
00:56:47.290 --> 00:56:48.245
Julia - SUB INSTRUCTOR: Alright?

325
00:56:49.240 --> 00:56:51.990
Julia - SUB INSTRUCTOR: yeah. So really, really, great.

326
00:56:52.640 --> 00:56:56.610
Julia - SUB INSTRUCTOR: does anyone notice anything about the output? That's

327
00:56:57.461 --> 00:57:00.480
Julia - SUB INSTRUCTOR: different than the last output we saw.

328
00:57:00.480 --> 00:57:03.890
Baro, Sonja: It looks like it's a dictionary or a list of a dictionary.

329
00:57:03.890 --> 00:57:07.250
Julia - SUB INSTRUCTOR: Yeah. Absolutely. Json format format. Right? Like.

330
00:57:07.250 --> 00:57:08.170
Baro, Sonja: That's time.

331
00:57:08.765 --> 00:57:28.259
Julia - SUB INSTRUCTOR: The format that you tend to see. So you know, because of that if we want it to kind of look a little nicer you know, we can always just kind of like focus on just the text by, you know, essentially kind of traversing the the Jason or the dictionary that you're seeing here. So I can take my output right.

332
00:57:28.470 --> 00:57:32.949
Julia - SUB INSTRUCTOR: which is holding my output, and say, Hey, I want to go into that first

333
00:57:33.366 --> 00:57:44.740
Julia - SUB INSTRUCTOR: element of my dictionary. And the key that I'm looking at is called translation text. So I'm gonna go ahead and say, I want just the stuff from tre translation text.

334
00:57:45.380 --> 00:57:50.880
Julia - SUB INSTRUCTOR: And if I run this, this should just give us the nice text there. So there we go.

335
00:57:50.900 --> 00:57:54.620
Julia - SUB INSTRUCTOR: So we don't get like all the extra stuff from that Json format.

336
00:57:55.329 --> 00:58:04.200
Julia - SUB INSTRUCTOR: Okay. So then says, Oh, define English text and translate it to French. Okay, great. So I'll actually just cheat a little bit

337
00:58:05.036 --> 00:58:08.093
Julia - SUB INSTRUCTOR: and just copy this up here.

338
00:58:08.880 --> 00:58:16.220
Julia - SUB INSTRUCTOR: and then I'll paste it down here. And then, instead of saying, You know, translate English to German. I'll just say, translate English to French

339
00:58:17.050 --> 00:58:25.579
Julia - SUB INSTRUCTOR: and go ahead and run this. And then we should. Oh, yeah, cause there was that was me copying, pasting. Not very well.

340
00:58:25.910 --> 00:58:29.029
Julia - SUB INSTRUCTOR: that was human error, so let it run.

341
00:58:29.150 --> 00:58:38.569
Julia - SUB INSTRUCTOR: and there we go. Also, apparently not a tgif equivalent in French, but we've got the same sentence translated into French.

342
00:58:39.390 --> 00:58:51.690
Julia - SUB INSTRUCTOR: Alright? So you guys actually have an activity. Where you're going to use a hugging face, pre-trained model to I think it's translating headlines

343
00:58:52.088 --> 00:59:08.359
Julia - SUB INSTRUCTOR: into different languages, which I think will actually be fun. It's a 15 min activity, but we are a little bit behind cause. We were chatting. We're chatting and so why don't we make it a 10 min activity?

344
00:59:08.410 --> 00:59:10.479
Julia - SUB INSTRUCTOR: And do you like

345
00:59:10.730 --> 00:59:13.959
Julia - SUB INSTRUCTOR: working together in groups.

346
00:59:15.032 --> 00:59:20.009
Julia - SUB INSTRUCTOR: Okay, yes. Okay. I see head nods and thumbs up. Okay, we can create some breakout rooms.

347
00:59:20.010 --> 00:59:22.789
michael mcpherson: Usually does random groups and random breakout rooms.

348
00:59:23.890 --> 00:59:25.410
Julia - SUB INSTRUCTOR: Okay, alright.

349
00:59:28.470 --> 00:59:30.609
Julia - SUB INSTRUCTOR: and just open up breakout rooms.

350
00:59:30.700 --> 00:59:33.650
Julia - SUB INSTRUCTOR: But it's activity 2

351
00:59:33.690 --> 00:59:34.495
Julia - SUB INSTRUCTOR: and

352
00:59:35.800 --> 00:59:40.910
Julia - SUB INSTRUCTOR: maybe you can create some really shocking headlines to translate into different languages.

353
00:59:41.706 --> 00:59:42.423
Julia - SUB INSTRUCTOR: But

354
00:59:43.410 --> 00:59:45.690
Julia - SUB INSTRUCTOR: We'll see you in 10 min.

355
00:59:46.210 --> 00:59:46.955
Julia - SUB INSTRUCTOR: and

356
00:59:48.150 --> 01:00:09.750
Julia - SUB INSTRUCTOR: And if if you feel like you need a little bit more than 10 min. We can give you a couple more minutes, but would love to be able to. Look at text generation, which is another fun application for transformers. So breakout rooms are open. You should have like a pop up window. That showed up. If you don't you should be able to hit the breakout room button and join your breakout rooms.

357
01:00:14.680 --> 01:00:16.710
Julia - SUB INSTRUCTOR: hey? Great.

358
01:00:23.470 --> 01:00:24.700
Julia - SUB INSTRUCTOR: wonderful!

359
01:00:25.260 --> 01:00:43.799
Julia - SUB INSTRUCTOR: So we're not gonna go through the solution for this. The solutions will be posted after class, so that if you wanna revisit it, you can want to keep moving, because I want to talk about text generation, which again, so many really great topics in this unit. And so relevant.

360
01:00:44.831 --> 01:00:46.540
Julia - SUB INSTRUCTOR: Let me get this in

361
01:00:46.550 --> 01:00:48.050
Julia - SUB INSTRUCTOR: full screen mode here.

362
01:00:48.863 --> 01:00:56.129
Julia - SUB INSTRUCTOR: But this is just another application for these transformer models. And you know, text generation is is I mean.

363
01:00:56.170 --> 01:01:18.360
Julia - SUB INSTRUCTOR: it's everywhere, right? Like, if you are, or even using your iphone there's you know, some text generation that happens when it tries to prompt you for what it thinks is gonna be the next word, or you know, other applications. Oh, great, great! Here's that nice image does someone wanna read this slide for me? Who's next to my zoom? Real? Oh, Curry.

364
01:01:19.630 --> 01:01:21.759
Julia - SUB INSTRUCTOR: tell us about text generation.

365
01:01:22.640 --> 01:01:25.890
Curry Gardner: Yeah. Tech generation is

366
01:01:27.250 --> 01:01:31.799
Curry Gardner: sorry text generation is one of the more common applications of a transform models.

367
01:01:32.400 --> 01:01:37.930
Curry Gardner: An example of text generation is using predictive text in text or email messages.

368
01:01:39.110 --> 01:01:42.049
Julia - SUB INSTRUCTOR: Perfect. Do you wanna popcorn it to someone else.

369
01:01:46.010 --> 01:01:47.160
Curry Gardner: Sure. Let's

370
01:01:47.470 --> 01:01:50.160
Curry Gardner: popcorn to

371
01:01:50.940 --> 01:01:51.910
Curry Gardner: Mike.

372
01:01:53.370 --> 01:01:58.279
Julia - SUB INSTRUCTOR: Alright, Mike, Curry gave you this slide with a lot of text. So good luck with this.

373
01:01:59.570 --> 01:02:00.836
Curry Gardner: Sorry, Mike. I didn't mean.

374
01:02:01.090 --> 01:02:01.870
Raugewitz, Tania: And David.

375
01:02:02.265 --> 01:02:02.660
michael mcpherson: He.

376
01:02:02.660 --> 01:02:03.926
Raugewitz, Tania: In German.

377
01:02:07.200 --> 01:02:09.779
michael mcpherson: We'll be here a long time.

378
01:02:10.574 --> 01:02:15.770
michael mcpherson: I I I can go German to English, but English to German is where I fail.

379
01:02:15.860 --> 01:02:17.520
michael mcpherson: Examples of prompts

380
01:02:17.530 --> 01:02:31.920
michael mcpherson: for text generation one. The user might provide an incomplete sentence and ask a model to finish it or ask a model to write a story, giving it a set setting characters or a few sentences as starting point

381
01:02:32.340 --> 01:02:33.210
michael mcpherson: tube.

382
01:02:33.360 --> 01:02:41.729
michael mcpherson: The user could even describe a programming problem and ask the model to generate code that solves the problem in a programming language of the user's choice.

383
01:02:42.510 --> 01:02:43.360
michael mcpherson: 3.

384
01:02:43.740 --> 01:02:53.130
michael mcpherson: To generate the output text from the prompt. The transformer uses a decoder in its knowledge of language, patterns, and context, to predict.

385
01:02:53.180 --> 01:02:56.010
michael mcpherson: IE. Generate the next word and

386
01:02:56.240 --> 01:02:59.249
michael mcpherson: sequence given the previous ones.

387
01:02:59.800 --> 01:03:01.559
Julia - SUB INSTRUCTOR: Perfect. Thank you so much.

388
01:03:01.943 --> 01:03:05.276
Julia - SUB INSTRUCTOR: Yeah, if you haven't checked it out, there's a new

389
01:03:06.392 --> 01:03:12.349
Julia - SUB INSTRUCTOR: you know. AI bot essentially a developer. Bot I think it's it's called Devin.

390
01:03:12.400 --> 01:03:22.760
Julia - SUB INSTRUCTOR: I forget what they named it. I wanna say it's Devin. But you know that that second use case is is really a reality, you know, a lot of the.

391
01:03:22.800 --> 01:03:40.640
Julia - SUB INSTRUCTOR: Although I I do wanna say a disclaimer being an instructor, you know, as part of this course highly recommend. You write all of your own code because that's really gonna help you learn but again, you know, from a business context. That's it. It's enabling a lot of people who may not have

392
01:03:41.180 --> 01:03:46.029
Julia - SUB INSTRUCTOR: that programming experience, or maybe the depth of programming experience that

393
01:03:46.800 --> 01:04:08.565
Julia - SUB INSTRUCTOR: you know that people would want. And so being able to, you know, say, Hey, I have a programming problem and and have that model like model generate, that code is just incredible. So so you can kind of see why these models got the name of generative AI right? Because this is really creating all these. You know this content? As part of

394
01:04:09.365 --> 01:04:12.039
Julia - SUB INSTRUCTOR: The output of these models and

395
01:04:12.320 --> 01:04:36.879
Julia - SUB INSTRUCTOR: these models are pre trained by analyzing really large amounts of text data, and they learn the probabilities that different words or tokens will occur in different contexts. And then, you know, basically using all of those probabilities, the transformer generates what is the output or what it's going to suggest or produce as text.

396
01:04:37.040 --> 01:04:49.430
Julia - SUB INSTRUCTOR: And you know, as the slide says, generative models can produce inaccurate, biased and even offensive responses. So you know you you do have to be careful. It is a reality.

397
01:04:49.930 --> 01:04:56.060
Julia - SUB INSTRUCTOR: curious. If anyone had any you know negative experiences or experiences with this, like.

398
01:04:56.170 --> 01:04:59.010
Julia - SUB INSTRUCTOR: while using Chat Gbt at all.

399
01:04:59.311 --> 01:05:06.600
Julia - SUB INSTRUCTOR: I had a coworker who had a fantastic time play with Chat Gbt. But curious, if any of you had any any experience like this.

400
01:05:09.600 --> 01:05:17.100
Meredith McCanse (she/her): I definitely use it when I do the homework assignments, and I would say, probably most folks in the class do, and it definitely is not always

401
01:05:17.370 --> 01:05:26.310
Meredith McCanse (she/her): accurate or correct, it still helps. I still feel like it's valuable, and it helps me get to something that works. But you can't take it at face value.

402
01:05:26.930 --> 01:05:28.570
Julia - SUB INSTRUCTOR: Yeah, absolutely.

403
01:05:28.650 --> 01:05:30.540
Julia - SUB INSTRUCTOR: That is 100%. True.

404
01:05:31.300 --> 01:05:33.599
Clayton Graves: There was. There was a time.

405
01:05:34.370 --> 01:05:38.379
Clayton Graves: was it last month, guys, that it started spitting out random gibberish?

406
01:05:39.530 --> 01:05:43.309
Clayton Graves: And it was it would make no sense whatsoever. You guys remember that.

407
01:05:45.543 --> 01:05:49.430
Clayton Graves: And then the other. The other thing I can think of is

408
01:05:50.360 --> 01:05:53.639
Clayton Graves: sometimes when I'm troubleshooting a problem at work.

409
01:05:53.930 --> 01:05:57.520
Clayton Graves: I'll get into a loop where it'll ask me to try something.

410
01:05:57.990 --> 01:06:01.080
Clayton Graves: and and I'll try it. I'm like, well today that didn't work.

411
01:06:01.210 --> 01:06:05.370
Clayton Graves: and then it'll have you try something else, and I'll say, well, that didn't work.

412
01:06:05.380 --> 01:06:10.790
Clayton Graves: and then it'll have me go back to the first one again. So I get stuck in these loops sometimes.

413
01:06:10.870 --> 01:06:12.560
Clayton Graves: but that's the worst of it.

414
01:06:13.470 --> 01:06:30.119
Julia - SUB INSTRUCTOR: Yeah, yeah. And and we'll see improvements. And you know, even in the next 6 months, for a lot of these things as these. These models mature and become better and better and better. But yeah, you know, you can see how, if if anyone had a concern about you, know

415
01:06:30.150 --> 01:06:37.791
Julia - SUB INSTRUCTOR: AI taking over the world and humans in the next year. It's we're we're not there yet at all. So not a concern.

416
01:06:38.110 --> 01:06:38.870
michael mcpherson: Yeah. Annette.

417
01:06:39.570 --> 01:06:40.912
Julia - SUB INSTRUCTOR: It goes kinda

418
01:06:41.640 --> 01:06:42.616
Julia - SUB INSTRUCTOR: yes,

419
01:06:44.874 --> 01:06:51.570
Meredith McCanse (she/her): This plot? 3. The one you were referring to, the new one that just came out in the last couple of days.

420
01:06:53.600 --> 01:06:58.449
Julia - SUB INSTRUCTOR: I mean, there's something coming out literally every week at this point. But yeah.

421
01:06:58.790 --> 01:06:59.979
Julia - SUB INSTRUCTOR: you can check it out.

422
01:07:01.002 --> 01:07:08.350
Julia - SUB INSTRUCTOR: Okay. So wanted to kind of show you an example of text generation in code. So

423
01:07:09.437 --> 01:07:12.520
Julia - SUB INSTRUCTOR: going into the

424
01:07:13.447 --> 01:07:25.000
Julia - SUB INSTRUCTOR: activities folder, and it's the 0 3 text generation unsolved. There should be a text underscore generation notebook files again. Gonna pull this open in.

425
01:07:25.140 --> 01:07:28.870
Julia - SUB INSTRUCTOR: collab and go ahead and just make sure I can

426
01:07:29.210 --> 01:07:30.619
Julia - SUB INSTRUCTOR: run my oops

427
01:07:30.690 --> 01:07:34.500
Julia - SUB INSTRUCTOR: oops. That was aggressive deleting here. So

428
01:07:34.700 --> 01:07:43.239
Julia - SUB INSTRUCTOR: yeah, and we're gonna use pipelines again. To make our lives easier. But we're gonna be looking at a different

429
01:07:43.559 --> 01:08:05.270
Julia - SUB INSTRUCTOR: pre-trained transformer model in this instance. And again, you know, I would poke around hugging face and take a look at the different models that are there. Because, again, as as much as some of these models have multi use, there's there. They're probably better in certain areas. And so you can kind of learn from that. I'm gonna go ahead and run this down here.

430
01:08:05.865 --> 01:08:15.364
Julia - SUB INSTRUCTOR: And so yes, okay. So once this runs, we'll be able to instantiate our generator. And the model that we're using here is called

431
01:08:15.840 --> 01:08:38.450
Julia - SUB INSTRUCTOR: a Luther, AI a Luther AI is actually a research collective really focused on large language models. They have some really great videos on Youtube. If you want to learn more about them. And essentially same thing, we're saying, Hey, pipeline, we want to do text generation. And we want to use this particular model here.

432
01:08:39.176 --> 01:09:00.600
Julia - SUB INSTRUCTOR: Here, we're actually using the Gpt Neo, 1.3 billion model, which has a really similar structure to chat. Gpt. 3, I believe, but uses fewer parameters than Chat Gpt. So if you think about chat gpt, I think it has somewhere in the order of like a hundred 75 billion

433
01:09:00.600 --> 01:09:23.179
Julia - SUB INSTRUCTOR: parameters that were used for training for the this particular model? Obviously, it, it's lot less only 1.3 compared to that 175 billion ish that check. Gpt 3 uses. And we're going to be using this model to actually generate some text.

434
01:09:23.500 --> 01:09:29.839
Julia - SUB INSTRUCTOR: Okay? So let me go ahead and instantiate my generator. And there it goes.

435
01:09:29.880 --> 01:09:48.120
Julia - SUB INSTRUCTOR: And the idea behind this, while it's kind of prepping itself is that we're gonna give it a prompt. So we're not gonna give it a full sentence, like we did in the last activity. We're just kinda starting people the the process off. So in this prompt it says, I like gardening because

436
01:09:48.120 --> 01:10:05.993
Julia - SUB INSTRUCTOR: and we're gonna leave it open ended because we want our model to fit in. Fill in the Y like, why do I like gardening. And it will be really curious. I'm I'm actually really curious what's gonna come out. But you kinda see, they left the output of the last run. Obviously, when we run it, we're we're gonna get a different output. Because.

437
01:10:06.260 --> 01:10:10.600
Julia - SUB INSTRUCTOR: you know, we're we're gonna be using a different

438
01:10:10.600 --> 01:10:27.059
Julia - SUB INSTRUCTOR: sort of instance of this model, but you know it says I like gardening, because it's something I enjoy doing. I think it makes me better at life in general. Well, that's that's great about gardening. But one of the things we will have to do is once we have while it's, you know, creating our generator.

439
01:10:27.340 --> 01:10:49.550
Julia - SUB INSTRUCTOR: we're gonna take our prompt and actually pass it to our generator and the way that works is that we'll call that generator and then pass in the prompt. We'll pass in the Max length, and we'll pass in something called a pad. Token. Id. Oh, great is done! Alright! So now we can actually use the generator. So I'm gonna call my generator or

440
01:10:51.340 --> 01:10:52.689
Julia - SUB INSTRUCTOR: bless you.

441
01:10:52.690 --> 01:10:53.710
Kevin Nguyen: Thank you. Sorry.

442
01:10:53.710 --> 01:10:55.630
Julia - SUB INSTRUCTOR: Wow! That was really loud.

443
01:11:00.680 --> 01:11:02.020
Clayton Graves: Thought you were barking.

444
01:11:03.200 --> 01:11:15.489
Julia - SUB INSTRUCTOR: Yeah, I think it was also because we were. We're in the middle of creating our generator. But anyways gonna pass in our prompt, which is the I like gardening because prompt.

445
01:11:16.070 --> 01:11:30.259
Julia - SUB INSTRUCTOR: We're gonna pass in the Max length, which is just saying, Hey, we don't want our our length to go beyond x value. And I think, the suggestion is just 125

446
01:11:32.490 --> 01:11:43.701
Julia - SUB INSTRUCTOR: token length here. And then we have to add the pad token id pad underscore token underscore id, and we're gonna set that to

447
01:11:44.160 --> 01:12:10.580
Julia - SUB INSTRUCTOR: 5, 0 2, 5, 6, and that that token id is actually very specific. Id, it allows for more open, ended text generation, which is kind of what we want. And then we have one other a fourth parameter. We're gonna pass in, which is a parameter called truncation. We actually do want it to kind of cut off like we don't want it to just kind of go on forever. So I'm gonna set my turn

448
01:12:10.820 --> 01:12:15.860
Julia - SUB INSTRUCTOR: truncation parameter to true with capital. T.

449
01:12:16.310 --> 01:12:26.840
Julia - SUB INSTRUCTOR: Alright. So I've got my generator. Oh, and we probably want to save the output of our generator into some like variable. So we'll just use the results variable

450
01:12:26.930 --> 01:12:29.190
Julia - SUB INSTRUCTOR: to kind of save that information.

451
01:12:29.700 --> 01:12:30.980
Julia - SUB INSTRUCTOR: And then.

452
01:12:31.110 --> 01:12:36.790
Julia - SUB INSTRUCTOR: once we do that, we can actually take a look at what's

453
01:12:36.790 --> 01:13:04.229
Julia - SUB INSTRUCTOR: you know? Returned right like, what is that generated text from that gardening prompt right and very similar to what we saw last time, where the output is in Json format. We're gonna have to do the same thing we're gonna have to look at, you know, the first element of our Json output. And then look at the key of generated text in order to just see, like, what is that text which we kind of saw down here?

454
01:13:05.136 --> 01:13:08.320
Julia - SUB INSTRUCTOR: So generated text

455
01:13:08.430 --> 01:13:12.819
Julia - SUB INSTRUCTOR: is going to be equal to whatever our results are right.

456
01:13:13.210 --> 01:13:20.919
Julia - SUB INSTRUCTOR: And again, because it's you know. You know essentially a dictionary or Json format. We'll look at that first

457
01:13:20.970 --> 01:13:22.340
Julia - SUB INSTRUCTOR: element.

458
01:13:22.420 --> 01:13:26.729
Julia - SUB INSTRUCTOR: and then I'm going to look at the key generated text.

459
01:13:27.440 --> 01:13:31.909
Julia - SUB INSTRUCTOR: and then then we could just print it out. Whoops, print

460
01:13:32.120 --> 01:13:34.910
Julia - SUB INSTRUCTOR: print generated text.

461
01:13:35.210 --> 01:13:37.250
Julia - SUB INSTRUCTOR: Okay, let me go ahead and run this.

462
01:13:37.390 --> 01:13:51.059
Julia - SUB INSTRUCTOR: And I think this is, gonna take a little bit of time, cause the generator has to take your prompt. It's gonna you know it's been trained on a corpus of information Tanya.

463
01:13:51.330 --> 01:13:57.820
Raugewitz, Tania: Yes, I'm sorry, Dan, while it's running. I was just curious if anybody else had an error trying to download a Luther

464
01:13:57.880 --> 01:13:59.949
Raugewitz, Tania: in theirs. Or is it just me?

465
01:14:02.311 --> 01:14:05.299
Julia - SUB INSTRUCTOR: Did you run the pip install at the very, very top.

466
01:14:05.300 --> 01:14:06.459
Raugewitz, Tania: Yes, I did.

467
01:14:06.680 --> 01:14:10.230
Julia - SUB INSTRUCTOR: Okay, and when you know when that.

468
01:14:10.230 --> 01:14:12.380
Raugewitz, Tania: If no one else has had that issue, I'll figure it out.

469
01:14:13.790 --> 01:14:15.919
Raugewitz, Tania: It's clearly just something that I did.

470
01:14:17.840 --> 01:14:19.010
Raugewitz, Tania: Okay, that's it.

471
01:14:19.340 --> 01:14:19.920
Julia - SUB INSTRUCTOR: Okay.

472
01:14:20.300 --> 01:14:23.800
Meredith McCanse (she/her): I mean, are you in? Are you in Google, Collab or in Bs code?

473
01:14:23.800 --> 01:14:25.370
Raugewitz, Tania: I am in Vs code.

474
01:14:26.960 --> 01:14:31.380
Meredith McCanse (she/her): I don't. I'm doing it in Co. Lab. I haven't tried it. Mbs. Code, but I wonder if that could be part of it.

475
01:14:31.729 --> 01:14:36.269
Clayton Graves: I did get that error in Bs code switch the code. There.

476
01:14:38.750 --> 01:14:39.330
Julia - SUB INSTRUCTOR: Okay.

477
01:14:45.200 --> 01:14:46.469
Julia - SUB INSTRUCTOR: trying to think. The only.

478
01:14:46.470 --> 01:14:47.120
Baro, Sonja: You

479
01:14:47.830 --> 01:14:51.910
Baro, Sonja: just walk through again on the index. So

480
01:14:52.230 --> 01:14:56.449
Baro, Sonja: results. So 0 is the first

481
01:14:57.490 --> 01:14:59.590
Baro, Sonja: value, right?

482
01:15:00.560 --> 01:15:05.999
Baro, Sonja: And then generated text in parentheses. Where are we getting that?

483
01:15:06.630 --> 01:15:07.470
Baro, Sonja: Yep.

484
01:15:07.470 --> 01:15:12.700
Julia - SUB INSTRUCTOR: So when okay? So we got our results from our generator model, right?

485
01:15:12.890 --> 01:15:13.260
Baro, Sonja: Right.

486
01:15:13.260 --> 01:15:19.019
Julia - SUB INSTRUCTOR: We're looking at the first index, and we're looking at the key.

487
01:15:19.554 --> 01:15:29.800
Julia - SUB INSTRUCTOR: Generated text, and the value is being stored in this variable called generated underscore text. And that's what I'm printing out here in this last line.

488
01:15:29.990 --> 01:15:37.462
Baro, Sonja: Yeah, my, my guess. My question is the parentheses generated text. The quote, that's no sorry

489
01:15:39.420 --> 01:15:45.089
Baro, Sonja: quotations helps if I use the right term, quotations generated. Text.

490
01:15:45.803 --> 01:15:49.470
Baro, Sonja: where does? Where did we identify that.

491
01:15:50.040 --> 01:16:08.410
Julia - SUB INSTRUCTOR: Oh, okay, yes. So let me scooch down a little bit. So we have a little room. If I just printed out the results variable from our generator. It actually looks like this. This is the Json format that's being passed back. But you'll see that, you know

492
01:16:09.000 --> 01:16:19.809
Julia - SUB INSTRUCTOR: dictionary key value pair. The first key value key value pair is gener. So the key is generated. Text and the value is all of this.

493
01:16:20.390 --> 01:16:21.759
Baro, Sonja: Sorry not good. Learn.

494
01:16:22.110 --> 01:16:27.250
Baro, Sonja: Yeah. And so is generated. Text always the key.

495
01:16:27.710 --> 01:16:29.570
Baro, Sonja: When we're doing this.

496
01:16:29.570 --> 01:16:33.220
Julia - SUB INSTRUCTOR: Correct for this particular model.

497
01:16:33.850 --> 01:16:38.100
Baro, Sonja: Okay, that's what I was like as I was like, how did we find that out? Like what.

498
01:16:39.000 --> 01:16:41.357
Baro, Sonja: Cause I hadn't seen that. So.

499
01:16:42.245 --> 01:16:44.719
Julia - SUB INSTRUCTOR: Documentation is your best friend.

500
01:16:44.900 --> 01:16:47.750
Baro, Sonja: That's right. Read the documentation, Sonya.

501
01:16:47.940 --> 01:17:04.849
Julia - SUB INSTRUCTOR: Yes, but most of the time it's it's usually some flavor. It's usually generated underscore tax or some flavoring. May it might be text, or it it there is there. Most of the models have similar kind of like labels for things, so

502
01:17:04.980 --> 01:17:09.320
Julia - SUB INSTRUCTOR: that also will help as you get more comfortable with seeing this.

503
01:17:10.910 --> 01:17:23.620
Julia - SUB INSTRUCTOR: Yeah. So let's see it says I like gardening, because there's always something to do with flowers. Nice. I hope you like flowers, too. And please don't get upset with the occasional. What is the meaning of this email you get?

504
01:17:23.680 --> 01:17:27.580
Julia - SUB INSTRUCTOR: It's not so bad, I promise. Wow! That's intense.

505
01:17:27.720 --> 01:17:49.229
Julia - SUB INSTRUCTOR: We have a big garden, and I love growing my own flowers and vegetables nice. It's kind of branching out from just flowers, because you could have vegetables in a garden, so it's getting a little smarter. My husband cultivates tomatoes, potatoes, and cucumbers, and I, of course, grow my own peppers and pumpkins. We are gardeners. Paradise and I love it here. It's a lot of work, but it is fun.

506
01:17:49.680 --> 01:17:52.449
Julia - SUB INSTRUCTOR: The email piece was really random.

507
01:17:52.680 --> 01:17:59.769
Julia - SUB INSTRUCTOR: I would say, but you know, it actually did a pretty good job right of generating this text.

508
01:17:59.980 --> 01:18:05.740
Julia - SUB INSTRUCTOR: Now, one thing I do want to show you which. There's like an example down here. Is that

509
01:18:05.900 --> 01:18:35.071
Julia - SUB INSTRUCTOR: the reason why this is probably not as good as Chat Gpt. 3 is because we use a smaller model right? We use one that was only trained on 1.3 billion parameters. Chat gpt again, orders of magnitude more. But you know, to really kind of bring home that point, that it does matter. You know how much training data you provide will look at another. A Luther AI model that uses even less.

510
01:18:35.720 --> 01:18:51.129
Julia - SUB INSTRUCTOR: data. I think it uses like a hundred. Where'd it go? Oh, yeah. So it uses a hundred 25 million parameters. So less than the 1.3 billion parameters. So we'll kind of see what comes out of this. And it will probably look like total nonsense, because it's just it's just not gonna

511
01:18:51.130 --> 01:18:58.350
Julia - SUB INSTRUCTOR: be able to cut it with this. But again, we'll do it for fun. So we've got our new pipeline instance here.

512
01:18:58.350 --> 01:19:22.119
Julia - SUB INSTRUCTOR: And we're again using text generation, and we're using an a Luther model AI model. But it is a different model. And this particular model, again, is the Gpt Neo 125 million perimeter model. And we're just storing that in a new variable called small underscore generator. So we'll actually use that to generate our next prompt.

513
01:19:22.120 --> 01:19:22.969
Julia - SUB INSTRUCTOR: So

514
01:19:24.110 --> 01:19:30.836
Julia - SUB INSTRUCTOR: This prompt is my fan. My favorite animal is the cat, because well, this is going to be interesting.

515
01:19:31.480 --> 01:20:00.659
Julia - SUB INSTRUCTOR: and so same thing here we're gonna pass the prompt to our generator. So you know, I can actually, just again, you can always copy and paste code that you know that works right? I do it because I it's hard to scroll and type and talk at the same time. But I am gonna label this new results. So I can kind of if I wanted to compare my old results with my new results. I can. But it's passing in my prompt.

516
01:20:00.890 --> 01:20:22.319
Julia - SUB INSTRUCTOR: It's passing in as the Max length. And I'm actually gonna make this shorter, because, again, it says, pass the prompt to the generator. Use a Max length of 25, and this will even make it more challenging because we are almost saying, Hey, we don't wanna set, you know, pass in as much information into our model to generate information. So

517
01:20:22.658 --> 01:20:46.009
Julia - SUB INSTRUCTOR: it's it's I think the output is, gonna be very interesting. And then we again up here use this idea of getting the results, looking at that first element and then using this key in order to get the generated tasks. So I'm gonna copy this. But I do have to make one modification which is just updating the the reference variable here because I hadn't

518
01:20:46.010 --> 01:20:50.929
Julia - SUB INSTRUCTOR: renamed it to new results. So I'm gonna update this to new results.

519
01:20:51.494 --> 01:20:55.509
Julia - SUB INSTRUCTOR: And then I'm gonna go ahead and print out my generated text.

520
01:20:57.310 --> 01:21:00.170
Julia - SUB INSTRUCTOR: generated text, there we go.

521
01:21:00.350 --> 01:21:07.390
Julia - SUB INSTRUCTOR: and so let me go ahead and run it and it. This should actually run much faster than the previous model, because it is much smaller.

522
01:21:07.480 --> 01:21:13.714
Julia - SUB INSTRUCTOR: and we'll see what comes out. But oftentimes, you know, especially again with these

523
01:21:14.820 --> 01:21:17.170
Julia - SUB INSTRUCTOR: language models. You do need.

524
01:21:17.360 --> 01:21:35.699
Julia - SUB INSTRUCTOR: you know, more more data, more data, more instances, more examples. Right? So we can get smarter and smarter and smarter. But I am curious. Oh, here it comes back. It says my favorite animal is the cat, because it loves to eat my cat food and sleep on my bed. You can never have to.

525
01:21:35.880 --> 01:21:42.290
Julia - SUB INSTRUCTOR: and it decided to leave you on the edge of your seats to figure out what could possibly

526
01:21:42.900 --> 01:21:49.160
Julia - SUB INSTRUCTOR: what could you never have? Too much of? Too little of? I don't know. But now I'm I'm hooked

527
01:21:50.070 --> 01:21:50.870
Julia - SUB INSTRUCTOR: so

528
01:21:51.670 --> 01:21:56.430
Clayton Graves: If I've got a closet full of cat food having a cat would be the best way to get rid of it.

529
01:21:57.060 --> 01:22:00.690
Julia - SUB INSTRUCTOR: That's true. That is true. So

530
01:22:01.175 --> 01:22:12.859
Julia - SUB INSTRUCTOR: yeah, there you go. But you know what you're seeing. Here is a hallucination, right? You've probably heard that terminology before, but it's just, you know, nonsensical or just, you know.

531
01:22:12.860 --> 01:22:34.700
Julia - SUB INSTRUCTOR: you know, kind of imaginative output. And again, when you see this kind of thing. It just means the models too small, and he needs to be trained on more data. So this is just kind of like bringing home. The fact that you know the size of the the number of parameters, the size of the training data that you're passing in really impacts the quality of your model and the output.

532
01:22:35.870 --> 01:22:40.259
Derek Rikke: I think you accidentally use the same model instead of the small model.

533
01:22:40.560 --> 01:22:41.449
Julia - SUB INSTRUCTOR: Oh, did I.

534
01:22:41.920 --> 01:22:42.829
Raugewitz, Tania: Yeah, I was. Gonna say.

535
01:22:43.150 --> 01:22:45.890
Raugewitz, Tania: do you have the use? The small generator.

536
01:22:46.190 --> 01:22:47.809
Julia - SUB INSTRUCTOR: Mall. Generate. Okay.

537
01:22:48.896 --> 01:22:49.989
Dipinto, Matt: Reassuring, because

538
01:22:50.160 --> 01:22:52.370
Dipinto, Matt: your model was way more better.

539
01:22:53.149 --> 01:23:01.780
Julia - SUB INSTRUCTOR: As well. Generator. Okay, this. That is one complication with with copying and pasting.

540
01:23:02.740 --> 01:23:05.896
Raugewitz, Tania: Go up one cell and and run that. Use the text generator.

541
01:23:06.160 --> 01:23:09.430
Julia - SUB INSTRUCTOR: Cause I yeah, cause I didn't actually run it, cause I was

542
01:23:10.000 --> 01:23:19.469
Julia - SUB INSTRUCTOR: chit chatting, trying to scroll. Yes, so alright. Let's see, let's see if we'll be even better, maybe we'll find out maybe what was too much.

543
01:23:21.450 --> 01:23:25.270
Julia - SUB INSTRUCTOR: Okay, all right, let's cross our fingers. Let's see.

544
01:23:27.010 --> 01:23:34.700
Julia - SUB INSTRUCTOR: Oh, actually, this isn't too bad. My favorite animal is the cat, because it's so cute. I love cats because they're so cute. I love cats because.

545
01:23:35.760 --> 01:23:37.860
Meredith McCanse (she/her): I got the exact email.

546
01:23:37.860 --> 01:23:39.310
Baro, Sonja: They're so cute.

547
01:23:39.440 --> 01:23:40.610
Julia - SUB INSTRUCTOR: They're so cute.

548
01:23:40.610 --> 01:23:41.550
Raugewitz, Tania: Is it a loop?

549
01:23:42.780 --> 01:23:52.540
Masarirambi, Rodney: Limiting like the out. So we're limiting the output right of the length of the story. And so, okay, sorry I was, yeah.

550
01:23:53.020 --> 01:23:53.580
Julia - SUB INSTRUCTOR: Yeah.

551
01:23:53.850 --> 01:23:54.690
Baro, Sonja: Rodney, Texas.

552
01:23:55.035 --> 01:23:55.380
michael mcpherson: Spring!

553
01:23:55.380 --> 01:23:57.080
Baro, Sonja: Sure is they're so cute.

554
01:23:57.360 --> 01:24:17.894
Julia - SUB INSTRUCTOR: Yeah, we we can. We can change this and see what happens. And that. And again, you know, that's the whole point of being able to have these code activities. You can always try different things and see what happens. You know, nothing's gonna really happen if you break it because it's just it's just code. Oh, God, yeah. So.

555
01:24:18.210 --> 01:24:19.779
Baro, Sonja: I mean I'll be 10 bucks.

556
01:24:20.730 --> 01:24:23.730
Julia - SUB INSTRUCTOR: Yeah, if so, if there was any question.

557
01:24:23.810 --> 01:24:35.239
Julia - SUB INSTRUCTOR: cats are so cute. Right? So again, this, this is an even better example, because I could. It was the combination of the the Max length and

558
01:24:36.848 --> 01:24:47.381
Julia - SUB INSTRUCTOR: yes, and they now small generator. But you know again the example, nation. And it's not very helpful. Right? But

559
01:24:48.880 --> 01:25:01.270
Julia - SUB INSTRUCTOR: yeah, we have a long ways to go before the robots and AI take over the world as you could see. So Tanya, you still have your hand up. Do you have a follow up question or

560
01:25:01.960 --> 01:25:03.440
Julia - SUB INSTRUCTOR: And did you get your code.

561
01:25:03.440 --> 01:25:07.329
Raugewitz, Tania: Okay, Rodney, I'm lowering our hands. I was just pointing out the small generator.

562
01:25:07.530 --> 01:25:11.639
Julia - SUB INSTRUCTOR: Yeah, no, that was perfect, amazing. And I'm glad that you were paying attention.

563
01:25:11.892 --> 01:25:17.017
Julia - SUB INSTRUCTOR: I could even say that I was doing it on purpose to make sure that you were all paying attention, but in actual.

564
01:25:17.240 --> 01:25:18.210
Raugewitz, Tania: Anthony would say.

565
01:25:19.951 --> 01:25:45.160
Julia - SUB INSTRUCTOR: Yeah. Well, it was more that I was copying pasting, because my typing is horrendous. But anyways, you have an activity where you're gonna test. I think maybe 3. I wanna say 3 different hugging face. Transformer models to generate some text. And again, it it's like a 15 min activity. But you you can see it's pretty straightforward, and it's it's pretty fun. So we'll make it 10 min.

566
01:25:45.639 --> 01:25:50.429
Julia - SUB INSTRUCTOR: And then we'll come back and go on break.

567
01:25:50.470 --> 01:26:06.199
Julia - SUB INSTRUCTOR: and then we'll actually look at question and answering, which I think is always really fun, because we actually use that in business contacts at my work. But I will open up breakout rooms. Do you want to stay in the same breakout rooms, or do you want to see? Different faces.

568
01:26:06.370 --> 01:26:07.510
Julia - SUB INSTRUCTOR: Yeah, okay.

569
01:26:07.510 --> 01:26:08.609
Meredith McCanse (she/her): Stay in the same

570
01:26:08.920 --> 01:26:09.650
Meredith McCanse (she/her): sorry.

571
01:26:11.360 --> 01:26:14.829
Julia - SUB INSTRUCTOR: Alright, so breakout rooms are open. We'll do 10 min.

572
01:26:15.340 --> 01:26:19.670
Julia - SUB INSTRUCTOR: Generate lots of interesting tacts, and then we'll go on break.

573
01:26:19.820 --> 01:26:23.219
Julia - SUB INSTRUCTOR: and you should have a pop-up window in front of you.

574
01:26:23.290 --> 01:26:26.720
Julia - SUB INSTRUCTOR: If you don't, you can hit the breakout room button at the bottom

575
01:26:27.250 --> 01:26:28.080
Julia - SUB INSTRUCTOR: and

576
01:26:30.590 --> 01:26:36.650
Julia - SUB INSTRUCTOR: I'll remember to actually keep time this time like last. Oh, no! I forgot to shorten the stupid.

577
01:26:38.120 --> 01:26:40.580
Julia - SUB INSTRUCTOR: you know how long it takes to close the rooms

578
01:26:41.240 --> 01:26:43.130
Julia - SUB INSTRUCTOR: hopefully, I'll remember that after break.

579
01:26:43.720 --> 01:26:45.090
Julia - SUB INSTRUCTOR: See you in a bit.

580
01:26:48.630 --> 01:27:17.979
Julia - SUB INSTRUCTOR: Alright! Here we go so welcome back from break. There was a couple of things I wanted to point out, one is the Luther. Ai website actually has some really great articles. If you're interested in reading more, they have a ton of papers. But one interesting paper that they published. I think it was in February. Oh, yeah, you hear? It is February. Is this idea around pink elephants. It's it's it's a term that's used in, you know, these large language models where

581
01:27:17.980 --> 01:27:26.559
Julia - SUB INSTRUCTOR: sometimes you know, it will. You know, suggest something that you don't want it to suggest. We're actually having this problem at our company.

582
01:27:27.256 --> 01:27:43.033
Julia - SUB INSTRUCTOR: we have a competitor, and our our. Our chat. Bot is very helpfully suggesting that our customers, when they ask questions, you know, go use our competitor, you know, as the solution. So obviously, that's that's sort of our pink elephant problem.

583
01:27:43.400 --> 01:28:07.800
Julia - SUB INSTRUCTOR: And we're we're trying to resolve it. But anyways, some really great articles here. If you're interested in re reading more. I'll throw these links in slack at at the next of activity. And then the other thing I wanted to point out is, aws! Has a ton of really great content around generative AI. What you want to do is you wanna look for the generative AI data edition cause. They have different

584
01:28:07.800 --> 01:28:20.931
Julia - SUB INSTRUCTOR: kind of versions of this. These conferences. This happens to be recording, because I wanna say, I can't remember when they held it. But there's another one coming up in June. They're all virtual. And

585
01:28:21.440 --> 01:28:42.069
Julia - SUB INSTRUCTOR: they have some really great speakers, some really great content. Some of it's technical. Some of it's not so if you're interested and just kinda like learning more and then hearing many different applications for generative AI, I suggest again, you know, checking out some of these recordings, and then again, they have one another conference coming up in I think it's June.

586
01:28:42.100 --> 01:28:46.979
Julia - SUB INSTRUCTOR: I can't remember if it was Aws or Google. But there's a whole bunch of conferences that are coming up

587
01:28:47.230 --> 01:29:02.110
Julia - SUB INSTRUCTOR: okay, alright. So we're gonna dive into the last topic for tonight. Or excuse me. Second to last topic for tonight, which is question and answering, let me get this into full screen mode, so we can get through this. But it's just another use case for our transformer models.

588
01:29:02.190 --> 01:29:09.269
Julia - SUB INSTRUCTOR: And you know, a great use case for these question and answer models is customer service chat bots.

589
01:29:09.655 --> 01:29:30.099
Julia - SUB INSTRUCTOR: That's how we use them at our company. And you know, if you go to a website right, you might get like one of those pop ups in the bottom corner that are like, you know. Hey? Do you need help with anything that very much likely is using a transformer model in the background to train the responses for these chat bots

590
01:29:30.100 --> 01:29:39.563
Julia - SUB INSTRUCTOR: and the thing that's a little bit unique because we did look at the transformer architecture which had the encoder and decoder, as like a 2 part system.

591
01:29:39.890 --> 01:29:58.119
Julia - SUB INSTRUCTOR: these types of QA. Or question and answering transformer models actually only use the encoder. And so this architecture is a little bit different because it doesn't have that 2 piece system where you have the encoder and the decoder. These QA.

592
01:29:58.430 --> 01:30:05.670
Julia - SUB INSTRUCTOR: transformer models just use the encoder piece. And what that means is that they just take the input text.

593
01:30:06.163 --> 01:30:22.196
Julia - SUB INSTRUCTOR: and they encode it. And it's used downstream in in downstream tasks, or, you know, generating output versus generating output text like we did with that predictive example right before the break.

594
01:30:22.630 --> 01:30:27.029
Clayton Graves: If there's no decoder, then how does it answer my question in English.

595
01:30:27.600 --> 01:30:30.399
Julia - SUB INSTRUCTOR: Well, let's take a look.

596
01:30:31.590 --> 01:30:33.670
Julia - SUB INSTRUCTOR: Yeah, no, there's

597
01:30:34.710 --> 01:30:37.249
Julia - SUB INSTRUCTOR: We'll kind of like. See this in action. But

598
01:30:37.738 --> 01:30:53.329
Julia - SUB INSTRUCTOR: the the best example is, or the most common transformer based encoder model is this one called Bert and you can see it. It stands for bi-directional encoder representations from transformers.

599
01:30:53.878 --> 01:30:58.270
Julia - SUB INSTRUCTOR: And these burp based models can be used

600
01:30:59.038 --> 01:31:03.231
Julia - SUB INSTRUCTOR: to get information from past and future

601
01:31:04.460 --> 01:31:12.193
Julia - SUB INSTRUCTOR: words or tokens in a sequence to really better understand what's there is. So it's almost like it's

602
01:31:12.940 --> 01:31:19.290
Julia - SUB INSTRUCTOR: It's not like really translating anything. It's just kind of like referencing stuff that it's seeing

603
01:31:19.708 --> 01:31:36.630
Julia - SUB INSTRUCTOR: but you can kind of see in this slide that one the Bart architecture allows for bi-directional information flow, and then the model uses past and future tokens. And then you have, like this sort of example here on this side and in the example, here, move my zoom window.

604
01:31:36.980 --> 01:31:43.280
Julia - SUB INSTRUCTOR: We start out with a question where it says, You know, is it true that male bees get kicked out of the hive during the winter?

605
01:31:43.290 --> 01:32:05.380
Julia - SUB INSTRUCTOR: And so again, it's it's been trained, and we'll see how this works. In a code example. But you know yes, you know Melbies or drones are forcibly removed from the hive when resources are scarce. Blah blah blah! And then you know, the person then goes on to say, what is the biggest whale on the planet. And you know it responds. A blue whale is the largest whale on the planet.

606
01:32:05.480 --> 01:32:24.990
Julia - SUB INSTRUCTOR: And then the person clearly thought of something else having to do with bees, and said, Do bees choose their queen by giving her purple honey, and then it responds, no, bees do not choose a queen based on honey color, and it kind of goes on. But the thing that's unique about this is that you know the

607
01:32:25.310 --> 01:32:38.689
Julia - SUB INSTRUCTOR: Third question kind of remembers the first question about bees, even though, the question in between is about whales. And so this is really important. Because I think

608
01:32:38.910 --> 01:32:40.389
Julia - SUB INSTRUCTOR: if you

609
01:32:40.660 --> 01:33:08.409
Julia - SUB INSTRUCTOR: if you were to interact with a chat bot, maybe 5 years ago. It's it's quite a frustrating experience, because there might have been something you mentioned earlier in the conversation that it didn't remember or pick up. And you almost have to kind of like it's it's like phone tree. It's it's literally like a logic tree almost in in the type of experience that you have. But now these chat bots have gotten much better at picking up again sort of that context, and in

610
01:33:08.750 --> 01:33:11.339
Julia - SUB INSTRUCTOR: responses that you had many, many

611
01:33:11.350 --> 01:33:13.559
Julia - SUB INSTRUCTOR: responses ago. Right? So

612
01:33:13.920 --> 01:33:24.070
Julia - SUB INSTRUCTOR: that's really kind of like the the point that I'm trying to make here. But these Burk based models are typically used for sentiment analysis.

613
01:33:24.070 --> 01:33:44.379
Julia - SUB INSTRUCTOR: because you might be talking about something, but maybe in combination with something that you said a few sentences ago. Maybe a few paragraphs ago. It gives it a different tone. So it's starting to incorporate some of that nuance. I forget who asked that question earlier, but it's now able to con, you know. Add another layer of context on

614
01:33:44.730 --> 01:34:08.530
Julia - SUB INSTRUCTOR: it can also be used for text prediction. text generation. Because it does a better job of kind of like navigating. You know different words in in the context of what you're trying to say, and then you know again, question and answering sort of like the the, you know, more popular use case for these birth based models.

615
01:34:08.550 --> 01:34:12.500
Julia - SUB INSTRUCTOR: But let's take a look at this encode. And we can actually see

616
01:34:14.540 --> 01:34:16.220
Julia - SUB INSTRUCTOR: well, how this works.

617
01:34:16.240 --> 01:34:21.500
Julia - SUB INSTRUCTOR: Okay, I'm not gonna close that because I don't even remember to put those links in slack.

618
01:34:22.040 --> 01:34:25.769
Julia - SUB INSTRUCTOR: Okay? So I'm gonna go and open up. It should be the

619
01:34:26.830 --> 01:34:29.020
Julia - SUB INSTRUCTOR: A fifth

620
01:34:29.140 --> 01:34:36.750
Julia - SUB INSTRUCTOR: folder question and answering unsolved. And there should be a question and answering notebook file. So I'm going to go ahead and open that up.

621
01:34:37.170 --> 01:34:41.390
Julia - SUB INSTRUCTOR: And then I'm going to go ahead and do the usual. So I'm going to run my

622
01:34:41.620 --> 01:34:43.510
Julia - SUB INSTRUCTOR: pip install.

623
01:34:44.060 --> 01:35:01.782
Julia - SUB INSTRUCTOR: And then I'm gonna import from transformers and use my pipeline. And then, same thing. You know, we're gonna say, Hey, I'm gonna scroll down a little bit, you know, we we're really interested in this question and answering type model. And this model here. This

624
01:35:03.485 --> 01:35:09.234
Julia - SUB INSTRUCTOR: distilled squad model is a I would say light

625
01:35:10.570 --> 01:35:20.670
Julia - SUB INSTRUCTOR: like a light version of Bert. So like it, it's kind of like a you know. Not not as is probably not as sophisticated as a typical Bert model. But

626
01:35:21.006 --> 01:35:42.530
Julia - SUB INSTRUCTOR: it's a hugging face, you know, pre change transformer model. So if you didn't get what the theme was for today, it's, you know, hugging face pre change transformer models. Let me go ahead and run this. So we get that loaded and then a little bit about this model. It was trained on this database called squad.

627
01:35:42.590 --> 01:36:01.460
Julia - SUB INSTRUCTOR: So usually with these question and answers, there's some kind of corpus of data or data set or database that it uses to learn from. And this particular model uses squad. I can't remember what it stands for. I think. Squat. The S. And squad stands for Stanford.

628
01:36:03.380 --> 01:36:08.279
Julia - SUB INSTRUCTOR: Maybe squad probably, is Stanford. Question answer.

629
01:36:08.680 --> 01:36:12.679
Julia - SUB INSTRUCTOR: Database probably is probably what the acronym I'm I'm guessing is

630
01:36:12.690 --> 01:36:15.475
Julia - SUB INSTRUCTOR: is, but it's a pretty comprehensive

631
01:36:16.270 --> 01:36:31.260
Julia - SUB INSTRUCTOR: data set. That was, you know, database that was created by by Stanford, and it's using that as its reference. But you can actually have it point to you know many different sources. If you were training your own model.

632
01:36:31.320 --> 01:36:52.970
Julia - SUB INSTRUCTOR: And okay, oh, great, it loaded. And so for our Bert model here, we're gonna provide some context. So again, because we're not it's it's not it. It kinda needs something to look at in order for it to figure out what's happening. We have to provide it some context. And so

633
01:36:52.970 --> 01:37:07.390
Julia - SUB INSTRUCTOR: this section of code here, where we've got this variable text and setting it to this article which is actually about transformers from Wikipedia. We're gonna use that as our context.

634
01:37:07.400 --> 01:37:23.419
Julia - SUB INSTRUCTOR: And so that is going to allow us to when we create this question and answer model. It's going to use this to pull out, answers and feed them back to us. Okay, so let me run this. It's just saying, hey, what is the tax?

635
01:37:23.610 --> 01:37:26.539
Julia - SUB INSTRUCTOR: And then here's a list of questions that we have.

636
01:37:26.929 --> 01:37:30.880
Julia - SUB INSTRUCTOR: And one of the things that's a little tricky about these

637
01:37:32.570 --> 01:37:37.280
Julia - SUB INSTRUCTOR: these types of models, and just kind of like AI in general, is that you know

638
01:37:37.330 --> 01:37:38.940
Julia - SUB INSTRUCTOR: how you know?

639
01:37:39.030 --> 01:37:53.040
Julia - SUB INSTRUCTOR: How do you cite your source? Right, or where? Where are you getting your source of information? Because, I think there's a lawsuit that's going on between when I say it's like the New York Times and Openai, but

640
01:37:53.180 --> 01:38:17.839
Julia - SUB INSTRUCTOR: they change, tap, gpt on like, you know, the New York Times and New New York Times is saying, Hey, this is this is our proprietary data, right? And so that's one thing to kind of think about when you're training your QA. Models, because you know one, where are you getting that data from? And is it actually accurate? And then 2, you know, is that actually data you can train your data set on? So for example, at my company.

641
01:38:17.840 --> 01:38:27.469
Julia - SUB INSTRUCTOR: We're training, obviously, these Q. And a models for our chat bots. And they're trained on our internal knowledge base that we. We generated ourselves.

642
01:38:27.836 --> 01:38:32.239
Julia - SUB INSTRUCTOR: But we also do incorporate external sources of data to kind of

643
01:38:32.552 --> 01:38:58.170
Julia - SUB INSTRUCTOR: make our our chat bot comprehensive. And you know, we we have to have legal agreements in order to use that data for that purpose. So just kind of be aware of that, as you know, an area of concern for a lot of people. But anyways, we're gonna generate a list of questions. So here's a list of questions. We've got 3 questions in our list. We've got one where it says, when we're transformers first introduced

644
01:38:58.799 --> 01:39:08.349
Julia - SUB INSTRUCTOR: question 2. Element 2 in our list is what our transformers better than. And then third question is, what are applications of transformers? So go ahead and run that.

645
01:39:08.700 --> 01:39:21.199
Julia - SUB INSTRUCTOR: And now we have to do a little bit of work. Oh, and then also something to think about as we kind of move through this activity, like, what if you ask question where the answer isn't in

646
01:39:21.280 --> 01:39:30.389
Julia - SUB INSTRUCTOR: the text? Right? Because all of the answers for these questions actually live in this text that we're passing in that article from Wikipedia. But

647
01:39:30.920 --> 01:39:33.950
Julia - SUB INSTRUCTOR: what do you think happens if you ask a question that

648
01:39:34.530 --> 01:39:36.199
Julia - SUB INSTRUCTOR: doesn't have an answer there.

649
01:39:36.670 --> 01:39:47.889
Julia - SUB INSTRUCTOR: Alright. So let's go ahead and write a little bit of code. It says, check the output from one question. So let's say, I wanted to. Just use the

650
01:39:48.750 --> 01:39:53.179
Julia - SUB INSTRUCTOR: the question. Okay, I'll just copy and paste this. Actually, I'll just say.

651
01:39:53.510 --> 01:39:57.250
Julia - SUB INSTRUCTOR: we'll use this first one. So question is, gonna be this.

652
01:39:58.260 --> 01:40:01.900
Julia - SUB INSTRUCTOR: when we're transformers for first introduced.

653
01:40:02.170 --> 01:40:06.990
Julia - SUB INSTRUCTOR: And then again, we can use that question and answer instance. So

654
01:40:07.140 --> 01:40:09.915
Julia - SUB INSTRUCTOR: here's our question and answer.

655
01:40:10.550 --> 01:40:16.160
Julia - SUB INSTRUCTOR: instance, that was based on this light version of Bert.

656
01:40:17.080 --> 01:40:26.080
Julia - SUB INSTRUCTOR: and I'll just copy and paste it. So I don't even have to type it out. But I'm going to go ahead and say, Hey, I'm going to use that question and answer a model to pass in my question.

657
01:40:27.780 --> 01:40:35.710
Julia - SUB INSTRUCTOR: And I'm going to say the answer is going to be in my text. And they call it context. So context is going to equal text.

658
01:40:35.730 --> 01:40:43.919
Julia - SUB INSTRUCTOR: and where I'm getting this is this variable here, and where I'm getting the context text is

659
01:40:44.210 --> 01:40:48.599
Julia - SUB INSTRUCTOR: this variable up here? So that's where those 2 variables are coming from.

660
01:40:49.190 --> 01:41:06.848
Julia - SUB INSTRUCTOR: And oh, I should capture my, you know, output my result right? So I'm just gonna save all of this into a result, and then I'll just print out my result here. So you can actually see what comes out. And we should get an answer actually from this that will hopefully

661
01:41:07.490 --> 01:41:10.410
Julia - SUB INSTRUCTOR: be the right answer. Okay, it actually came back.

662
01:41:10.490 --> 01:41:14.890
Julia - SUB INSTRUCTOR: So we can see that. We have

663
01:41:15.830 --> 01:41:19.580
Julia - SUB INSTRUCTOR: Actually, it's saying that the answer is 2017.

664
01:41:20.705 --> 01:41:21.770
Julia - SUB INSTRUCTOR: Text.

665
01:41:22.460 --> 01:41:24.180
Julia - SUB INSTRUCTOR: Let me run this.

666
01:41:26.010 --> 01:41:35.240
Julia - SUB INSTRUCTOR: Just want to print it out. Okay, yes. So transformers. So I'm looking at the paragraph. Transformers were introduced in 2,017.

667
01:41:35.250 --> 01:41:55.030
Julia - SUB INSTRUCTOR: So our question and answer model came back with the answer, 2017. And that's correct. It's actually in our article and the score here is, essentially saying, what is the probability associated with the answer? And how close is that answer

668
01:41:55.030 --> 01:42:07.370
Julia - SUB INSTRUCTOR: to what the the question is looking for. How close is that match to the answer that the question is looking for? And it's pretty high in this instance. Right? It's a little more. It's almost 91%, which is great.

669
01:42:07.866 --> 01:42:11.836
Julia - SUB INSTRUCTOR: And then these 2 fields here the start and end

670
01:42:12.980 --> 01:42:37.610
Julia - SUB INSTRUCTOR: Attributes are essentially, you know, where in the context, where in the article does this answer live? Right? And so if we actually looked at the position of 2, you know, 2,017 that it would likely be here. You know, this position actually is position 8640, I'm sorry. Wrong one.

671
01:42:37.936 --> 01:42:54.590
Julia - SUB INSTRUCTOR: This one is the position of 864. And then this is the position, or maybe the white space, maybe might be the position 868. So these start and end positions correspond to. Where, in this body of text does that answer live

672
01:42:54.940 --> 01:43:09.960
Julia - SUB INSTRUCTOR: which is really great? And the score is actually the highest probability score, which is why it looks so great. It's like almost 91 so what's happening is when we pass in. Our question.

673
01:43:10.010 --> 01:43:15.709
Julia - SUB INSTRUCTOR: you know, which was when we're transformers first introduced, you know, it's breaking up

674
01:43:16.060 --> 01:43:32.380
Julia - SUB INSTRUCTOR: our sentence into words, and then it's passing it into our question and answer model, and then to get the answer to our question. Our model is trying to guess or predict where the answer starts and end in the context. So it's actually, you know.

675
01:43:32.380 --> 01:43:59.640
Julia - SUB INSTRUCTOR: doing a probability analysis based on the text here to say, I think the answer is, you know. Maybe here it's it's like a point 0 0 one likelihood that it starts there. But you know, as we get closer to here, it's getting higher and higher. And so this 91 is sort of like, okay, we think this is the answer, because this is the highest score that I found in analyzing this entire set of text.

676
01:44:00.630 --> 01:44:09.530
Julia - SUB INSTRUCTOR: Okay, so moving on. Any questions before I kind of go into this function that we are going to be building?

677
01:44:11.360 --> 01:44:12.550
Julia - SUB INSTRUCTOR: No, okay.

678
01:44:12.660 --> 01:44:31.079
Julia - SUB INSTRUCTOR: you know one great thing again. We. I think it was very early on in this course. You, you know, learned about modularity, object oriented programming, you know, reusing code, you know, because this idea of question answer is probably, gonna you know, happen over and over again with many different questions.

679
01:44:31.347 --> 01:44:52.450
Julia - SUB INSTRUCTOR: What this section of code is saying. It's just, hey, you know, it makes sense to build a function right? Like, why rewrite this code all the time we're in the. In the previous example. I was copying pasting some code, and you know there were some mistakes that you know ended up happening. And and that's because, you know, we could have written a function. And so what we're gonna do now is actually write a function.

680
01:44:52.680 --> 01:45:00.280
Julia - SUB INSTRUCTOR: And you can see that we're importing pandas here because we're actually going to make use of a good old data frame.

681
01:45:00.540 --> 01:45:24.850
Julia - SUB INSTRUCTOR: And then we're going to define this question answer function here. So you can see that it actually takes 2 parameters. One is questions, and one is the text so similar to the function that we actually, you know, use when we were passing in information into our model? But essentially, these things are gonna go into the model every time we call this function.

682
01:45:25.200 --> 01:45:35.249
Julia - SUB INSTRUCTOR: Okay? So let's walk through the comments. And, you know, try to write out the code that we want cause. It's just kind of pseudo coding it for us. So it says, create a list

683
01:45:35.290 --> 01:45:41.310
Julia - SUB INSTRUCTOR: to hold the data that will be added to our data frame. So I'm gonna create a list oops data.

684
01:45:42.500 --> 01:45:44.739
Julia - SUB INSTRUCTOR: It's just an empty list, right?

685
01:45:45.180 --> 01:45:49.190
Julia - SUB INSTRUCTOR: That says, Use a for loop to iterate through the questions.

686
01:45:49.260 --> 01:45:51.069
Julia - SUB INSTRUCTOR: So I'm going to create my for loop.

687
01:45:51.230 --> 01:45:54.959
Julia - SUB INSTRUCTOR: And and it sounds like, I'm going to be using these questions

688
01:45:54.990 --> 01:45:59.330
Julia - SUB INSTRUCTOR: as the thing that I'm iterating through. So for each question

689
01:46:00.508 --> 01:46:04.550
Julia - SUB INSTRUCTOR: and questions, whoops, questions, right?

690
01:46:05.260 --> 01:46:09.909
Julia - SUB INSTRUCTOR: What are we going to do? We're going to pass the question and text

691
01:46:10.060 --> 01:46:36.340
Julia - SUB INSTRUCTOR: to the initialized question, answer. So this is again, where like it was? It's because this is something that's going to be repetitive. It's helpful to build this into our function. So I can actually, you know, again, scooch up here copy this because that's essentially what we're trying to do. But we're putting in a function. So we don't have to rewrite this code every time we we wanna do a question and answer.

692
01:46:36.620 --> 01:46:44.829
Julia - SUB INSTRUCTOR: and then it says, retrieve the answer, or excuse me, retrieve the question, answer and the score.

693
01:46:45.200 --> 01:46:51.669
Julia - SUB INSTRUCTOR: And you know, essentially, I think it wants us to append it to this data

694
01:46:51.770 --> 01:46:57.039
Julia - SUB INSTRUCTOR: list, this list that we labeled as data. So I'm going to take my data

695
01:46:57.210 --> 01:46:58.230
Julia - SUB INSTRUCTOR: list

696
01:46:59.020 --> 01:47:01.119
Julia - SUB INSTRUCTOR: because it's a list I'm going to append.

697
01:47:02.294 --> 01:47:05.290
Julia - SUB INSTRUCTOR: Whatever the question to answer and whatnots are.

698
01:47:05.370 --> 01:47:11.540
Julia - SUB INSTRUCTOR: And so what is it? Question? Okay? So I'm gonna append the question. Question

699
01:47:12.170 --> 01:47:19.630
Julia - SUB INSTRUCTOR: right? The answer and the answer, right is going to be in the result.

700
01:47:20.450 --> 01:47:29.290
Julia - SUB INSTRUCTOR: because the question, so it's saying, hey, retrieve the question, and the question is, what the current iteration of this for loop is on right.

701
01:47:29.350 --> 01:47:38.223
Julia - SUB INSTRUCTOR: And then the second thing is the answer which is going to be stored in here, but if you remember we when we looked at it before the

702
01:47:38.790 --> 01:47:39.756
Julia - SUB INSTRUCTOR: you know

703
01:47:40.600 --> 01:48:05.040
Julia - SUB INSTRUCTOR: they let me scroll up. So you can actually see the answer is actually here. Right? So we can actually say, Hey, in order to get the actual answer, we're gonna have to use a reference to this key, right? And so what we can then do and say, hey? In my result, I'm gonna reference, the answer to get the answer and store in that list. So

704
01:48:05.200 --> 01:48:09.150
Julia - SUB INSTRUCTOR: result and using the answer

705
01:48:09.160 --> 01:48:18.619
Julia - SUB INSTRUCTOR: key right to get that value. And then what else do we need the score. So again, same thing. When we looked at the result

706
01:48:19.110 --> 01:48:30.549
Julia - SUB INSTRUCTOR: the key in order to access the score is call score. Right? So I'm saying, Okay, great in my results set. I'm gonna use that score

707
01:48:30.984 --> 01:48:38.059
Julia - SUB INSTRUCTOR: key in order to get the value for that score. What else? Oh, the starting and ending of where the answers, okay, so

708
01:48:38.380 --> 01:48:44.350
Julia - SUB INSTRUCTOR: was it called, start and end. Okay, so same thing. So I'm just looking to see what are the the key

709
01:48:45.330 --> 01:48:47.220
Julia - SUB INSTRUCTOR: keys of these

710
01:48:48.615 --> 01:48:49.299
Julia - SUB INSTRUCTOR: Elements

711
01:48:49.420 --> 01:48:50.969
Julia - SUB INSTRUCTOR: in my result

712
01:48:51.140 --> 01:48:54.979
Julia - SUB INSTRUCTOR: dictionary. And then I'm going to add them here.

713
01:48:56.003 --> 01:48:57.269
Julia - SUB INSTRUCTOR: Yes, okay.

714
01:48:57.450 --> 01:49:01.960
Julia - SUB INSTRUCTOR: So I've got my result. What was the next one starting? Okay, so start.

715
01:49:02.850 --> 01:49:05.249
Julia - SUB INSTRUCTOR: And then I'm going to do results.

716
01:49:06.050 --> 01:49:06.990
Julia - SUB INSTRUCTOR: And

717
01:49:07.220 --> 01:49:16.190
Julia - SUB INSTRUCTOR: and is there anything else? No, I think that's it. So we got answer, score restart result and done, okay, great.

718
01:49:16.330 --> 01:49:17.680
Julia - SUB INSTRUCTOR: And so

719
01:49:17.800 --> 01:49:24.819
Julia - SUB INSTRUCTOR: now, we're pretty much done. But then it says, create a data frame from the data with appropriate column. So we can just use our

720
01:49:25.629 --> 01:49:31.910
Julia - SUB INSTRUCTOR: data frame constructor. So I'm gonna create a data frame. I'm gonna use my data frame constructor

721
01:49:32.530 --> 01:49:42.850
Julia - SUB INSTRUCTOR: and then pass in my data right? Because at this point it has iterated through all the questions gathered all those little bits of pieces, put them in my new data list.

722
01:49:43.080 --> 01:49:52.570
Julia - SUB INSTRUCTOR: And then we probably wanna name the column. So we know it's coming out. And also I see down here there's a nicely formatted example of what should be coming out of this thing.

723
01:49:52.640 --> 01:49:58.850
Julia - SUB INSTRUCTOR: So I'm gonna say, columns is equal to question.

724
01:49:59.360 --> 01:50:02.610
Julia - SUB INSTRUCTOR: What was the next one score? No answer.

725
01:50:02.730 --> 01:50:04.980
Julia - SUB INSTRUCTOR: Question, answer whoops, answer.

726
01:50:05.690 --> 01:50:09.520
Julia - SUB INSTRUCTOR: score, start

727
01:50:10.060 --> 01:50:16.569
Julia - SUB INSTRUCTOR: and end. So I'm just kind of giving the columns their names, and then we're gonna return the date frame

728
01:50:16.880 --> 01:50:18.430
Julia - SUB INSTRUCTOR: return. Df.

729
01:50:18.690 --> 01:50:20.850
Julia - SUB INSTRUCTOR: alright. So I've got my function.

730
01:50:20.970 --> 01:50:24.090
Julia - SUB INSTRUCTOR: It's going to take questions and some text.

731
01:50:24.170 --> 01:50:36.120
Julia - SUB INSTRUCTOR: And then it's gonna iterate through those questions. Run them through our question, answer a model, get the results, put them in a data frame and return it. Okay, so let's load that function. It. Let me make sure

732
01:50:36.210 --> 01:50:44.070
Julia - SUB INSTRUCTOR: and then call the question, answer, function. So then, I can just go ahead and just say question.

733
01:50:44.150 --> 01:50:46.100
Julia - SUB INSTRUCTOR: answer.

734
01:50:46.930 --> 01:50:59.690
Julia - SUB INSTRUCTOR: I call answer, or what was called hands. Answer. It's called question, answer, function. We're going to pass in the questions and the tags, and we should get the same exact table below. So let's go ahead and run it.

735
01:51:00.610 --> 01:51:01.930
Julia - SUB INSTRUCTOR: And

736
01:51:02.090 --> 01:51:04.230
Julia - SUB INSTRUCTOR: it's gonna take a little while because it has to

737
01:51:04.610 --> 01:51:06.179
Julia - SUB INSTRUCTOR: actually kind of like.

738
01:51:06.270 --> 01:51:22.820
Julia - SUB INSTRUCTOR: think about it. But it actually did it right when we're transformers. First introduce 2017. What transformers, what? Our transformers better than Rn's. That's true cause for current neural nets. We're sort of like the initial approach. When you use nlp.

739
01:51:23.170 --> 01:51:28.100
Julia - SUB INSTRUCTOR: and what are applications of transformers? Translation, text summarization. There we go

740
01:51:29.400 --> 01:51:30.300
Julia - SUB INSTRUCTOR: thumbs up.

741
01:51:32.260 --> 01:51:33.730
Julia - SUB INSTRUCTOR: Awesome. Okay?

742
01:51:33.980 --> 01:51:42.709
Julia - SUB INSTRUCTOR: We have one more topic. So. We'll talk about text summarization first.

743
01:51:42.720 --> 01:51:59.699
Julia - SUB INSTRUCTOR: and then I will give you the option. Would you like to go back and do an activity on question and answer, would you like to do an activity on text summarization? Or would you like to call it an early evening because it's a Thursday and everyone gets tired as we move through the week, and

744
01:52:00.210 --> 01:52:01.040
Julia - SUB INSTRUCTOR: you know.

745
01:52:01.230 --> 01:52:14.380
Julia - SUB INSTRUCTOR: I think I forget what I wrote earlier in one of the comments, but you know as great as Thursday is Friday. That's the day we're all waiting for but anyways let's go through tech summarization.

746
01:52:14.520 --> 01:52:21.010
Julia - SUB INSTRUCTOR: and then we'll we'll have a poll and see what people want to do. Okay.

747
01:52:21.800 --> 01:52:34.379
Julia - SUB INSTRUCTOR: alright. So the last. This is the last kind of topic around transformer models, you know, and used cases for them, which is really just using these models to summarize bodies of text. Right?

748
01:52:38.100 --> 01:52:43.600
Julia - SUB INSTRUCTOR: oh, gosh, I guess. Okay, there's I missed the slide. So

749
01:52:43.690 --> 01:52:45.530
Julia - SUB INSTRUCTOR: let's go back into code.

750
01:52:47.712 --> 01:53:03.389
Julia - SUB INSTRUCTOR: Alright upload notebook. Look at the code example. So this is the last one. It's activity. 7 text summarization unsolved. There's a text summarization notebook file.

751
01:53:03.760 --> 01:53:10.370
Julia - SUB INSTRUCTOR: But all the concepts that you know we're in the slide we'll talk about as we go through this code together.

752
01:53:12.727 --> 01:53:14.302
Julia - SUB INSTRUCTOR: Alright. So

753
01:53:15.540 --> 01:53:37.761
Julia - SUB INSTRUCTOR: you know, by this point you should be pretty familiar with our Transformers library. We're importing that pipeline class so that we can actually utilize that instead of having to do those extra steps that we did in that first activity of the evening. And here we're gonna do a text summarization model again. It's just, you know, taking some text and

754
01:53:38.330 --> 01:53:39.129
Julia - SUB INSTRUCTOR: you know.

755
01:53:39.350 --> 01:53:43.630
Julia - SUB INSTRUCTOR: kind of creating an abstract around it right? A summary of it.

756
01:53:44.150 --> 01:53:56.219
Julia - SUB INSTRUCTOR: And what we're using here is a another model. It's a Facebook model. And it's called the Bart large Cnn model.

757
01:53:56.460 --> 01:53:58.630
Julia - SUB INSTRUCTOR: and Bart

758
01:53:58.700 --> 01:54:10.759
Julia - SUB INSTRUCTOR: actually stands for Bart is not the same as Bert. But Bart is stand stands for bi-directional auto regressive transformer and Bart. Models are

759
01:54:11.164 --> 01:54:18.750
Julia - SUB INSTRUCTOR: more similar to what we saw initially, which are encoder decoder models like the transformer architecture we saw earlier in the class

760
01:54:18.860 --> 01:54:23.280
Julia - SUB INSTRUCTOR: and part models. Let me run this so that we can get this loaded

761
01:54:23.772 --> 01:54:45.507
Julia - SUB INSTRUCTOR: Bart. Models are, you know, usually used for sequence to sequence tasks like summarization or translation. It can also be used for like text classification. You can use it for question and answering, if you want to. But you know these auto regressive models.

762
01:54:46.080 --> 01:55:01.810
Julia - SUB INSTRUCTOR: that's what the a stands for in in Bart. You know, are a similar concept to the Time Series models you might have seen earlier in the course and that the idea behind auto regressive models just in general is that

763
01:55:02.070 --> 01:55:13.615
Julia - SUB INSTRUCTOR: you're using prior data to predict future outcomes. Right? So you're looking at with time series, you're looking at prior prices or data points.

764
01:55:14.453 --> 01:55:33.340
Julia - SUB INSTRUCTOR: you know, within a period of time. But in this case with text, we're looking at prior tokens or prior text to predict future outcomes or predict what next? What's gonna come next in the end? Text, or what should be associated with that text?

765
01:55:33.785 --> 01:55:47.989
Julia - SUB INSTRUCTOR: And it's it's kind of done iteratively. So, we have a variable okay, great. And so it's all loaded and all set to go. We have an article here which is from Wikipedia and about deep learning.

766
01:55:48.190 --> 01:56:04.029
Julia - SUB INSTRUCTOR: And I'm gonna go ahead and run it. And we're gonna need to summarize this because it's quite a lengthy article, right? And so the way that it works is that you can actually take, you know, any any kind of text in this case it's a Wikipedia article

767
01:56:04.470 --> 01:56:18.369
Julia - SUB INSTRUCTOR: longer the better, because, you know, summarization is really helpful when you have a super long article. But you know it could be a technical article. It could be a, you know, a a funny like lifestyle article doesn't matter. But

768
01:56:18.370 --> 01:56:46.550
Julia - SUB INSTRUCTOR: what we're going to do is we're gonna take it and approach it in 2 ways. One is that we can come up with what is the most likely summary, and that's what we're gonna do first. But then we can also do what is called a diverse summary, which is, you know, just saying, Hey, just kinda you know, give me the gist of it. But you know, understanding that it may not be as accurate as the most likely summary. So we'll look at both examples here. Okay.

769
01:56:47.190 --> 01:57:06.819
Julia - SUB INSTRUCTOR: So up here, what we did was we took the pipeline class to create a model from Facebook, the Bart large Cnn model to summarize, and we stored it in the summarizer variable. So that's our, you know instance of our model.

770
01:57:06.950 --> 01:57:14.720
Julia - SUB INSTRUCTOR: And so, in order to you know, get the most likely summary, we can actually create a variable called most

771
01:57:14.870 --> 01:57:17.620
Julia - SUB INSTRUCTOR: likely summary.

772
01:57:17.940 --> 01:57:20.239
Julia - SUB INSTRUCTOR: and use our summarizer

773
01:57:20.440 --> 01:57:25.719
Julia - SUB INSTRUCTOR: instance right to pass in a few bits of information. This is where, like the

774
01:57:26.066 --> 01:57:32.500
Julia - SUB INSTRUCTOR: pop up is helpful. So we need to first pass it. The article that we're going to be using so article.

775
01:57:33.080 --> 01:57:49.657
Julia - SUB INSTRUCTOR: and then you can actually define things like, Oh, what's the minimum length? What's the Max length? And this is helpful for speed. And you know, accuracy. We kind of saw what happened when we played around with the Max length, then the last activity. So

776
01:57:50.030 --> 01:57:57.149
Julia - SUB INSTRUCTOR: no you could pick like I don't know. 30, 40, 50 I'm gonna pick 40

777
01:57:58.145 --> 01:58:06.080
Julia - SUB INSTRUCTOR: Max length again. You know. You can just pick anything within a range. I'm gonna pick. I don't know 150

778
01:58:06.601 --> 01:58:11.528
Julia - SUB INSTRUCTOR: and then the last bit is this is is this comment? Here it says,

779
01:58:12.090 --> 01:58:22.940
Julia - SUB INSTRUCTOR: you know, get the most likely summary of the article using false for the do sample parameter. So we're going to add this fourth parameter called do sample. We're going to set that to false.

780
01:58:23.510 --> 01:58:43.449
Julia - SUB INSTRUCTOR: And what that's gonna do is that it's gonna that's the flag that's saying, hey, get the most likely summary. If we do do sample true, then we'll get sort of I diverse summary. So that's what's controlling, whether you're getting the most likely summary, or you're getting sort of like, you know, whatever comes.

781
01:58:43.500 --> 01:58:48.880
Julia - SUB INSTRUCTOR: So I'm gonna go ahead and print this out. Most likely Summary.

782
01:58:49.400 --> 01:58:51.340
Julia - SUB INSTRUCTOR: and I'll go ahead and run it.

783
01:58:51.530 --> 01:58:54.780
Julia - SUB INSTRUCTOR: And hopefully it'll do a really great job.

784
01:58:55.350 --> 01:59:06.350
Julia - SUB INSTRUCTOR: But one thing to note is that this particular model, the Bart model that we're using can only support a certain number of tokens. So it maxes out at like.

785
01:59:06.640 --> 01:59:21.819
Julia - SUB INSTRUCTOR: I can't remember what number it is, but it maxes out at a certain number of tokens. It's it's large. But you know, obviously input tokens but you know you do have to kind of take that into consideration. So that's also why. We'll kind of.

786
01:59:21.880 --> 01:59:34.499
Julia - SUB INSTRUCTOR: you know as much as like you can add any article you want or you know, summarize any text body of text you're interested in depending on the model. Some models may not be able to handle a lot of text.

787
01:59:34.881 --> 01:59:38.949
Julia - SUB INSTRUCTOR: But you know, for our purposes this is totally fine. Alright. So we came back.

788
01:59:39.430 --> 01:59:49.809
Julia - SUB INSTRUCTOR: And so, you know, you can see this is again quite a lengthy article. I'm scrolling back and forth, and it did a great job of summarizing it into, you know. Essentially, you know.

789
01:59:50.170 --> 01:59:57.310
Julia - SUB INSTRUCTOR: like 3 sentences. Right? Deep learning is a part of a broader family of machine learning methods based on artificial neural nets.

790
01:59:57.690 --> 02:00:08.219
Julia - SUB INSTRUCTOR: So great. Great job, the one thing. You'll notice that again. It came back in Json format. So if we wanted to just get the text, we would have to again

791
02:00:08.660 --> 02:00:22.769
Julia - SUB INSTRUCTOR: use that approach of using that index 0 to say, Hey, I want to get the value for the key summary text in order to just get so if I do that here I can say, what did I call? Most likely

792
02:00:24.350 --> 02:00:33.020
Julia - SUB INSTRUCTOR: likely summary. So again, it's using that index 0. And then saying, What key am I looking for summary text.

793
02:00:33.390 --> 02:00:46.499
Julia - SUB INSTRUCTOR: And then if I run that, it's just gonna bring back the text here. So that's just one little thing that you always have to kind of remember with these models that they're they're probably gonna come back in a a format that's not necessarily very human readable.

794
02:00:46.580 --> 02:00:52.300
Julia - SUB INSTRUCTOR: And then, if I wanted to show you what it looked like if I were to change this to

795
02:00:53.900 --> 02:00:56.860
Julia - SUB INSTRUCTOR: diverse summary. So this is this is

796
02:00:57.360 --> 02:01:00.719
Julia - SUB INSTRUCTOR: diverse summary, diverse

797
02:01:01.340 --> 02:01:02.770
Julia - SUB INSTRUCTOR: summary.

798
02:01:02.830 --> 02:01:12.709
Julia - SUB INSTRUCTOR: And I'm gonna change this parameter to true instead of false. So this is, gonna give me a little bit more back. I'm gonna print that out

799
02:01:13.720 --> 02:01:16.050
Julia - SUB INSTRUCTOR: and let's see what comes back.

800
02:01:17.850 --> 02:01:18.710
Julia - SUB INSTRUCTOR: And

801
02:01:19.780 --> 02:01:23.000
Julia - SUB INSTRUCTOR: you know, this is, gonna take a little bit longer, because

802
02:01:24.570 --> 02:01:26.779
Julia - SUB INSTRUCTOR: it's not as efficient.

803
02:01:27.160 --> 02:01:29.920
Julia - SUB INSTRUCTOR: but hopefully you'll come back soon

804
02:01:31.040 --> 02:01:32.110
Julia - SUB INSTRUCTOR: pounds.

805
02:01:32.300 --> 02:01:36.899
Julia - SUB INSTRUCTOR: while it's thinking, let's take a poll.

806
02:01:38.540 --> 02:01:42.710
Julia - SUB INSTRUCTOR: How many of you want to do the activity around?

807
02:01:43.527 --> 02:01:45.200
Julia - SUB INSTRUCTOR: Question and answering

808
02:01:46.810 --> 02:01:47.880
Julia - SUB INSTRUCTOR: anybody?

809
02:01:50.120 --> 02:01:55.249
Julia - SUB INSTRUCTOR: Everyone's like, no? Oh, okay. Oh, I see some hands. I see a couple of head knots. Okay.

810
02:01:57.730 --> 02:01:59.891
Julia - SUB INSTRUCTOR: maybe I should put this. I have

811
02:02:00.350 --> 02:02:03.849
Julia - SUB INSTRUCTOR: You guys in this very limited

812
02:02:05.310 --> 02:02:12.349
Julia - SUB INSTRUCTOR: thumbnail. Real view. Okay. Now, I have to put you in this gallery view so I can count all the hands alright.

813
02:02:12.380 --> 02:02:16.259
Julia - SUB INSTRUCTOR: How many of you want to do perfect. Thank you.

814
02:02:16.703 --> 02:02:21.030
Julia - SUB INSTRUCTOR: How many of you want to do the text summarization activity

815
02:02:23.480 --> 02:02:29.250
Julia - SUB INSTRUCTOR: and was like, no, it sucks. I want just the QA. Activity that sounds so much cooler.

816
02:02:30.510 --> 02:02:31.540
Julia - SUB INSTRUCTOR: Alright.

817
02:02:31.839 --> 02:02:34.630
Julia - SUB INSTRUCTOR: How many of you want to call in an evening?

818
02:02:35.790 --> 02:02:39.179
Julia - SUB INSTRUCTOR: Oh, oh, oh, wow! Okay!

819
02:02:39.250 --> 02:02:42.040
Julia - SUB INSTRUCTOR: And we got a virtual hand raise here.

820
02:02:42.544 --> 02:02:51.610
Julia - SUB INSTRUCTOR: Okay, because we're democracy here. I don't know if it's like this when your regular instructors here so apologies if I

821
02:02:52.110 --> 02:03:07.079
Julia - SUB INSTRUCTOR: suddenly introduced democracy, and that's not how it was before. But it looks like the let's end A little more than 10 min early. One, the majority vote here.

822
02:03:07.995 --> 02:03:23.549
Julia - SUB INSTRUCTOR: But oh, good! It came back. So you can see with the diverse example here, it's you know it. It's similar right in terms, is it? Unfortunately, because I made the zoom window bigger. I now can't see. Okay, there we go.

823
02:03:24.585 --> 02:03:31.639
Julia - SUB INSTRUCTOR: Blah blah blah semi supervised or super deep learning. Actually, I feel like they look very.

824
02:03:31.640 --> 02:03:33.710
Meredith McCanse (she/her): I think they're exactly the same.

825
02:03:33.710 --> 02:03:35.412
Julia - SUB INSTRUCTOR: Yeah, I'm like trying. Oh, no.

826
02:03:35.980 --> 02:03:36.570
Dipinto, Matt: I have a favorite.

827
02:03:36.570 --> 02:03:37.540
Julia - SUB INSTRUCTOR: Climate site.

828
02:03:37.540 --> 02:03:46.489
Dipinto, Matt: At the beginning. And so it's part of a broader family, or is a family of machine learning? There's like almost exactly the same very subtle differences.

829
02:03:46.490 --> 02:03:50.159
Julia - SUB INSTRUCTOR: And then an a nice reference to climate science

830
02:03:50.450 --> 02:04:14.762
Julia - SUB INSTRUCTOR: here. That wasn't in the other original one. So I guess. Maybe if this was a Dei situation they might have to rethink the diversity and I their diverse summary. But you know you kind of get the gist of it. There, there's different options for you. And then also this was, if you think about the the text that we're looking at.

831
02:04:15.494 --> 02:04:29.345
Julia - SUB INSTRUCTOR: it. It was fairly uniform, and then about a single topic, and then also not not that long. So that contributes to lack of diversity. But anyways you get you get the idea.

832
02:04:30.430 --> 02:04:53.909
Julia - SUB INSTRUCTOR: alright so last couple of things I there's a copy of the slides in get lab for you. There's a series of questions. If you wanted to kind of review some of the topics that we talked about more in the in the bottom part of the slides. So if you feel like, Hey, I wanna kind of review this idea of transformers a little bit before a little bit more before Monday's class.

833
02:04:53.910 --> 02:05:04.069
Julia - SUB INSTRUCTOR: Please take a look there. The act. 2 activities that we kind of skip tonight obviously are in. Get lab. So if you want to do them in your own time over the weekend.

834
02:05:04.070 --> 02:05:14.730
Julia - SUB INSTRUCTOR: maybe watch a little bit of Netflix, and do some coding, go for it. And then of course. I believe your homework assignment is coming up

835
02:05:14.840 --> 02:05:24.209
Julia - SUB INSTRUCTOR: are going to be is going to be due soon, so if you need any help. Of course there's always ask Bcs. And

836
02:05:24.720 --> 02:05:29.259
Julia - SUB INSTRUCTOR: well, it's a little bit late in the the course, but if you really need a tutor you could

837
02:05:29.627 --> 02:05:47.309
Julia - SUB INSTRUCTOR: get a tutor to help you finish up one of the last homework, and I think, if I remember correctly, I don't think there's a homework assignment with, I think the homework assignment with 22 is optional, or maybe nonexistent. So essentially, this is kinda your last homework assignment. So hooray and Craig congratulations.

838
02:05:48.090 --> 02:05:48.860
Julia - SUB INSTRUCTOR: So

839
02:05:49.010 --> 02:05:56.920
Julia - SUB INSTRUCTOR: what is it for? Class? I hope you learn something. You're probably like, thank God, Anthony is coming back.

840
02:05:56.940 --> 02:05:58.710
Julia - SUB INSTRUCTOR: you know, on Monday.

841
02:05:59.189 --> 02:06:03.580
Julia - SUB INSTRUCTOR: But hope you all have a fantastic weekend and

842
02:06:04.260 --> 02:06:17.640
Julia - SUB INSTRUCTOR: again, I'll throw those links on the a Luther AI site, and then the aws it's like a recordings of their last Gen. AI. Conference. If you want to take a look, there.

843
02:06:18.830 --> 02:06:21.075
Baro, Sonja: Julia, you are awesome. Thank you.

844
02:06:22.130 --> 02:06:23.369
Meredith McCanse (she/her): Yeah, thank, you.

845
02:06:23.370 --> 02:06:46.625
Julia - SUB INSTRUCTOR: It's a wild ride today. There was just a lot going on. But yeah, no, I had a lot of fun. These topics are so much fun. And again so relevant. I mean, you could literally go to a dinner party this weekend be like, yeah, I know, I know how chat? Tpt works. Yeah, like transformers, you know, encoding, decoding, you know. Self attention. It's all there, you know.

846
02:06:47.873 --> 02:06:50.806
Julia - SUB INSTRUCTOR: But yeah, I hope you all have a great night.

847
02:06:52.090 --> 02:06:56.109
Meredith McCanse (she/her): Are you and Julie? Are you? Are you still here for office hours today?

848
02:06:56.110 --> 02:07:10.260
Julia - SUB INSTRUCTOR: Yes, yes, oh, so sorry. Yes, if you have any issues, absolutely. We are here for office hours. I'm just gonna pause the recording. But for those of you, you know, if you're feeling like you're done for the evening, I hope you have a fantastic

849
02:07:10.270 --> 02:07:11.610
Julia - SUB INSTRUCTOR: rest of your night.


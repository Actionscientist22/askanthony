WEBVTT

1
00:00:03.425 --> 00:00:04.100
Anthony Taylor: Boom!

2
00:00:05.380 --> 00:00:06.580
Anthony Taylor: Today

3
00:00:06.760 --> 00:00:13.650
Anthony Taylor: is a big day. Every day is a big day till the end of class. Okay? But today is the big day.

4
00:00:16.950 --> 00:00:24.610
Anthony Taylor: we're going to really dig in to Ll. Ms for real today. We're going to be using hugging face

5
00:00:25.535 --> 00:00:30.000
Anthony Taylor: before the end of the week. You're gonna be using significant

6
00:00:30.060 --> 00:00:31.470
Anthony Taylor: models

7
00:00:32.360 --> 00:00:36.405
Anthony Taylor: that have been created in the last year or 2

8
00:00:37.090 --> 00:00:39.219
Anthony Taylor: as opposed to 20 years ago.

9
00:00:39.990 --> 00:00:42.949
Anthony Taylor: Okay? Or 30 or 40, some cases.

10
00:00:44.820 --> 00:00:45.760
Anthony Taylor: so yeah.

11
00:00:46.030 --> 00:00:48.549
Anthony Taylor: everything you have done

12
00:00:49.260 --> 00:00:50.930
Anthony Taylor: has led us to here.

13
00:00:51.970 --> 00:00:54.100
Anthony Taylor: Okay, this next 2 weeks.

14
00:00:54.510 --> 00:01:00.179
Anthony Taylor: This is what like, it's like, I told you, when you guys signed up for an AI course

15
00:01:00.430 --> 00:01:02.000
Anthony Taylor: right after

16
00:01:02.550 --> 00:01:06.839
Anthony Taylor: chat, Gpt became a thing or right as Chat Gpt became a thing

17
00:01:07.040 --> 00:01:09.980
Anthony Taylor: right. This was probably what you were thinking

18
00:01:10.320 --> 00:01:13.619
Anthony Taylor: and realize you'd have to do a whole bunch of work before you got

19
00:01:14.350 --> 00:01:15.600
Anthony Taylor: okay. But

20
00:01:15.780 --> 00:01:16.770
Anthony Taylor: here we are.

21
00:01:17.010 --> 00:01:17.960
Anthony Taylor: So

22
00:01:18.620 --> 00:01:23.010
Anthony Taylor: let's get to. We got a lot to cover. I gotta tell you guys that it's sorry for the

23
00:01:24.030 --> 00:01:29.889
Anthony Taylor: or Thurs. No Thursday whatever. What day today's Wednesday. Yeah. So yeah. The sub tomorrow.

24
00:01:30.250 --> 00:01:32.889
Anthony Taylor: Whoop! They got a laptop. Hopefully, you get a good one.

25
00:01:33.770 --> 00:01:34.830
Anthony Taylor: If not.

26
00:01:35.350 --> 00:01:41.049
Anthony Taylor: we'll we'll figure it out. Okay. So we're gonna talk about. We're gonna have a lot of like

27
00:01:41.240 --> 00:01:43.049
Anthony Taylor: this is how we got here.

28
00:01:43.671 --> 00:01:47.319
Anthony Taylor: I I kind of showed you a little bit of that

29
00:01:47.990 --> 00:01:50.090
Anthony Taylor: yesterday.

30
00:01:51.410 --> 00:01:53.720
Anthony Taylor: So pretty exciting stuff.

31
00:01:55.250 --> 00:01:56.699
Anthony Taylor: Monday. Sorry.

32
00:01:57.010 --> 00:02:03.700
Anthony Taylor: So we're going to do that. We're going to talk about converting words and raised to vectors. Explain how large language models work.

33
00:02:03.850 --> 00:02:07.010
Anthony Taylor: describe how tokenizers. Process sentences

34
00:02:07.300 --> 00:02:13.169
Anthony Taylor: explain the benefits of hugging face Tokenizer. Maybe I not heard of hugging face.

35
00:02:14.130 --> 00:02:15.240
Anthony Taylor: Ho!

36
00:02:15.240 --> 00:02:20.190
Clayton Graves: Those are those things that hatch out of the eggs and and plant aliens in you right.

37
00:02:20.980 --> 00:02:21.840
Anthony Taylor: No, sir.

38
00:02:22.870 --> 00:02:23.510
Anthony Taylor: no.

39
00:02:24.511 --> 00:02:28.440
Anthony Taylor: Hugging face is basically like a github for

40
00:02:28.450 --> 00:02:32.749
Anthony Taylor: large language models and tools associated with them.

41
00:02:33.857 --> 00:02:39.579
Anthony Taylor: You're gonna convert sentences into tokens explains similarity. We're actually gonna be able to do similarity

42
00:02:39.650 --> 00:02:41.569
Anthony Taylor: stuff today, which is cool.

43
00:02:41.630 --> 00:02:44.759
Anthony Taylor: And we're we're gonna do it alright. Moving on

44
00:02:45.360 --> 00:02:47.659
Anthony Taylor: a brief history of, you know.

45
00:02:48.740 --> 00:02:50.170
Anthony Taylor: in the beginning.

46
00:02:50.370 --> 00:02:54.700
Anthony Taylor: 1950, S. Alan Turing introduced the imitation game.

47
00:02:54.760 --> 00:02:57.090
Anthony Taylor: Basically, this was a test

48
00:02:57.380 --> 00:03:02.159
Anthony Taylor: or a computer to pass to see if it was like a human.

49
00:03:02.900 --> 00:03:03.850
Anthony Taylor: pretty much

50
00:03:04.040 --> 00:03:04.950
Anthony Taylor: good.

51
00:03:06.320 --> 00:03:07.470
Anthony Taylor: Eliza

52
00:03:08.820 --> 00:03:12.560
Anthony Taylor: was created in 1960 at Mit.

53
00:03:12.890 --> 00:03:16.390
Anthony Taylor: It was actually released 1966,

54
00:03:16.740 --> 00:03:22.140
Anthony Taylor: okay. And it was an early example of string test that used pattern matching and substitution

55
00:03:22.727 --> 00:03:27.169
Anthony Taylor: was able to make users feel as though they were conversing with the therapist.

56
00:03:27.390 --> 00:03:34.539
Anthony Taylor: Okay, it was basically rule-based. What's rule-based mean? If I ask you a question, or if I ask the computer.

57
00:03:34.650 --> 00:03:41.280
Anthony Taylor: what is an apple? Basically, there was a rule in there that said an apple is a fruit.

58
00:03:42.750 --> 00:03:44.400
Anthony Taylor: and that's the answer you got.

59
00:03:45.090 --> 00:03:48.720
Anthony Taylor: Okay. It didn't generate anything. It was simply

60
00:03:48.920 --> 00:03:54.010
Anthony Taylor: billions of rules or millions, probably maybe thousands back. Then.

61
00:03:54.799 --> 00:04:00.199
Anthony Taylor: That just add predetermined answers, my favorite thing on this.

62
00:04:02.320 --> 00:04:07.410
Anthony Taylor: you know what most of you probably too young. Anybody remember text based video games.

63
00:04:09.640 --> 00:04:10.889
Anthony Taylor: couple of you.

64
00:04:10.920 --> 00:04:12.399
Anthony Taylor: where? Where? It would say.

65
00:04:12.400 --> 00:04:14.030
Mason, Natalie: Oregon, Trail.

66
00:04:14.340 --> 00:04:18.065
Anthony Taylor: Like Oregon Trail, except I mean, even before that.

67
00:04:18.660 --> 00:04:20.379
Anthony Taylor: but like it would say, like.

68
00:04:20.480 --> 00:04:25.959
Anthony Taylor: you're a a a night walking through the you know through the forest in.

69
00:04:25.960 --> 00:04:27.009
Baro, Sonja: Pick left, or right.

70
00:04:27.380 --> 00:04:28.490
Anthony Taylor: Across the fort?

71
00:04:28.570 --> 00:04:29.970
Anthony Taylor: Yeah, right? Which?

72
00:04:29.970 --> 00:04:31.110
Baro, Sonja: Yeah, right? Okay.

73
00:04:31.710 --> 00:04:36.779
Anthony Taylor: There was no, I mean, this was just the whole story. All of the branches story were just written out.

74
00:04:36.890 --> 00:04:41.640
Anthony Taylor: and whatever you picked it. Just read you the branch. Well, that's a rule based chaplain.

75
00:04:41.640 --> 00:04:42.650
Clayton Graves: Zorch.

76
00:04:43.540 --> 00:04:45.070
Anthony Taylor: Work. Yeah, that was a good.

77
00:04:45.450 --> 00:04:47.580
Baro, Sonja: I'm the good old TRS. 80.

78
00:04:48.695 --> 00:04:49.100
Baro, Sonja: So.

79
00:04:49.100 --> 00:04:59.229
Anthony Taylor: So 1,966, Eliza, that made new seventies. Pretty much people did the same thing. They just kept doing rule based. So look at this jump, guys

80
00:04:59.650 --> 00:05:01.649
Anthony Taylor: 70 s. To the 90

81
00:05:01.730 --> 00:05:05.499
Anthony Taylor: so pretty much throughout the 70 s. And the 80 s.

82
00:05:05.570 --> 00:05:09.640
Anthony Taylor: No one was doing anything but dancing to really bad music.

83
00:05:10.430 --> 00:05:13.179
Anthony Taylor: Okay? And just, you know, Disco.

84
00:05:14.880 --> 00:05:21.800
Anthony Taylor: counting. When when is another, the seagulls block of seagulls. Yeah, radio stack.

85
00:05:21.860 --> 00:05:23.999
Anthony Taylor: I have pricing.

86
00:05:25.310 --> 00:05:28.080
Anthony Taylor: so so yeah, okay.

87
00:05:29.726 --> 00:05:31.550
Anthony Taylor: in the 90 s.

88
00:05:31.720 --> 00:05:38.079
Anthony Taylor: now, we started doing statistical. Nlp, so this is here. This is where we start doing

89
00:05:38.090 --> 00:05:44.309
Anthony Taylor: like, I don't know if Radiant boost was in place at this point, but the different models that we did the other day.

90
00:05:45.220 --> 00:05:46.320
Anthony Taylor: Okay.

91
00:05:47.438 --> 00:05:51.829
Anthony Taylor: you know the the Svc. With with with

92
00:05:52.170 --> 00:05:54.509
Anthony Taylor: tokenized data and stuff like that.

93
00:05:54.680 --> 00:05:57.850
Anthony Taylor: Alright, it. It's still

94
00:05:58.010 --> 00:06:08.010
Anthony Taylor: it. It allowed us to get away from the simple, rule-based stuff and start looking mathematically at. How can we associate words to phrases?

95
00:06:08.688 --> 00:06:13.480
Anthony Taylor: The tf, idf. With a big enough deal that in 1,998 it made the news

96
00:06:13.730 --> 00:06:16.630
Anthony Taylor: okay, because it basically

97
00:06:16.810 --> 00:06:25.030
Anthony Taylor: gave us the ability, statistical ability to deal with a bag of words. You're going to hear bag of words, a lot bag of words is really

98
00:06:25.530 --> 00:06:27.489
Anthony Taylor: the meat of n lp.

99
00:06:28.080 --> 00:06:39.610
Anthony Taylor: okay, that is, the center is the is the ability to take all the words do counts come up with the importance of those words. Now the bag of words we did before was pretty much just to count.

100
00:06:40.680 --> 00:06:47.329
Anthony Taylor: Okay. Tf, idf gave us a little bit more statistical understanding, and said, Hey, well, this words

101
00:06:47.480 --> 00:06:51.200
Anthony Taylor: there a lot. But it's not very important. Or this word super important.

102
00:06:51.640 --> 00:06:52.500
Anthony Taylor: Okay,

103
00:06:53.580 --> 00:06:58.420
Anthony Taylor: So so this was a big deal. And this didn't come around really till 90 98.

104
00:06:58.900 --> 00:07:03.539
Anthony Taylor: Okay, I've been out of high school for 13 years at this point.

105
00:07:05.030 --> 00:07:06.350
Anthony Taylor: That's crazy.

106
00:07:06.350 --> 00:07:10.520
Mason, Natalie: I had been on Aol since 1992.

107
00:07:11.550 --> 00:07:12.780
Anthony Taylor: No way.

108
00:07:12.780 --> 00:07:13.410
Mason, Natalie: Yeah.

109
00:07:13.410 --> 00:07:14.280
Anthony Taylor: 5.

110
00:07:14.650 --> 00:07:15.999
Mason, Natalie: Yeah, I was little like

111
00:07:17.040 --> 00:07:21.290
Mason, Natalie: elementary. I had a brother that was in gave us a computer. Never mind.

112
00:07:21.410 --> 00:07:23.030
Mason, Natalie: Wow, okay.

113
00:07:24.044 --> 00:07:24.750
Baro, Sonja: Hi.

114
00:07:24.750 --> 00:07:26.670
Anthony Taylor: Think I'm a Yay.

115
00:07:26.670 --> 00:07:28.500
Baro, Sonja: You're breathing, baby.

116
00:07:28.500 --> 00:07:29.970
Anthony Taylor: 1,200 bored

117
00:07:30.670 --> 00:07:38.680
Anthony Taylor: other statistical methods. Okay? So around this time, we also started getting the Ingrams Markov model. So we're starting to get better.

118
00:07:39.640 --> 00:07:44.299
Anthony Taylor: Okay. And this is where we first kind of started getting into

119
00:07:45.650 --> 00:07:48.560
Anthony Taylor: like like word prediction.

120
00:07:49.720 --> 00:07:54.039
Anthony Taylor: But and it was all word by word, by word. It was not

121
00:07:54.050 --> 00:07:55.809
Anthony Taylor: contextual at all.

122
00:07:55.850 --> 00:08:01.909
Anthony Taylor: It was like, well, if there's 4 words that sound negative, we can say, this is the negative thing.

123
00:08:02.680 --> 00:08:08.119
Anthony Taylor: Okay? Which is why, you can say this thing was freaking bad, dude?

124
00:08:08.350 --> 00:08:11.860
Anthony Taylor: And you would always get sentiment is negative.

125
00:08:13.420 --> 00:08:16.469
Anthony Taylor: because that just as far as it was concerned, you said bad.

126
00:08:17.090 --> 00:08:18.720
Anthony Taylor: and you use profanity.

127
00:08:18.960 --> 00:08:20.440
Anthony Taylor: So it must have been bad.

128
00:08:20.900 --> 00:08:21.860
Anthony Taylor: Okay.

129
00:08:22.338 --> 00:08:27.580
Anthony Taylor: again, we're in the 90 S. Still, you know, we you know, we've made a lot of progress in the 90 s.

130
00:08:27.650 --> 00:08:31.289
Anthony Taylor: But again, it was all statistical. It was all

131
00:08:31.500 --> 00:08:33.119
Anthony Taylor: more or less

132
00:08:34.419 --> 00:08:37.289
Anthony Taylor: just using what we did the other day.

133
00:08:37.940 --> 00:08:44.640
Anthony Taylor: Okay? So we tokenize, we vectorize, it's statistical methods. And we could come up with

134
00:08:44.700 --> 00:08:50.080
Anthony Taylor: predicting words doing sentiment analysis, which was incredible.

135
00:08:50.510 --> 00:08:51.630
Anthony Taylor: Incredible.

136
00:08:52.630 --> 00:08:53.410
Anthony Taylor: Okay?

137
00:08:54.140 --> 00:08:56.830
Anthony Taylor: And then long comes the 2,000.

138
00:08:57.150 --> 00:09:01.330
Anthony Taylor: So in the 2 thousands, this is where we picked up R and ends.

139
00:09:01.750 --> 00:09:03.300
Anthony Taylor: Arenas were

140
00:09:03.560 --> 00:09:09.280
Anthony Taylor: a huge leap ahead, because what was what made our and in so amazing.

141
00:09:10.980 --> 00:09:13.450
Anthony Taylor: And I remember the one important thing.

142
00:09:13.700 --> 00:09:14.630
Baro, Sonja: Memory.

143
00:09:15.170 --> 00:09:16.260
Anthony Taylor: Memory.

144
00:09:16.580 --> 00:09:22.129
Anthony Taylor: They have the ability to carry multiple words and thus gain some

145
00:09:22.210 --> 00:09:24.490
Anthony Taylor: capability of context

146
00:09:24.840 --> 00:09:27.829
Anthony Taylor: right? And this is where we started being able to

147
00:09:29.520 --> 00:09:32.460
Anthony Taylor: predict like the next word in a sin.

148
00:09:33.190 --> 00:09:35.420
Anthony Taylor: And this was text completion.

149
00:09:36.680 --> 00:09:38.160
Anthony Taylor: This was a big deal.

150
00:09:39.300 --> 00:09:45.889
Anthony Taylor: all right. And so that came around, we were like, yeah, problem was, Oh, my God, was it hard to do

151
00:09:46.050 --> 00:09:53.790
Anthony Taylor: as far as computational? All of that kind of no one really looks at the code, by the way, and says, Oh, that code's complicated. No one cares.

152
00:09:54.270 --> 00:09:57.169
Anthony Taylor: Okay. But it took a lot of power

153
00:09:57.290 --> 00:09:58.149
Anthony Taylor: to do it.

154
00:10:00.330 --> 00:10:01.020
Anthony Taylor: yeah.

155
00:10:01.220 --> 00:10:08.249
Anthony Taylor: okay. And then that whole vanishing gradient thing. It really didn't have much memory. So it was really only good with short sentences.

156
00:10:09.780 --> 00:10:10.710
Anthony Taylor: Okay.

157
00:10:10.870 --> 00:10:17.040
Anthony Taylor: now, Lstm came around and we're like yay, because we could remember more.

158
00:10:18.290 --> 00:10:20.320
Anthony Taylor: But still it wasn't like

159
00:10:20.350 --> 00:10:21.480
Anthony Taylor: funds.

160
00:10:23.640 --> 00:10:25.479
Anthony Taylor: you know. So they they were.

161
00:10:25.680 --> 00:10:34.669
Anthony Taylor: you know, that weren't suited for classification, not really for prediction. And if the data wasn't in sequence, it struggled with that.

162
00:10:35.740 --> 00:10:36.750
Anthony Taylor: Okay.

163
00:10:36.790 --> 00:10:39.089
Anthony Taylor: so this is where we left off

164
00:10:39.580 --> 00:10:40.500
Anthony Taylor: on Monday.

165
00:10:41.820 --> 00:10:44.090
Anthony Taylor: right here, 2,007

166
00:10:46.190 --> 00:10:58.919
Anthony Taylor: in the 2,000. 12,014. We started talking about attention, and there was an article that came out there. It is in 2,017, called attention is all you need.

167
00:11:00.730 --> 00:11:02.180
Anthony Taylor: At this point

168
00:11:03.140 --> 00:11:05.799
Anthony Taylor: we are using transformers

169
00:11:06.150 --> 00:11:07.380
Anthony Taylor: and

170
00:11:07.810 --> 00:11:09.290
Anthony Taylor: transformers

171
00:11:09.460 --> 00:11:10.739
Anthony Taylor: gave us some

172
00:11:10.980 --> 00:11:13.150
Anthony Taylor: amazing capabilities.

173
00:11:13.360 --> 00:11:19.640
Anthony Taylor: They allowed us. And this is what we're going to be studying this week. Well, you know what. I'll wait till we get to that. I want to repeat it.

174
00:11:19.690 --> 00:11:22.840
Anthony Taylor: So since the paper transformers have become

175
00:11:22.930 --> 00:11:25.380
Anthony Taylor: the thing. So so let me be clear on this.

176
00:11:25.480 --> 00:11:27.259
Anthony Taylor: When he wrote this.

177
00:11:28.580 --> 00:11:29.590
Anthony Taylor: K.

178
00:11:29.880 --> 00:11:32.930
Anthony Taylor: Transformers were just kind of getting started.

179
00:11:33.250 --> 00:11:38.280
Anthony Taylor: It actually took a couple of years before people were really able

180
00:11:38.300 --> 00:11:40.129
Anthony Taylor: to do something with this.

181
00:11:41.130 --> 00:11:43.089
Anthony Taylor: but this paper

182
00:11:43.880 --> 00:11:49.160
Anthony Taylor: will go down in history as pretty much the paper that led us to

183
00:11:49.270 --> 00:11:52.740
Anthony Taylor: where we are today with Chat Gpt and all of these tools.

184
00:11:53.810 --> 00:11:54.820
Anthony Taylor: Okay?

185
00:11:55.980 --> 00:11:59.980
Anthony Taylor: Oh, well, it doesn't even go into. So, as we all know.

186
00:12:00.460 --> 00:12:08.850
Anthony Taylor: I don't have the exact date sometimes, but we can. We can talk about more recent history in 2,023 around September.

187
00:12:09.700 --> 00:12:11.240
Anthony Taylor: Openai.

188
00:12:12.240 --> 00:12:19.230
Anthony Taylor: And well, a lot of companies have these Llm. Models. Let me start with that before 2,023. Openai was not the first.

189
00:12:19.830 --> 00:12:25.300
Anthony Taylor: Okay, what they were was the first to make it accessible by Api.

190
00:12:26.060 --> 00:12:30.049
Anthony Taylor: thus giving you guys and myself

191
00:12:30.310 --> 00:12:31.440
Anthony Taylor: access

192
00:12:31.910 --> 00:12:38.090
Anthony Taylor: to utilize this Llm. Any way we want with within reason.

193
00:12:39.390 --> 00:12:41.319
Anthony Taylor: That is what

194
00:12:41.760 --> 00:12:43.170
Anthony Taylor: change the world

195
00:12:44.830 --> 00:12:46.160
Anthony Taylor: that day.

196
00:12:46.870 --> 00:12:56.199
Anthony Taylor: You know we talk about. You know there are. There will be in history, you know the day the Internet was, you know, made public or people blindly were able to get on the Internet.

197
00:12:56.610 --> 00:12:58.450
Anthony Taylor: Okay, Google.

198
00:12:58.710 --> 00:13:00.940
Anthony Taylor: big Day, change the world.

199
00:13:01.050 --> 00:13:04.279
Anthony Taylor: Facebook big day change the world.

200
00:13:05.270 --> 00:13:08.170
Anthony Taylor: Okay. Hadoop Spark

201
00:13:09.420 --> 00:13:11.849
Anthony Taylor: and Openai, opening that Api

202
00:13:12.130 --> 00:13:13.369
Anthony Taylor: change the world.

203
00:13:13.870 --> 00:13:16.330
Anthony Taylor: and if I was to be on it.

204
00:13:16.380 --> 00:13:20.159
Anthony Taylor: that one is probably in the top 3 of all the ones I just listed.

205
00:13:21.600 --> 00:13:25.659
Anthony Taylor: Okay, cause it's really, really changing the world.

206
00:13:28.390 --> 00:13:29.140
Anthony Taylor: yeah.

207
00:13:29.370 --> 00:13:43.509
Anthony Taylor: it's funny. I'm gonna do a quick aside. I was having a meeting today with the AI Advisory Council edx, the the actual creator of the original edx that was started by Mit.

208
00:13:43.600 --> 00:13:46.379
Anthony Taylor: was there today just talking to us all.

209
00:13:46.520 --> 00:13:50.009
Anthony Taylor: And and he was talking. He's like

210
00:13:50.220 --> 00:13:54.289
Anthony Taylor: he's like man. He says he said we need to change the marketing

211
00:13:54.650 --> 00:14:01.809
Anthony Taylor: and just say, we're going to teach full stack, and Gabe will get this. He took full stack. We're going to teach full stack with AI.

212
00:14:02.060 --> 00:14:08.489
Anthony Taylor: Now we're going to change the name of the class. It's not going to be full stack anymore. It's going to be full stack with AI.

213
00:14:08.920 --> 00:14:12.240
Anthony Taylor: It's going to be data and analytics with a

214
00:14:12.550 --> 00:14:16.919
Anthony Taylor: right. He goes because we can't tell them not to use AI. It's stupid.

215
00:14:17.290 --> 00:14:23.099
Anthony Taylor: And then I did my whole story about. If you would have told me not to use Google. And everyone was like, Yeah, exactly.

216
00:14:23.210 --> 00:14:24.079
Anthony Taylor: And I'm like.

217
00:14:24.550 --> 00:14:25.450
Anthony Taylor: so

218
00:14:26.500 --> 00:14:34.590
Anthony Taylor: it's changed the world. We're all going to use it. I use it every day when I find people, other employees that aren't using it. I go. What the hell is wrong.

219
00:14:34.920 --> 00:14:36.420
Anthony Taylor: What are you trying to prove?

220
00:14:37.640 --> 00:14:42.209
Anthony Taylor: I it's it doesn't make any sense. It's here. It ain't going any.

221
00:14:43.180 --> 00:14:44.080
Anthony Taylor: Alright.

222
00:14:44.300 --> 00:14:45.110
Anthony Taylor: Okay.

223
00:14:47.080 --> 00:14:50.650
Anthony Taylor: I like today because it's a whole bunch of fun stuff I get to just talk about.

224
00:14:52.860 --> 00:14:55.899
Anthony Taylor: My favorite thing to do is just talk about stuff.

225
00:15:00.190 --> 00:15:03.200
Anthony Taylor: I'm seeing if they had anything else they want me to tell you. Okay, no, alright.

226
00:15:03.350 --> 00:15:04.850
Anthony Taylor: So here we are.

227
00:15:05.880 --> 00:15:09.320
Anthony Taylor: It is now post open. AI.

228
00:15:09.820 --> 00:15:15.810
Anthony Taylor: Everyone is going. Oh, I mean, before September of last year anybody ever heard

229
00:15:16.020 --> 00:15:18.759
Anthony Taylor: had anyone ever heard of an Ll. N,

230
00:15:21.100 --> 00:15:23.720
Anthony Taylor: you have Jennifer. Okay, you have, Natalie.

231
00:15:24.270 --> 00:15:26.299
Anthony Taylor: I'm impressed. I don't know that I

232
00:15:26.610 --> 00:15:28.650
Anthony Taylor: I'm pretty impressed on you guys that.

233
00:15:28.650 --> 00:15:31.880
Mason, Natalie: I didn't know what it means, but I heard my friend say it.

234
00:15:32.730 --> 00:15:37.879
Anthony Taylor: Okay? Alright. Well, that's cool. Where did you hear, Jennifer? You heard in your your schooling.

235
00:15:38.977 --> 00:15:41.440
Jennifer Dahlgren: No! I was working with them with qualtrics.

236
00:15:42.430 --> 00:15:45.370
Anthony Taylor: Okay. Well, there you go. That's good. Rodney.

237
00:15:46.536 --> 00:15:48.324
Masarirambi, Rodney: Through apple's

238
00:15:48.960 --> 00:16:03.860
Masarirambi, Rodney: ml papers that they bring out with whatever? Well, I should say it sounds like whatever they're just doing. But it's actually what they did a couple of years ago, like couple of months ago. But they released later. Make it seem like that's what they just did. But no, they're actually further along what they are.

239
00:16:05.110 --> 00:16:09.079
Anthony Taylor: Yeah, except for they still ain't caught up to even Google, who's like the furthest behind.

240
00:16:09.820 --> 00:16:11.820
Masarirambi, Rodney: I don't know. I don't know. I mean.

241
00:16:12.139 --> 00:16:21.730
Anthony Taylor: It's weird. Now, some companies didn't do it right. I I I'm not gonna pick on apple on this, because really I would say the worst player right now.

242
00:16:22.090 --> 00:16:26.109
Anthony Taylor: I don't know Gemini's helping them. But up until Gem and I was Google.

243
00:16:26.170 --> 00:16:27.200
Anthony Taylor: they were. So

244
00:16:27.210 --> 00:16:30.289
Anthony Taylor: I mean, Apple just wasn't doing anything. So no one cared.

245
00:16:30.500 --> 00:16:33.959
Anthony Taylor: you know, but Google was kept releasing just garbage.

246
00:16:34.160 --> 00:16:39.989
Anthony Taylor: It's kind of an interesting thing, the the the clear winner

247
00:16:40.170 --> 00:16:41.900
Anthony Taylor: of this, so far

248
00:16:42.110 --> 00:16:43.280
Anthony Taylor: as Microsoft.

249
00:16:43.700 --> 00:16:44.810
Anthony Taylor: They

250
00:16:45.130 --> 00:16:47.180
Anthony Taylor: and they just totally

251
00:16:47.290 --> 00:16:51.010
Anthony Taylor: said, This is going to be big, and they did the right thing.

252
00:16:51.020 --> 00:16:56.719
Anthony Taylor: So I don't know. We'll see what happens, you know, 5 years from now it could be a whole different face. But

253
00:16:57.110 --> 00:16:57.939
Anthony Taylor: I don't know.

254
00:16:58.240 --> 00:17:00.720
Anthony Taylor: Okay, so ll is

255
00:17:01.950 --> 00:17:08.540
Anthony Taylor: so bag of words. We've talked about bag of words. Bag of words still plays an important part in Llfm.

256
00:17:08.630 --> 00:17:20.980
Anthony Taylor: It's still developing a vocabulary. We need to get the count of words. The the the difference will be as you're going to find out. We're not going to count one word at a time

257
00:17:22.140 --> 00:17:23.010
Anthony Taylor: and can't

258
00:17:23.870 --> 00:17:32.880
Anthony Taylor: so in LP. Applications. Predictive text. And this is all in LP. This is not just. Ll. Mlms are particularly good at this

259
00:17:33.710 --> 00:17:50.100
Anthony Taylor: right? But we've we've all we've talked about the predictive texting a few times with language models. And this is all language models. We have pre-processing. We have data coming in. We're gonna fit, evaluate, optimize. And there's our routine.

260
00:17:50.210 --> 00:17:52.800
Anthony Taylor: So Llms are AI models

261
00:17:53.140 --> 00:17:56.450
Anthony Taylor: and why we don't call them machine learning models. I don't know.

262
00:17:57.850 --> 00:18:02.329
Anthony Taylor: I mean, technically, there's still machine learning models. But we're going to go with AI models

263
00:18:02.450 --> 00:18:08.760
Anthony Taylor: that have been trained on an exceptionally large corpus of text data.

264
00:18:08.860 --> 00:18:15.340
Anthony Taylor: The model has a high number of parameters. Now, everybody remembers what parameters are. Right?

265
00:18:16.820 --> 00:18:21.320
Anthony Taylor: Right? We've been so changing, doing hyper parameters on our models since

266
00:18:21.970 --> 00:18:23.260
Anthony Taylor: classification.

267
00:18:24.130 --> 00:18:25.160
Anthony Taylor: Okay.

268
00:18:25.250 --> 00:18:30.450
Anthony Taylor: well, these parameters are basically weights. Actually, does it talk about this?

269
00:18:32.230 --> 00:18:34.069
Anthony Taylor: Why are we coding already?

270
00:18:34.470 --> 00:18:36.449
Anthony Taylor: Darn it, okay,

271
00:18:43.570 --> 00:18:46.870
Anthony Taylor: so I want you to remember, we'll get back to the weights thing in just a second.

272
00:18:46.890 --> 00:18:53.910
Anthony Taylor: Remember that primary objective of Nlp in general is to have a computer that has no understanding of language.

273
00:18:54.270 --> 00:18:57.939
Anthony Taylor: recognize and infer the meaning of words and contexts.

274
00:18:58.340 --> 00:19:05.019
Anthony Taylor: Okay, so first, we have to translate our language into numerical representations for the model to work with.

275
00:19:07.430 --> 00:19:12.629
Anthony Taylor: and you know, we did that with. We did the counts with body, with the bag of words.

276
00:19:15.030 --> 00:19:15.820
Anthony Taylor: yeah.

277
00:19:16.040 --> 00:19:17.890
Anthony Taylor: we're past that move on.

278
00:19:20.280 --> 00:19:22.189
Anthony Taylor: So it develops vocabulary.

279
00:19:28.950 --> 00:19:35.099
Anthony Taylor: So just as a reminder. Let's just see, let's get some understanding. So back to back words.

280
00:19:37.760 --> 00:19:39.329
Anthony Taylor: If I send in

281
00:19:40.090 --> 00:19:46.090
Anthony Taylor: a corpus like, or if I just send in, how about a book I'm put in? Who put in the script of Princess Bri?

282
00:19:47.550 --> 00:19:52.370
Anthony Taylor: How? What? What is that first step to break it into

283
00:19:52.430 --> 00:19:55.919
Anthony Taylor: words or sentences, that bag of words. Does

284
00:19:56.550 --> 00:19:58.310
Anthony Taylor: anybody remember the term.

285
00:19:58.310 --> 00:19:59.420
Baro, Sonja: Tokenize.

286
00:20:00.070 --> 00:20:05.479
Anthony Taylor: Tokenized it exactly. Now, tokenize could be sentences. It could be words.

287
00:20:05.550 --> 00:20:14.939
Anthony Taylor: Okay, the aim of tokenization is to analyze the text for important trends. How did we prepare the data to narrow the focus

288
00:20:15.050 --> 00:20:16.880
Anthony Taylor: of our analysis?

289
00:20:18.310 --> 00:20:20.770
Anthony Taylor: So think about the next step in

290
00:20:20.940 --> 00:20:23.109
Anthony Taylor: the preprocess. And what did we do.

291
00:20:23.110 --> 00:20:25.479
Baro, Sonja: We removed. Stop! Words.

292
00:20:26.760 --> 00:20:27.190
Anthony Taylor: And.

293
00:20:27.190 --> 00:20:29.880
Baro, Sonja: I'm I'm feeling good today.

294
00:20:30.290 --> 00:20:31.689
Anthony Taylor: You are mailing it.

295
00:20:32.350 --> 00:20:33.380
Anthony Taylor: But okay.

296
00:20:33.380 --> 00:20:35.109
Mason, Natalie: And Sonya.

297
00:20:35.890 --> 00:20:38.547
Anthony Taylor: Alright. So with that, let's go now.

298
00:20:38.880 --> 00:20:42.319
Clayton Graves: Remove punctuation and stuff first, right, though.

299
00:20:42.320 --> 00:20:46.310
Anthony Taylor: Well punctuation, and then stop words. But you could have done. Stop words and then punctuation.

300
00:20:46.310 --> 00:20:47.000
Clayton Graves: It wouldn't have matter.

301
00:20:47.300 --> 00:20:50.609
Anthony Taylor: Those 2. Those 2 are okay because they they don't overlap

302
00:20:51.110 --> 00:20:54.000
Anthony Taylor: alright. Okay. So

303
00:20:54.670 --> 00:20:56.469
Anthony Taylor: I will tell you guys

304
00:20:56.670 --> 00:21:00.359
Anthony Taylor: that we will absolutely have to go to Google Co lab today.

305
00:21:00.600 --> 00:21:05.000
Anthony Taylor: This first exercise does not require it because it's really just a review.

306
00:21:05.360 --> 00:21:06.170
Anthony Taylor: Okay.

307
00:21:06.856 --> 00:21:09.789
Anthony Taylor: but I wanted you guys to know that

308
00:21:11.080 --> 00:21:13.799
Anthony Taylor: we will be there today. So be prepared.

309
00:21:13.890 --> 00:21:15.490
Anthony Taylor: Also.

310
00:21:16.020 --> 00:21:19.610
Anthony Taylor: and and maybe we'll jump in and I'll show you guys how to do it.

311
00:21:21.091 --> 00:21:25.180
Anthony Taylor: hugging face. There, there is an Api. We don't need it

312
00:21:25.210 --> 00:21:31.049
Anthony Taylor: today, and probably don't need it tomorrow, but we will need it before the end of class.

313
00:21:31.290 --> 00:21:35.130
Anthony Taylor: and you will use your open AI Api for those of you that got one

314
00:21:36.070 --> 00:21:37.600
Anthony Taylor: you'll all have to get with

315
00:21:37.960 --> 00:21:40.370
Anthony Taylor: before the end of class, not today, but

316
00:21:41.110 --> 00:21:42.370
Anthony Taylor: next 2 weeks.

317
00:21:42.580 --> 00:21:52.630
Anthony Taylor: Okay, so first, we're gonna bring. This is just that you know the natural language toolkit we're gonna bring in. This is all stuff we've done before. This is the same thing we're kind of reviewing.

318
00:21:52.880 --> 00:21:55.580
Anthony Taylor: Here's some sentences, do do, do.

319
00:21:56.110 --> 00:21:57.090
Anthony Taylor: Okay.

320
00:21:57.610 --> 00:21:59.080
Anthony Taylor: we're going to.

321
00:21:59.420 --> 00:22:06.970
Anthony Taylor: Here's what we're gonna do our Regex thing to get rid of everything pretty much that is not letters or spaces.

322
00:22:07.605 --> 00:22:13.940
Anthony Taylor: So we're gonna replace it with nothing for each sentence. And we're gonna lowercase each sentence

323
00:22:14.110 --> 00:22:15.810
Anthony Taylor: and append

324
00:22:16.160 --> 00:22:20.760
Anthony Taylor: all of those individual items into this list of tokens.

325
00:22:20.780 --> 00:22:22.159
Anthony Taylor: And we're going to take a look

326
00:22:23.220 --> 00:22:24.680
Anthony Taylor: pretty nice.

327
00:22:25.190 --> 00:22:25.960
Anthony Taylor: Huh?

328
00:22:27.950 --> 00:22:30.879
Anthony Taylor: Then we we have stop words.

329
00:22:30.920 --> 00:22:32.810
Anthony Taylor: Where did we make the stop words?

330
00:22:33.480 --> 00:22:35.000
Anthony Taylor: I didn't see us do that.

331
00:22:35.790 --> 00:22:39.479
Anthony Taylor: Oh, right here. So we have our list of stop words.

332
00:22:39.580 --> 00:22:43.170
Anthony Taylor: And we're basically going to loop through and say

333
00:22:43.940 --> 00:22:48.089
Anthony Taylor: for word and token these tokens. If it's not

334
00:22:48.350 --> 00:22:50.890
Anthony Taylor: in stop words, output it.

335
00:22:52.000 --> 00:23:03.739
Anthony Taylor: Okay? And we're gonna output it into this variable. And then this variable, we're going to add to this list. So when we're all done, we will only have words that we're not in stop words.

336
00:23:04.820 --> 00:23:05.670
Anthony Taylor: Okay?

337
00:23:06.560 --> 00:23:10.169
Anthony Taylor: Last, we're going to create our actual bag of words. So we're going to say.

338
00:23:10.290 --> 00:23:12.099
Anthony Taylor: for I am

339
00:23:12.390 --> 00:23:14.179
Anthony Taylor: the length of this hole

340
00:23:14.330 --> 00:23:15.290
Anthony Taylor: list.

341
00:23:16.030 --> 00:23:18.940
Anthony Taylor: I want you to grab each word

342
00:23:18.950 --> 00:23:21.879
Anthony Taylor: from each list. So the first one is going to be

343
00:23:22.490 --> 00:23:23.600
Anthony Taylor: this item.

344
00:23:23.950 --> 00:23:33.579
Anthony Taylor: Grab the word. If the words, not in bag of words. I want you to. We're going to create a dictionary. So it's going to say bag of words word. So what

345
00:23:34.020 --> 00:23:35.940
Anthony Taylor: equals 0.

346
00:23:36.540 --> 00:23:37.420
Anthony Taylor: Okay?

347
00:23:37.890 --> 00:23:41.230
Anthony Taylor: And then it's gonna add one to what

348
00:23:41.630 --> 00:23:45.830
Anthony Taylor: everybody understand. So dictionary value, the key will be once

349
00:23:45.870 --> 00:23:47.469
Anthony Taylor: the value would be one.

350
00:23:47.490 --> 00:23:49.839
Anthony Taylor: Gonna come back around, grab it vest

351
00:23:50.680 --> 00:23:54.880
Anthony Taylor: it's a new word. Bag of words equals 0. Add one investment, one.

352
00:23:54.910 --> 00:23:56.889
Anthony Taylor: then retirement. Same thing.

353
00:23:57.100 --> 00:23:59.490
Anthony Taylor: Okay, that's gonna come here.

354
00:24:00.410 --> 00:24:01.610
Anthony Taylor: invest

355
00:24:02.320 --> 00:24:06.530
Anthony Taylor: does exist. So just add one. So now, invest is 2.

356
00:24:06.730 --> 00:24:07.920
Anthony Taylor: Everybody got it.

357
00:24:08.090 --> 00:24:09.769
Anthony Taylor: That's what's happening in this code.

358
00:24:10.000 --> 00:24:12.309
Anthony Taylor: Now we're all done. We should. Yes.

359
00:24:12.310 --> 00:24:19.470
Baro, Sonja: Just a quick question is the addition of the one because of we're making a dictionary, not a list.

360
00:24:20.220 --> 00:24:25.569
Anthony Taylor: We? Well, we're we're doing a bag of words. So we're just doing account. So this is how we count.

361
00:24:25.670 --> 00:24:26.210
Anthony Taylor: So.

362
00:24:26.210 --> 00:24:27.050
Baro, Sonja: Michigan.

363
00:24:27.050 --> 00:24:31.369
Anthony Taylor: Yeah, this is just creating the dictionary item.

364
00:24:31.560 --> 00:24:31.920
Baro, Sonja: Right.

365
00:24:31.920 --> 00:24:34.630
Anthony Taylor: This, we're gonna add one, because it was it.

366
00:24:35.040 --> 00:24:41.490
Anthony Taylor: Yeah. So every time it shows up, it's gonna add one. So we end up with. See, there were 3 invests.

367
00:24:41.490 --> 00:24:42.590
Baro, Sonja: Account. Okay?

368
00:24:42.590 --> 00:24:44.059
Anthony Taylor: Pretty much the only work. Yeah.

369
00:24:44.090 --> 00:24:48.560
Anthony Taylor: So we ended up with the bag of words. This is the standard old school.

370
00:24:48.660 --> 00:24:51.130
Anthony Taylor: 2,007 bag of words.

371
00:24:51.440 --> 00:24:52.370
Anthony Taylor: Okay.

372
00:24:52.908 --> 00:25:00.989
Anthony Taylor: but this is the old. This is literally like I got no, I got. I got only python skills, and I don't know how to use any other tool

373
00:25:02.080 --> 00:25:02.920
Anthony Taylor: ken.

374
00:25:03.070 --> 00:25:04.140
Anthony Taylor: So there's one.

375
00:25:04.770 --> 00:25:08.940
Anthony Taylor: So we modernize this bit beside hit. Learn the other day.

376
00:25:09.100 --> 00:25:13.160
Anthony Taylor: And we did it with count. Vector, so here.

377
00:25:14.190 --> 00:25:22.409
Anthony Taylor: we're going to use count vectorizer, stop words, English, we're going to say, Hey, put these sentences into vectorizer and train it

378
00:25:22.480 --> 00:25:24.440
Anthony Taylor: and then print them out.

379
00:25:24.820 --> 00:25:26.389
Anthony Taylor: Get something like this.

380
00:25:26.580 --> 00:25:29.769
Anthony Taylor: not easy to to understand. But we do know

381
00:25:30.360 --> 00:25:32.870
Anthony Taylor: that this is, you know, the counts right?

382
00:25:33.747 --> 00:25:35.269
Anthony Taylor: Yeah, we're there.

383
00:25:35.270 --> 00:25:37.539
Clayton Graves: Did before, right? Because I can see.

384
00:25:37.540 --> 00:25:41.490
Anthony Taylor: Well, except for it's by line, right? So.

385
00:25:41.960 --> 00:25:45.779
Anthony Taylor: But yes, ultimately, we're we're as you're gonna see as we

386
00:25:45.800 --> 00:25:54.960
Anthony Taylor: go down, we're going to see that word ends up in there twice. So we get. We add those together. So here we got the bag of words to a single array.

387
00:25:55.150 --> 00:26:00.809
Anthony Taylor: right? And then we're going to use the feature names, which is the original words. And we end up with

388
00:26:00.850 --> 00:26:05.499
Anthony Taylor: this data frame. So here you can see invest is in there 3 times.

389
00:26:06.480 --> 00:26:07.470
Anthony Taylor: Alright.

390
00:26:09.070 --> 00:26:10.920
Anthony Taylor: So we're going to take this.

391
00:26:11.150 --> 00:26:13.390
Anthony Taylor: create a list of the words.

392
00:26:13.510 --> 00:26:15.899
Anthony Taylor: and then we're going to sum

393
00:26:16.300 --> 00:26:18.749
Anthony Taylor: each word and gets

394
00:26:19.150 --> 00:26:20.929
Anthony Taylor: our bag of words. This one.

395
00:26:21.990 --> 00:26:22.660
Anthony Taylor: okay?

396
00:26:23.090 --> 00:26:25.470
Anthony Taylor: So that's pretty interesting. Right?

397
00:26:25.470 --> 00:26:28.390
Clayton Graves: So that was just 3 different ways of getting the same output.

398
00:26:28.390 --> 00:26:32.100
Anthony Taylor: Well, 2 different ways of getting the same thing right? So we have

399
00:26:32.110 --> 00:26:34.390
Anthony Taylor: the the join it by hand.

400
00:26:35.400 --> 00:26:38.570
Anthony Taylor: Okay? And then the doing it with SK. Learns

401
00:26:39.450 --> 00:26:40.720
Anthony Taylor: count capability.

402
00:26:40.790 --> 00:26:42.590
Anthony Taylor: Our vector count vectorized.

403
00:26:42.800 --> 00:26:44.580
Anthony Taylor: Okay, you're going to get another one

404
00:26:44.920 --> 00:26:45.800
Anthony Taylor: soon.

405
00:26:46.540 --> 00:26:47.560
Anthony Taylor: Alright.

406
00:26:47.830 --> 00:26:48.730
Anthony Taylor: alright.

407
00:26:49.190 --> 00:26:50.540
Anthony Taylor: So

408
00:26:51.150 --> 00:26:55.329
Anthony Taylor: let's see what we're supposed to. That's it on that.

409
00:26:59.060 --> 00:27:00.020
Anthony Taylor: Okay?

410
00:27:02.360 --> 00:27:03.820
Anthony Taylor: So it basically

411
00:27:05.370 --> 00:27:08.900
Anthony Taylor: last module. We did this exact same thing, the second

412
00:27:09.170 --> 00:27:18.550
Anthony Taylor: with thousands of sentences across a document and observed that the vocabulary could grow to thousands of words. Remember, we had like really huge bag of work.

413
00:27:20.350 --> 00:27:25.480
Anthony Taylor: but there were a lot of zeros as well, because a lot of sentencing didn't have all the work makes sense.

414
00:27:25.890 --> 00:27:26.730
Anthony Taylor: though

415
00:27:27.050 --> 00:27:29.830
Anthony Taylor: over the years we started

416
00:27:29.870 --> 00:27:31.469
Anthony Taylor: to build out

417
00:27:31.620 --> 00:27:42.570
Anthony Taylor: the model of bag of words and develop more advanced techniques. Language models, capture, semantic meaning and contextual understanding of the sentence itself.

418
00:27:42.750 --> 00:27:44.069
Anthony Taylor: No longer

419
00:27:44.150 --> 00:27:46.590
Anthony Taylor: are we gonna only look at a single word.

420
00:27:47.380 --> 00:27:50.610
Anthony Taylor: Okay? So a language model is.

421
00:27:50.630 --> 00:27:57.149
Anthony Taylor: It's just a really another statistical model, I mean, I kind of said, oh, we got the models back in the nineties.

422
00:27:57.160 --> 00:27:58.170
Anthony Taylor: Yes.

423
00:27:58.560 --> 00:28:00.510
Anthony Taylor: it's still a statistical model.

424
00:28:01.030 --> 00:28:08.560
Anthony Taylor: Okay, but we've added some additional capabilities that are going to allow it to understand larger context.

425
00:28:09.309 --> 00:28:16.120
Anthony Taylor: It's, you know, piece deep learning models, which means they can be compiled and trained and used to predict and make inferences.

426
00:28:16.340 --> 00:28:27.190
Anthony Taylor: The kind of predictions of models are probabilistic assessments of what word comes next in sequence. How many of you have ever asked any bean, chat.

427
00:28:27.700 --> 00:28:31.780
Anthony Taylor: chat, gpt any of them the same question twice.

428
00:28:33.300 --> 00:28:34.979
Anthony Taylor: Did you get the same answer

429
00:28:36.560 --> 00:28:42.240
Anthony Taylor: almost never. Now, if you ask it like a code like a logic question or a code question, maybe.

430
00:28:42.330 --> 00:28:43.979
Anthony Taylor: But have you asked it like a

431
00:28:44.170 --> 00:28:45.729
Anthony Taylor: like? Write me a story.

432
00:28:46.180 --> 00:28:48.169
Anthony Taylor: Okay, about blah blah blah.

433
00:28:48.660 --> 00:28:51.139
Anthony Taylor: It will not write the same story twice.

434
00:28:51.250 --> 00:28:58.090
Anthony Taylor: because there is a mathematical output that says, well, this is what I think comes next.

435
00:28:58.660 --> 00:29:07.969
Anthony Taylor: while you would think that mathematical output would guarantee you the same story. It's exactly the opposite. The probability is is is based on

436
00:29:08.870 --> 00:29:10.739
Anthony Taylor: who knows? The moment?

437
00:29:11.600 --> 00:29:14.990
Anthony Taylor: Okay? So you could get a.

438
00:29:15.490 --> 00:29:23.509
Anthony Taylor: it's possible you get a hundred percent different story. It's also possible you could get a similar story, but every word

439
00:29:25.420 --> 00:29:34.910
Anthony Taylor: alright. So so let's say you say, write me a story about a little girl that goes out in the woods and finds a house where Pre bears live.

440
00:29:35.580 --> 00:29:40.580
Anthony Taylor: Okay, it'll start writing the story, but as it's building, it's literally

441
00:29:40.700 --> 00:29:44.029
Anthony Taylor: coming up with the next sentence, it doesn't come up with

442
00:29:44.470 --> 00:29:45.920
Anthony Taylor: the whole story.

443
00:29:46.290 --> 00:29:49.139
Anthony Taylor: It comes up with a couple of lines at a time.

444
00:29:50.280 --> 00:29:51.970
Anthony Taylor: Okay, so it's like.

445
00:29:52.000 --> 00:29:57.330
Anthony Taylor: really crazy on the way it works. So anyway, we'll keep going. We'll get into more of that later.

446
00:29:59.540 --> 00:30:08.960
Anthony Taylor: So if you've ever used phones, you've done predictive email clients, you know, they're really good nowadays with like almost answering your email for you.

447
00:30:09.110 --> 00:30:11.200
Anthony Taylor: Right? You just have to like, select it.

448
00:30:12.790 --> 00:30:14.240
Anthony Taylor: And you're done.

449
00:30:18.520 --> 00:30:19.230
Anthony Taylor: yeah.

450
00:30:21.040 --> 00:30:22.060
Anthony Taylor: okay.

451
00:30:22.670 --> 00:30:23.590
Anthony Taylor: so

452
00:30:24.970 --> 00:30:31.380
Anthony Taylor: we're going to get. It's it's funny there's no more slides for this. But there's like a whole history of the current situation.

453
00:30:31.490 --> 00:30:38.030
Anthony Taylor: So in earlier discussions, the history and lp, we ended off at at with attention in 2,014.

454
00:30:38.130 --> 00:30:39.530
Anthony Taylor: This was huge.

455
00:30:39.730 --> 00:30:43.299
Anthony Taylor: It wasn't until 2,017. There you go

456
00:30:43.310 --> 00:30:47.020
Anthony Taylor: when the transformers became were introduced.

457
00:30:47.190 --> 00:30:52.669
Anthony Taylor: So 2,014, he came up with the article 2,017 is when we actually

458
00:30:52.700 --> 00:30:54.799
Anthony Taylor: had the capability to do it.

459
00:30:57.740 --> 00:30:58.850
Anthony Taylor: Transformer

460
00:30:58.920 --> 00:31:08.100
Anthony Taylor: was a paradigm shift entirely. The architecture gave rise to a burst of Llms which are what's being used today.

461
00:31:08.920 --> 00:31:11.800
Anthony Taylor: Okay, we're going to actually use one that's pretty old.

462
00:31:12.170 --> 00:31:13.670
Anthony Taylor: but it still works.

463
00:31:14.060 --> 00:31:20.650
Anthony Taylor: Meta, open AI, Google. Windows hugging face are the primo players. Actually.

464
00:31:21.060 --> 00:31:26.229
Anthony Taylor: Facebook is playing. You know this, you know, we talk about. We talk about Microsoft, Google, Apple

465
00:31:26.590 --> 00:31:29.090
Anthony Taylor: and Openai. We didn't mention Meta.

466
00:31:29.880 --> 00:31:33.659
Anthony Taylor: I'm telling you the Meta model that they open sourced

467
00:31:33.980 --> 00:31:36.410
Anthony Taylor: is excellent.

468
00:31:36.690 --> 00:31:39.299
Anthony Taylor: like, really really good.

469
00:31:39.440 --> 00:31:40.680
Anthony Taylor: I use it

470
00:31:40.800 --> 00:31:47.099
Anthony Taylor: almost day almost as much as I use chat, gpt, and only reason I use chat, gpt, I pay for it, and it's slightly better.

471
00:31:47.450 --> 00:31:52.000
Anthony Taylor: but for a free model that you could load on your own own computer.

472
00:31:52.610 --> 00:31:53.760
Anthony Taylor: It's amazing.

473
00:31:54.450 --> 00:31:55.420
Anthony Taylor: really? Good.

474
00:31:55.750 --> 00:31:57.280
Anthony Taylor: Okay, so keep that in mind.

475
00:31:59.890 --> 00:32:06.780
Anthony Taylor: yeah. So met is met is a pretty good player, but they're doing it in an interesting way, completely different than Microsoft.

476
00:32:07.240 --> 00:32:11.259
Anthony Taylor: Microsoft is like, well, they're giving it away for free, too. Basically. So

477
00:32:11.370 --> 00:32:13.570
Anthony Taylor: I don't know. They're just not giving you the source

478
00:32:18.620 --> 00:32:24.939
Anthony Taylor: anyway. So each developing a considerable number of Llms that use the encoder, decoder.

479
00:32:24.990 --> 00:32:30.279
Anthony Taylor: encoder only or decoder only transformer architecture.

480
00:32:30.910 --> 00:32:35.649
Anthony Taylor: I'm going to explain that. Don't worry. Early models favored encoder. Only.

481
00:32:36.450 --> 00:32:43.110
Anthony Taylor: Okay? So it encoded the data and then it outputs something and it had to get translated back externally.

482
00:32:43.980 --> 00:32:44.880
Anthony Taylor: the

483
00:32:46.540 --> 00:32:51.050
Anthony Taylor: the encoder, decorter, transformation architectures

484
00:32:51.110 --> 00:32:54.109
Anthony Taylor: where a task was to predict a masked word.

485
00:32:54.560 --> 00:32:59.950
Anthony Taylor: Okay. So what it did was you gave its sentence, and, like one of the words, would be massed out

486
00:33:01.910 --> 00:33:05.979
Anthony Taylor: and the early Llms. Their job was to figure out the work

487
00:33:06.530 --> 00:33:09.120
Anthony Taylor: that was missing. So it was still text

488
00:33:09.200 --> 00:33:17.299
Anthony Taylor: completion. But it wasn't text completion at the end of the sent. It basically had context before and context at.

489
00:33:17.330 --> 00:33:19.029
Anthony Taylor: and could fill in the middle.

490
00:33:20.110 --> 00:33:22.419
Anthony Taylor: which was still pretty awesome.

491
00:33:25.250 --> 00:33:29.690
Anthony Taylor: so a lot of them really favored that. And that was the big thing. Okay?

492
00:33:31.750 --> 00:33:37.090
Anthony Taylor: But GPT. 3 and guys, we just jumped to 2,020.

493
00:33:39.050 --> 00:33:40.220
Anthony Taylor: Okay.

494
00:33:40.290 --> 00:33:43.500
Anthony Taylor: used a decoder only architecture.

495
00:33:43.510 --> 00:33:49.560
Anthony Taylor: And since then this has been kind of the big thing. So what does that mean? That means

496
00:33:49.570 --> 00:33:51.730
Anthony Taylor: it's only predicting

497
00:33:52.220 --> 00:33:54.370
Anthony Taylor: the next word.

498
00:33:55.820 --> 00:33:56.800
Anthony Taylor: That's it.

499
00:33:58.100 --> 00:34:06.419
Anthony Taylor: Now, can you give something in the middle. Can you say? You know, I have a question about this, and here's some more information absolutely. But

500
00:34:06.430 --> 00:34:11.910
Anthony Taylor: the goal of, or the the point of Gpt. 3.5 4.

501
00:34:12.710 --> 00:34:14.929
Anthony Taylor: They were all text completion.

502
00:34:16.800 --> 00:34:17.650
Anthony Taylor: Okay.

503
00:34:19.850 --> 00:34:23.950
Anthony Taylor: So in this module we're going to use one called Bert.

504
00:34:27.580 --> 00:34:37.219
Anthony Taylor: it says, with, oh, this whole module. Yeah. So today we're going to use for before this this week is over this 3 classes we will use some Gpt stuff.

505
00:34:37.846 --> 00:34:39.430
Anthony Taylor: And you will get

506
00:34:41.320 --> 00:34:45.669
Anthony Taylor: text completion. You will also be able to do translation.

507
00:34:45.770 --> 00:34:48.169
Anthony Taylor: He was like, well, I could do translation out well

508
00:34:48.300 --> 00:34:50.559
Anthony Taylor: with the Lms way easy.

509
00:34:51.280 --> 00:34:56.020
Anthony Taylor: Literally, we could pass in some text. They make it German comes back German.

510
00:34:56.800 --> 00:34:58.269
Anthony Taylor: 4 or 5 lines of code.

511
00:35:00.050 --> 00:35:00.960
Anthony Taylor: Okay.

512
00:35:01.090 --> 00:35:03.109
Anthony Taylor: And many of these models are free.

513
00:35:03.120 --> 00:35:04.140
Anthony Taylor: Just so, you know.

514
00:35:08.160 --> 00:35:14.769
Anthony Taylor: so the most prevalent language models, and what we're going to focus on between now and Project 3

515
00:35:14.920 --> 00:35:16.669
Anthony Taylor: are going to be ll ends

516
00:35:19.360 --> 00:35:22.539
Anthony Taylor: So I mentioned earlier about parameters, right?

517
00:35:22.710 --> 00:35:26.489
Anthony Taylor: And we talked about the parameters hypertunying, and all that kind of stuff.

518
00:35:28.970 --> 00:35:29.800
Anthony Taylor: the

519
00:35:30.070 --> 00:35:31.659
Anthony Taylor: one thing about

520
00:35:32.080 --> 00:35:33.580
Anthony Taylor: neural networks

521
00:35:33.990 --> 00:35:35.760
Anthony Taylor: when we save them.

522
00:35:35.970 --> 00:35:37.209
Anthony Taylor: What are we saving?

523
00:35:37.290 --> 00:35:38.849
Anthony Taylor: Remember, we talked about that.

524
00:35:40.270 --> 00:35:41.540
Anthony Taylor: What are we saving.

525
00:35:42.080 --> 00:35:42.600
Baro, Sonja: We have.

526
00:35:42.600 --> 00:35:43.920
Meredith McCanse (she/her): The waiting.

527
00:35:43.920 --> 00:35:46.820
Anthony Taylor: The weights and the biases right?

528
00:35:47.030 --> 00:35:48.400
Anthony Taylor: When we.

529
00:35:48.460 --> 00:35:53.929
Anthony Taylor: So when you see this Llm, so ll, m, so, lama, 70, p.

530
00:35:54.230 --> 00:35:55.110
Anthony Taylor: Right?

531
00:35:55.650 --> 00:35:57.210
Anthony Taylor: 70 billion.

532
00:35:58.480 --> 00:36:00.250
Anthony Taylor: Okay, that's 70

533
00:36:00.540 --> 00:36:02.510
Anthony Taylor: 1 billion parameters.

534
00:36:03.500 --> 00:36:05.240
Anthony Taylor: Our tokens that was token.

535
00:36:07.170 --> 00:36:10.520
Anthony Taylor: hold on. We call it previous manager there, and that

536
00:36:11.570 --> 00:36:16.499
Anthony Taylor: so 70 billion tokens used to train it, but they have.

537
00:36:16.640 --> 00:36:20.369
Anthony Taylor: There's so many parameters, so many weights, that they're working with.

538
00:36:20.990 --> 00:36:29.709
Anthony Taylor: The good news is is again. Once they're trained they can be exposed as an Api. Now, not something you're going to do on your laptop.

539
00:36:30.960 --> 00:36:32.189
Anthony Taylor: In fact.

540
00:36:32.240 --> 00:36:38.009
Anthony Taylor: if you read some of the articles to train, and it's funny how these have kind of faded away.

541
00:36:38.070 --> 00:36:43.399
Anthony Taylor: But to train Openai's GPT. 4, they're saying it was like

542
00:36:44.320 --> 00:36:49.129
Anthony Taylor: the amount of carbon emissions that New York City does, you know, in 10 years?

543
00:36:50.520 --> 00:36:57.800
Anthony Taylor: Okay, I mean, it was such a huge amount of energy and resources to just train it.

544
00:37:00.450 --> 00:37:03.670
Anthony Taylor: Okay? So it's kind of, I haven't heard those stories in a while.

545
00:37:03.850 --> 00:37:07.560
Anthony Taylor: but it's a big, it's a huge amount. I don't know exact numbers.

546
00:37:07.850 --> 00:37:08.910
Anthony Taylor: But

547
00:37:11.840 --> 00:37:12.750
Anthony Taylor: okay.

548
00:37:13.100 --> 00:37:22.069
Anthony Taylor: so benefit of using pre-trained models is, we don't have to train it. And that's what we're going to be doing. Most of our pretend models are going to come from ugging face.

549
00:37:22.570 --> 00:37:28.159
Anthony Taylor: Okay, what did I tell you? Hugging face was like a Github repo of trained models

550
00:37:28.760 --> 00:37:31.830
Anthony Taylor: and transformers and encoders.

551
00:37:32.160 --> 00:37:36.370
Anthony Taylor: all of the tools you need to do Llms on your local computer.

552
00:37:40.470 --> 00:37:41.160
Anthony Taylor: yeah.

553
00:37:41.730 --> 00:37:45.240
Anthony Taylor: So we did, Reuters, that whole giant

554
00:37:45.390 --> 00:37:46.590
Anthony Taylor: corpora

555
00:37:47.070 --> 00:37:52.930
Anthony Taylor: of routers. And I mean, that's like a drop in the bucket compared to what we're doing with the Lms

556
00:37:55.890 --> 00:38:01.110
Anthony Taylor: the scale of data used in Lolms. It's trained over weeks or months

557
00:38:01.930 --> 00:38:07.020
Anthony Taylor: with a lot of hardware accelerators. Primarily.

558
00:38:07.240 --> 00:38:13.990
Anthony Taylor: Nvidia's graphic processing units and Google's tensor processing units. Tpus.

559
00:38:14.780 --> 00:38:17.649
Anthony Taylor: okay, which is basically like a Gpu.

560
00:38:17.980 --> 00:38:20.540
Anthony Taylor: it's processing tensor. Yeah, it's not.

561
00:38:21.360 --> 00:38:27.100
Baro, Sonja: So one thing you've told us is pre-processing is the bulk of the work.

562
00:38:27.982 --> 00:38:33.319
Baro, Sonja: So if it's taking weeks and months to train

563
00:38:34.020 --> 00:38:38.999
Baro, Sonja: like, how long does it take a team of people to pre process.

564
00:38:39.000 --> 00:38:40.030
Anthony Taylor: Process it.

565
00:38:40.030 --> 00:38:41.110
Baro, Sonja: Yeah.

566
00:38:41.530 --> 00:38:43.030
Anthony Taylor: Well, the good news is.

567
00:38:43.552 --> 00:38:47.210
Baro, Sonja: Like in such short times. It's crazy.

568
00:38:47.870 --> 00:38:49.470
Anthony Taylor: What you're gonna see

569
00:38:50.420 --> 00:38:52.270
Anthony Taylor: today. And this week

570
00:38:52.380 --> 00:38:55.339
Anthony Taylor: is that there are a lot of tools that are going to help us with that.

571
00:38:55.630 --> 00:38:56.635
Anthony Taylor: Okay,

572
00:38:58.100 --> 00:39:01.869
Anthony Taylor: most of them are available on hugging face. Most of them come

573
00:39:02.260 --> 00:39:04.640
Anthony Taylor: from the same people that make the model.

574
00:39:04.900 --> 00:39:10.309
Anthony Taylor: The reason for that is is when we're going, when we do tokenization and embedding.

575
00:39:10.630 --> 00:39:12.939
Anthony Taylor: However, they did it in the model

576
00:39:13.780 --> 00:39:15.779
Anthony Taylor: is how you need to do it.

577
00:39:16.180 --> 00:39:22.029
Anthony Taylor: You can't do it like for the. You can't use the Meta tokenizer and embedded embedding

578
00:39:22.090 --> 00:39:24.389
Anthony Taylor: necessarily, for

579
00:39:25.060 --> 00:39:27.600
Anthony Taylor: you know, a good a gem. And I

580
00:39:27.720 --> 00:39:28.649
Anthony Taylor: you just can't.

581
00:39:29.020 --> 00:39:38.049
Anthony Taylor: Okay, because it was probably done different now, and there's always some overlap, but it was probably done different. So you can't use it.

582
00:39:38.100 --> 00:39:41.379
Anthony Taylor: But yeah, I mean it. You know, there's a reason

583
00:39:41.650 --> 00:39:45.600
Anthony Taylor: these are not done by, you know, a classroom student.

584
00:39:45.660 --> 00:39:48.429
Anthony Taylor: These are done by entire companies.

585
00:39:49.190 --> 00:39:55.719
Anthony Taylor: Okay, these aren't built in a garage. These are built by very highly funded

586
00:39:55.990 --> 00:40:01.300
Anthony Taylor: organizations because they're dealing with. I mean just the data alone

587
00:40:02.320 --> 00:40:07.150
Anthony Taylor: that they're pulling in to use for the training would probably cost

588
00:40:07.290 --> 00:40:10.099
Anthony Taylor: hundreds of thousands of dollars a month just to store.

589
00:40:11.610 --> 00:40:12.650
Anthony Taylor: Okay?

590
00:40:12.890 --> 00:40:14.170
Baro, Sonja: So if

591
00:40:14.730 --> 00:40:22.740
Baro, Sonja: it, I just it promised a question on the latency of the data set that the models are trained on

592
00:40:23.050 --> 00:40:29.149
Baro, Sonja: because I was interesting. I was having conversation with of all people my stuff, my hair

593
00:40:29.600 --> 00:40:48.190
Baro, Sonja: stylist, and he's doing. He didn't know that the Chat Gpt had an end date as far as it's learning. And so we're talking about, how do we get it to be more up to date, and how long it? And I was explaining how long it takes. All the stuff they have to do. Is there like.

594
00:40:48.270 --> 00:40:51.689
Baro, Sonja: are there techniques to speed that up? Because

595
00:40:52.150 --> 00:40:58.090
Baro, Sonja: the more we rely on, because that my concern is if we're relying on old

596
00:40:58.200 --> 00:41:00.040
Baro, Sonja: old information

597
00:41:00.906 --> 00:41:05.500
Baro, Sonja: for some of the questions that you ask it to do, it's gonna limit it

598
00:41:06.210 --> 00:41:07.230
Baro, Sonja: so limits.

599
00:41:07.580 --> 00:41:08.880
Baro, Sonja: Useful. Pilot. Yes.

600
00:41:08.880 --> 00:41:09.670
Anthony Taylor: Sonia.

601
00:41:09.670 --> 00:41:10.500
Baro, Sonja: I'm sorry.

602
00:41:10.960 --> 00:41:13.069
Anthony Taylor: Have you used being co pilots yet.

603
00:41:13.210 --> 00:41:14.019
Baro, Sonja: I have.

604
00:41:14.170 --> 00:41:14.670
Baro, Sonja: yeah.

605
00:41:14.670 --> 00:41:17.009
Anthony Taylor: Okay? So there's that's how they fix.

606
00:41:17.240 --> 00:41:22.000
Anthony Taylor: Okay. Bing Co pilot is still using Chat Gpt, for which.

607
00:41:22.330 --> 00:41:25.619
Anthony Taylor: to the best of my knowledge, stop sometime in 23.

608
00:41:25.620 --> 00:41:28.989
Baro, Sonja: So the integration of Google, or.

609
00:41:28.990 --> 00:41:30.010
Anthony Taylor: Browsing.

610
00:41:30.010 --> 00:41:30.590
Baro, Sonja: Browsing.

611
00:41:30.590 --> 00:41:31.180
Anthony Taylor: That.

612
00:41:31.180 --> 00:41:32.310
Baro, Sonja: AI, okay.

613
00:41:32.310 --> 00:41:33.490
Clayton Graves: April, Chelsea.

614
00:41:33.921 --> 00:41:38.239
Anthony Taylor: Does that, too? April 20. Thank you. Clayton. So

615
00:41:38.620 --> 00:41:42.590
Anthony Taylor: everything you ask it, I mean, if you ask it. You know who was in the super bowl

616
00:41:43.500 --> 00:41:44.010
Anthony Taylor: right.

617
00:41:44.010 --> 00:41:44.680
Baro, Sonja: Makes so.

618
00:41:44.680 --> 00:41:47.190
Anthony Taylor: In theory, chat. Tpt doesn't know.

619
00:41:48.050 --> 00:41:55.009
Baro, Sonja: So you're relying on the the browser for the just the delta of what's missing.

620
00:41:56.120 --> 00:42:00.590
Anthony Taylor: Here's what happens. And and and hopefully, I'm pretty sure we're gonna do this in class

621
00:42:01.230 --> 00:42:04.480
Anthony Taylor: it it let's say, I create a chat bot

622
00:42:05.078 --> 00:42:12.530
Anthony Taylor: and like, like, I have an example I'm doing at work right now where I'm basically bringing in like this entire website.

623
00:42:12.890 --> 00:42:17.370
Anthony Taylor: I tokenize it, encode it and embed it into the model.

624
00:42:17.750 --> 00:42:24.079
Anthony Taylor: Okay, creating a vector, database. And all this kind of stuff which we do talk about at least superficially. Next week.

625
00:42:26.430 --> 00:42:28.050
Anthony Taylor: and that

626
00:42:28.280 --> 00:42:30.300
Anthony Taylor: information, so

627
00:42:30.400 --> 00:42:37.019
Anthony Taylor: that information will now be included in the response of my larger Llm.

628
00:42:37.190 --> 00:42:46.480
Anthony Taylor: So my larger ll live can now, if I ask it something. It will use that information as well as what it already knows. To answer the question.

629
00:42:46.840 --> 00:42:49.679
Anthony Taylor: Okay? And that's what they're doing today. That's how

630
00:42:49.750 --> 00:42:51.719
Anthony Taylor: you can go work at a company.

631
00:42:51.800 --> 00:43:02.430
Anthony Taylor: and you want your company's data in there in the model, you can fairly easily integrate it with Lama 70 p. Or Openai or any of

632
00:43:02.730 --> 00:43:19.409
Anthony Taylor: using a vector database and and kind of what we're gonna do today with similarity. And all this kind of stuff. And it'll basically add that to it. So what's happening with being co-pilot? And all of these things is and and and chat. Tpt. 4. If you guys are using the premium version.

633
00:43:19.970 --> 00:43:23.539
Anthony Taylor: It just does a search, adds those results

634
00:43:23.890 --> 00:43:27.290
Anthony Taylor: to its answer, and then answers the question.

635
00:43:27.730 --> 00:43:29.550
Baro, Sonja: That's pretty cool. That's like.

636
00:43:29.550 --> 00:43:30.209
Anthony Taylor: Yeah. Oh, yeah.

637
00:43:30.210 --> 00:43:32.450
Baro, Sonja: Using the idea of the Internet

638
00:43:32.850 --> 00:43:35.099
Baro, Sonja: to help fill those gaps.

639
00:43:35.395 --> 00:43:35.690
Anthony Taylor: Anything?

640
00:43:36.110 --> 00:43:37.250
Baro, Sonja: Yeah, yeah. Yeah. The other day.

641
00:43:37.250 --> 00:43:42.610
Anthony Taylor: The cool thing about being co-pilot is, it gives you references to what it answers with

642
00:43:43.150 --> 00:43:45.610
Anthony Taylor: while Chat Gpt still won't do that.

643
00:43:47.340 --> 00:44:00.300
Anthony Taylor: Okay, so big. Microsoft is smart to do that because everyone gets upset. All these things are copying, you know, people's private stuff. Well, no, they're saying dude. Here's here's the websites. We got this information.

644
00:44:01.920 --> 00:44:08.890
Anthony Taylor: Okay? And I mean, you can't argue with that, because that's what Google's been doing for, you know, 40 years, 30 years whatever.

645
00:44:09.310 --> 00:44:10.274
Anthony Taylor: Right?

646
00:44:11.710 --> 00:44:13.420
Anthony Taylor: so there's that.

647
00:44:14.080 --> 00:44:16.360
Anthony Taylor: Okay. Alright, that's a great question.

648
00:44:16.650 --> 00:44:19.280
Anthony Taylor: Okay, when we're almost, we're gonna yeah. Go ahead.

649
00:44:19.280 --> 00:44:21.056
Masarirambi, Rodney: Sorry. Just one. Just one quick question.

650
00:44:21.310 --> 00:44:21.740
Anthony Taylor: Scotland.

651
00:44:21.740 --> 00:44:26.679
Masarirambi, Rodney: So. Something that you said made me think of this. And well, I've been thinking about it for a while.

652
00:44:27.940 --> 00:44:31.699
Masarirambi, Rodney: It takes weeks, months to train some of these.

653
00:44:32.710 --> 00:44:35.019
Anthony Taylor: All of them. Yes, right? The big ones. Yeah.

654
00:44:35.020 --> 00:44:35.770
Masarirambi, Rodney: So

655
00:44:36.940 --> 00:44:42.700
Masarirambi, Rodney: say, right now we're in a company we're about to commence on this training.

656
00:44:43.020 --> 00:44:52.849
Masarirambi, Rodney: Do we take like a subsection and test it out, like, you know, like, you know, like something that's gonna take like 5 min, 10 min like an hour, like 2 h, so that we at least

657
00:44:53.150 --> 00:44:59.009
Masarirambi, Rodney: can get an understanding of what we're seeing and what we're expecting, and rather than just starting this sort of like.

658
00:44:59.070 --> 00:45:05.199
Masarirambi, Rodney: I mean, I know that this won't be like this, but like oops, I forgot a comma, but there's nothing to do with it. But it's like.

659
00:45:05.565 --> 00:45:17.774
Masarirambi, Rodney: you know, 2 months like like, what? How do you? How do you safeguard getting, you know? How do you? How do you pre train before you train, I guess, is is like a like a like a subsection.

660
00:45:18.080 --> 00:45:21.089
Anthony Taylor: You can certainly use the smaller corporate

661
00:45:21.720 --> 00:45:25.359
Anthony Taylor: Copora, or or Corpus. Even if you want to go really small.

662
00:45:25.530 --> 00:45:30.500
Anthony Taylor: you can do it. But you're really not going to get the full answer until you do it. All

663
00:45:30.770 --> 00:45:36.410
Anthony Taylor: right. I mean, I mean, what do you do? So you could throw in. We're gonna throw in all of week.

664
00:45:36.550 --> 00:45:40.950
Anthony Taylor: Okay, maybe that takes 2 days to train. I don't know. I

665
00:45:41.595 --> 00:45:50.389
Anthony Taylor: and and you say, Okay, let's test it out. You certainly do that right. The one I'm doing. I took like a whole my whole company's website

666
00:45:50.470 --> 00:45:55.690
Anthony Taylor: and threw everything into it right? I mean, it only took like 4 h to train

667
00:45:58.130 --> 00:46:00.470
Anthony Taylor: But I mean, it's just one company's website.

668
00:46:00.520 --> 00:46:03.600
Anthony Taylor: These larger ones are training

669
00:46:03.720 --> 00:46:05.410
Anthony Taylor: the entire Internet.

670
00:46:06.830 --> 00:46:10.470
Anthony Taylor: So so you know. I mean, it's a huge difference.

671
00:46:10.500 --> 00:46:11.115
Anthony Taylor: The

672
00:46:12.590 --> 00:46:19.700
Anthony Taylor: My, I don't. I'll be honest with you. I haven't worked for these guys, and I don't know what they're doing, I one day I'll find somebody and ask them.

673
00:46:19.810 --> 00:46:20.690
Anthony Taylor: But

674
00:46:21.070 --> 00:46:24.650
Anthony Taylor: I would assume as an architect, which is what I do.

675
00:46:24.990 --> 00:46:26.500
Anthony Taylor: You would overlap.

676
00:46:26.810 --> 00:46:29.229
Anthony Taylor: so you would start a training.

677
00:46:29.480 --> 00:46:35.600
Anthony Taylor: and you would tune some parameters that you think might make a difference and start another train

678
00:46:36.560 --> 00:46:40.429
Anthony Taylor: right, and maybe have a few going offset.

679
00:46:40.850 --> 00:46:45.030
Anthony Taylor: so that when you're all done you just kind of pick the best of the group.

680
00:46:45.640 --> 00:46:47.820
Anthony Taylor: or if they're just crap, I guess.

681
00:46:47.860 --> 00:46:55.850
Anthony Taylor: train. So you tune some more and train some more, but you're hoping that one at the what. However, you tune it will get you close to the answer.

682
00:46:57.730 --> 00:47:04.049
Anthony Taylor: I don't know. I mean, from an architectural standpoint. That's probably where I would go with it. Given if I had enough hardware to do it.

683
00:47:05.490 --> 00:47:12.630
Anthony Taylor: Yeah, it's a good question. One of these days I'll have to find somebody and ask them. It might even be out there on the Internet. How they do.

684
00:47:12.850 --> 00:47:13.950
Anthony Taylor: I haven't looked

685
00:47:14.140 --> 00:47:15.630
Anthony Taylor: so pretty cool.

686
00:47:16.740 --> 00:47:17.850
Anthony Taylor: Great question.

687
00:47:19.355 --> 00:47:20.590
Anthony Taylor: Okay.

688
00:47:20.620 --> 00:47:21.990
Anthony Taylor: So

689
00:47:23.090 --> 00:47:26.370
Anthony Taylor: so the transport architecture. That's what we're talking about today.

690
00:47:26.700 --> 00:47:29.659
Anthony Taylor: We're going to do some smaller ones.

691
00:47:30.180 --> 00:47:34.390
Anthony Taylor: One of the most talked about Lm applications at present is chat tpt

692
00:47:35.210 --> 00:47:39.350
Anthony Taylor: and it it generated pre-trained transformers

693
00:47:39.670 --> 00:47:41.819
Anthony Taylor: is what Gpt stands for.

694
00:47:42.530 --> 00:47:43.859
Anthony Taylor: Everybody catch that

695
00:47:43.970 --> 00:47:47.830
Anthony Taylor: generative, retrained transformers.

696
00:47:47.990 --> 00:47:55.739
Anthony Taylor: There are powerful, useful lms that are pre-trained on massive amounts of data and built on the transformer architecture.

697
00:47:56.502 --> 00:47:59.669
Anthony Taylor: And they generate text. They're text completion.

698
00:48:00.380 --> 00:48:02.930
Anthony Taylor: Okay, at least they were. I assume they still are.

699
00:48:03.030 --> 00:48:07.120
Anthony Taylor: So do Llam's learn from supervised or unsupervised.

700
00:48:09.650 --> 00:48:10.769
Anthony Taylor: Do you guys think.

701
00:48:13.030 --> 00:48:14.430
Clayton Graves: I would imagine

702
00:48:14.550 --> 00:48:16.860
Clayton Graves: I'm gonna guess I'm supervised

703
00:48:17.670 --> 00:48:18.450
Clayton Graves: Ham.

704
00:48:20.160 --> 00:48:24.070
Clayton Graves: Only because, you know, you're you're you're basically

705
00:48:24.550 --> 00:48:29.680
Clayton Graves: you're typing in a question or or whatever it is you're you're wanting to do.

706
00:48:30.050 --> 00:48:39.219
Clayton Graves: and then it has to go through and and figure all that out. Now, I don't know, maybe, that it has classifications for nouns and verbs and things like that, but I don't know.

707
00:48:40.140 --> 00:48:41.119
Baro, Sonja: There are the models.

708
00:48:41.120 --> 00:48:41.860
Anthony Taylor: Dancing.

709
00:48:42.870 --> 00:48:46.719
Baro, Sonja: What about the models we use cause? Aren't those about predicting.

710
00:48:50.000 --> 00:48:51.549
Anthony Taylor: What do you mean? What models

711
00:48:52.700 --> 00:48:53.910
Anthony Taylor: which models.

712
00:48:54.380 --> 00:48:57.261
Baro, Sonja: Yeah, I know it's too too general. Maybe I have.

713
00:48:57.895 --> 00:48:59.619
Anthony Taylor: We've done a lot of.

714
00:48:59.620 --> 00:49:00.990
Baro, Sonja: Mom, yeah.

715
00:49:01.780 --> 00:49:05.900
Baro, Sonja: because something has to give it a framework, because it's it's.

716
00:49:05.900 --> 00:49:09.949
Anthony Taylor: Right? Well, the transformers, we're using the transformer architecture.

717
00:49:10.200 --> 00:49:16.569
Anthony Taylor: So so I that I like the unsupervised statement. And I like that. Well, I don't know.

718
00:49:16.720 --> 00:49:19.080
Anthony Taylor: Okay. Anybody think it's supervised?

719
00:49:21.650 --> 00:49:22.490
Anthony Taylor: No.

720
00:49:22.490 --> 00:49:24.916
Masarirambi, Rodney: So I maybe it's not way too much.

721
00:49:25.220 --> 00:49:26.160
michael mcpherson: Yes, but

722
00:49:27.400 --> 00:49:28.299
michael mcpherson: go ahead. Go ahead!

723
00:49:28.300 --> 00:49:29.310
Anthony Taylor: And Ronnie.

724
00:49:29.740 --> 00:49:31.060
Masarirambi, Rodney: I was. I was hoping.

725
00:49:31.940 --> 00:49:42.349
Masarirambi, Rodney: just taking a look at the name and the way that we've kind of like go with what the name is when we're when when somebody says Transform right? I think it's like, could bring it into something else

726
00:49:42.360 --> 00:49:48.169
Masarirambi, Rodney: which would say to me that the likelihood is more unsupervised if just going by the name.

727
00:49:49.240 --> 00:49:50.080
Masarirambi, Rodney: but.

728
00:49:50.080 --> 00:49:50.670
Anthony Taylor: I like that.

729
00:49:50.670 --> 00:49:51.980
Masarirambi, Rodney: And that's just going on the name.

730
00:49:55.355 --> 00:49:55.780
Anthony Taylor: K.

731
00:49:56.940 --> 00:50:00.089
Anthony Taylor: Alright. So the answer is, you believe it is.

732
00:50:00.330 --> 00:50:01.469
Anthony Taylor: It's beau.

733
00:50:01.620 --> 00:50:05.219
Anthony Taylor: It's a self-supervised approach to learning.

734
00:50:05.500 --> 00:50:09.150
Anthony Taylor: Okay, it starts out unsupervised.

735
00:50:09.280 --> 00:50:16.839
Anthony Taylor: The vowels as it through deep learning to start reusing the answers that it gets. Now

736
00:50:18.780 --> 00:50:27.129
Anthony Taylor: in 2,024, in early, 2,024. Samsung hit the news

737
00:50:27.720 --> 00:50:32.650
Anthony Taylor: because they were using the Llm's. One of the very first adopters.

738
00:50:32.750 --> 00:50:36.159
Anthony Taylor: and some of their private data

739
00:50:36.810 --> 00:50:39.149
Anthony Taylor: ended up in the Llm.

740
00:50:40.190 --> 00:50:40.960
Anthony Taylor: But

741
00:50:41.120 --> 00:50:42.349
Anthony Taylor: oh, yeah.

742
00:50:42.630 --> 00:50:44.040
Anthony Taylor: how does that happen?

743
00:50:44.340 --> 00:50:53.539
Anthony Taylor: Well, in the beginning Openai was very open about the fact that, hey? Whatever you use, we're going to continue to train our model.

744
00:50:54.730 --> 00:50:57.769
Anthony Taylor: Okay? So they were doing that. This

745
00:50:58.020 --> 00:51:01.030
Anthony Taylor: gave me the biggest headache of my life.

746
00:51:01.540 --> 00:51:07.560
Anthony Taylor: Okay, I wanted to use it, but as soon as that new story came out. Every major company

747
00:51:07.880 --> 00:51:09.580
Anthony Taylor: in the world when

748
00:51:09.920 --> 00:51:10.690
Anthony Taylor: no

749
00:51:11.440 --> 00:51:12.500
Anthony Taylor: can't use it.

750
00:51:14.140 --> 00:51:15.100
Anthony Taylor: Okay.

751
00:51:15.270 --> 00:51:21.150
Anthony Taylor: even though Openai, like a month later, came out and said, we will not use your information.

752
00:51:21.500 --> 00:51:23.449
Anthony Taylor: They swore they wouldn't do it

753
00:51:23.860 --> 00:51:28.260
Anthony Taylor: alright. Now, to their credit they're still the biggest and the best.

754
00:51:28.620 --> 00:51:36.279
Anthony Taylor: So they did. Okay, but I mean, I can tell you my company still will not allow you to use chat tpt on their network.

755
00:51:37.770 --> 00:51:44.580
Clayton Graves: I I get a war, I get a warning about their their policy, and that I can't generate code with it.

756
00:51:44.680 --> 00:51:47.749
Clayton Graves: and that I can't put any proprietary information on it.

757
00:51:48.020 --> 00:51:49.940
Clayton Graves: but if they let me use it.

758
00:51:50.800 --> 00:51:54.210
Anthony Taylor: Right? So I mean the key to that. By the way, for me.

759
00:51:54.300 --> 00:52:04.490
Anthony Taylor: I was talking to the AI Director this morning, and he asked me a question. I said, Dude, just you could type this in Chat Gbt, and get the answer. I don't know why you're asking me this question

760
00:52:04.610 --> 00:52:05.870
Anthony Taylor: right. And

761
00:52:06.326 --> 00:52:12.343
Anthony Taylor: he goes. I can't. It's against company policy. And I go to do it on your home computer, like, I do

762
00:52:12.830 --> 00:52:17.120
Anthony Taylor: like what the hell I mean, come on or do it on your phone.

763
00:52:17.480 --> 00:52:18.320
Clayton Graves: For the record.

764
00:52:18.320 --> 00:52:19.889
Anthony Taylor: You could do it on your phone.

765
00:52:19.890 --> 00:52:21.360
Clayton Graves: Claude still.

766
00:52:21.690 --> 00:52:25.880
Clayton Graves: but the record. Claud still says that they will use your data to train.

767
00:52:26.680 --> 00:52:27.680
Anthony Taylor: Claude does.

768
00:52:27.680 --> 00:52:28.320
Clayton Graves: Yes.

769
00:52:28.990 --> 00:52:29.630
Anthony Taylor: Okay.

770
00:52:29.750 --> 00:52:32.622
Anthony Taylor: so and and there's a lot of them that do.

771
00:52:33.110 --> 00:52:39.720
Anthony Taylor: yeah, there is a lot that still say that they'll do that. So you do want to be at least aware that that can happen.

772
00:52:39.840 --> 00:52:48.679
Anthony Taylor: Okay, but Openai. And then there's even a better one. If you're using azure. The Cloud Service, the Microsoft Cloud Service. They have

773
00:52:48.800 --> 00:52:52.980
Anthony Taylor: a a a purchase of Openai that

774
00:52:53.100 --> 00:53:05.190
Anthony Taylor: goes with their enterprise agreement. So like, if you're a company like ours, where everything is on azure. We could bring up open AI and be pretty much locked in by Microsoft's agreement with us

775
00:53:05.330 --> 00:53:07.550
Anthony Taylor: that they won't use any of our data.

776
00:53:07.920 --> 00:53:12.190
Anthony Taylor: I want you to think about that for a second, any of you that work for a company that is in the cloud.

777
00:53:12.470 --> 00:53:13.410
Anthony Taylor: Okay.

778
00:53:13.970 --> 00:53:16.499
Anthony Taylor: all of our data is in azure.

779
00:53:17.380 --> 00:53:18.669
Anthony Taylor: All of it

780
00:53:18.920 --> 00:53:23.950
Anthony Taylor: is in azure. Because that's where we do our data analytics. That's where everything sits

781
00:53:23.960 --> 00:53:29.150
Anthony Taylor: right? So using openai, that is under that same agreement for us is no big deal.

782
00:53:29.630 --> 00:53:32.767
Anthony Taylor: but we still have to convince me. Anyway.

783
00:53:33.570 --> 00:53:34.830
Anthony Taylor: important.

784
00:53:35.320 --> 00:53:39.609
Anthony Taylor: This is, I mean, we we kind of talked about this during ethics.

785
00:53:40.040 --> 00:53:44.320
Anthony Taylor: these tools all have cognitive bias as well as just straight up bias.

786
00:53:44.930 --> 00:53:48.280
Anthony Taylor: Okay, they just do. They were trained on the Internet.

787
00:53:48.470 --> 00:53:51.120
Anthony Taylor: If you know, if it's

788
00:53:51.420 --> 00:53:54.759
Anthony Taylor: if the Internet represents one group more than another.

789
00:53:54.840 --> 00:53:56.940
Anthony Taylor: the Llm has that training.

790
00:53:58.330 --> 00:54:04.119
Anthony Taylor: Okay, unless you do like what Google did and like intentionally try to counter it.

791
00:54:05.260 --> 00:54:08.780
Anthony Taylor: It's just that's just a fact. I mean, it's not I. I.

792
00:54:09.660 --> 00:54:12.669
Anthony Taylor: This is one of those things where, as a tech guy

793
00:54:13.050 --> 00:54:18.510
Anthony Taylor: and not I'm taking. I'm taking off my teacher hat. And I'm going to be Mr. Techy guy for a sec.

794
00:54:18.840 --> 00:54:21.920
Anthony Taylor: Okay, this is the data. I got guys.

795
00:54:23.220 --> 00:54:25.979
Anthony Taylor: you want the answer. This is the data you gave.

796
00:54:27.260 --> 00:54:30.060
Anthony Taylor: Okay, if I put my teacher hat back on

797
00:54:30.150 --> 00:54:40.829
Anthony Taylor: it, would it? It is important for us to understand that statement for what it really means, and what it really means is that not everybody is represented

798
00:54:40.990 --> 00:54:42.550
Anthony Taylor: by these Llips.

799
00:54:43.120 --> 00:54:44.309
Anthony Taylor: that's a fact

800
00:54:44.620 --> 00:54:51.500
Anthony Taylor: as long as we use them, knowing that fact and work towards trying to get full representation.

801
00:54:51.740 --> 00:54:53.360
Anthony Taylor: then we're doing what we can do.

802
00:54:53.650 --> 00:54:54.430
Anthony Taylor: But

803
00:54:55.160 --> 00:54:58.640
Anthony Taylor: the flip side of that is just. Don't use them at all well, at the set

804
00:54:58.700 --> 00:55:00.480
Anthony Taylor: they add so much value.

805
00:55:01.170 --> 00:55:04.130
Anthony Taylor: but we have to kind of find that balance, and we have to.

806
00:55:04.960 --> 00:55:09.749
Anthony Taylor: I'm not gonna say deal with it. But we can make a difference just by educating people on

807
00:55:10.320 --> 00:55:12.650
Anthony Taylor: just by making sure you understand

808
00:55:12.990 --> 00:55:16.139
Anthony Taylor: right. I don't believe for a second

809
00:55:16.860 --> 00:55:21.420
Anthony Taylor: that the people at Openai said, let's intentionally leave out. You know all these people.

810
00:55:22.420 --> 00:55:27.140
Anthony Taylor: There was probably some tech dudes that went. This is the data you gave me. Man, what do you want me to do?

811
00:55:27.530 --> 00:55:30.350
Anthony Taylor: This is the data available. What do you want me to do.

812
00:55:31.710 --> 00:55:33.080
Anthony Taylor: Okay? So

813
00:55:33.660 --> 00:55:35.270
Anthony Taylor: it's enough for them. Alright.

814
00:55:38.200 --> 00:55:39.490
Anthony Taylor: let's do some code.

815
00:55:40.700 --> 00:55:43.129
Anthony Taylor: We're not doing everyone do.

816
00:55:44.820 --> 00:55:47.809
Anthony Taylor: And okay, so this is, we're gonna do

817
00:55:48.230 --> 00:55:51.290
Anthony Taylor: on Google Co lab. So everybody go to Google Co Lab

818
00:55:53.550 --> 00:55:54.780
Anthony Taylor: and

819
00:55:55.750 --> 00:55:59.060
Anthony Taylor: upload that notebook, grab to upload a couple of things.

820
00:55:59.350 --> 00:56:03.229
Anthony Taylor: cause there's a I don't know if this one has a resource file, but the other one does.

821
00:56:04.955 --> 00:56:08.230
Anthony Taylor: So we're on number 2 unsolved.

822
00:56:13.420 --> 00:56:21.340
Anthony Taylor: Now, as soon as this puppy comes up, install transformers. It takes like 2 min for them to install, and we have to do it for every single notebook today.

823
00:56:21.870 --> 00:56:25.540
Anthony Taylor: So get that installed as quickly as you can.

824
00:56:34.910 --> 00:56:35.350
Baro, Sonja: Anthony.

825
00:56:35.350 --> 00:56:36.100
Anthony Taylor: Alright, so.

826
00:56:36.100 --> 00:56:39.220
Baro, Sonja: Can you call out for for this one

827
00:56:40.020 --> 00:56:40.670
Baro, Sonja: and.

828
00:56:40.670 --> 00:56:45.509
Anthony Taylor: I. I just put you in here because all of the rest of them there's going to be issues.

829
00:56:45.510 --> 00:56:45.980
Baro, Sonja: Okay.

830
00:56:45.980 --> 00:56:49.109
Anthony Taylor: So we might as well just start in here. It's it's just

831
00:56:49.380 --> 00:56:52.400
Anthony Taylor: there. There are issues with some of the

832
00:56:52.470 --> 00:56:56.660
Anthony Taylor: the things we have to load if you do it locally. But look how fast that cube!

833
00:56:56.680 --> 00:56:59.550
Anthony Taylor: Okay. Well, mine came up fast. Maybe it's the next one, has it.

834
00:56:59.900 --> 00:57:00.535
Anthony Taylor: Daniel?

835
00:57:01.330 --> 00:57:03.359
Meredith McCanse (she/her): Message for the fifth install.

836
00:57:04.190 --> 00:57:05.439
Anthony Taylor: I'm sorry, Meredith.

837
00:57:05.440 --> 00:57:08.900
Meredith McCanse (she/her): Is anyone else getting an error message for the Pip install.

838
00:57:08.900 --> 00:57:10.179
Masarirambi, Rodney: Yeah, I'm getting an.

839
00:57:10.180 --> 00:57:11.250
Anthony Taylor: Google, Colab.

840
00:57:11.460 --> 00:57:11.890
Meredith McCanse (she/her): Yeah.

841
00:57:12.320 --> 00:57:16.049
Baro, Sonja: Just check your space. I had to back out a space.

842
00:57:18.100 --> 00:57:18.690
Baro, Sonja: It doesn't.

843
00:57:18.690 --> 00:57:20.330
Anthony Taylor: Oh, you mean there might be a.

844
00:57:20.800 --> 00:57:21.170
Baro, Sonja: Yeah.

845
00:57:21.170 --> 00:57:22.720
Anthony Taylor: Oh, like right ahead of.

846
00:57:22.720 --> 00:57:26.860
Baro, Sonja: Yeah. Before the exclamation point there was a space.

847
00:57:27.210 --> 00:57:29.940
Meredith McCanse (she/her): Got it. Yeah, it seems to be working. Thank you.

848
00:57:30.560 --> 00:57:31.180
Anthony Taylor: Okay.

849
00:57:31.180 --> 00:57:33.110
Masarirambi, Rodney: Main, though I mean.

850
00:57:34.480 --> 00:57:35.170
Anthony Taylor: All right.

851
00:57:35.630 --> 00:57:38.079
Anthony Taylor: So today, we're gonna focus on 2

852
00:57:38.300 --> 00:57:43.900
Anthony Taylor: tokenizers. We're going to focus on word tokenization and sub word tokenization.

853
00:57:44.270 --> 00:57:50.139
Anthony Taylor: Okay, so word termination. Simplest form is basically what we've been doing. But we're going to do it in a different way.

854
00:57:50.691 --> 00:57:53.570
Anthony Taylor: You know, you just saw 2 ways a little while ago.

855
00:57:53.660 --> 00:57:57.670
Anthony Taylor: We did it like with code. And then we did it with count vectorize.

856
00:58:01.910 --> 00:58:03.750
Anthony Taylor: though.

857
00:58:04.000 --> 00:58:09.609
Anthony Taylor: Yeah, so almost like models like Tpd, 3 and 4. They're using sub word tokenization.

858
00:58:10.240 --> 00:58:12.500
Anthony Taylor: We're going to get into what that is that

859
00:58:13.054 --> 00:58:20.329
Anthony Taylor: it's basically so tokenization. Right now, we take the whole word. So we just go. Give me the word. Give me the word. Give me word. Give them the word. Give me the word.

860
00:58:20.420 --> 00:58:21.589
Anthony Taylor: Okay, and we

861
00:58:22.210 --> 00:58:23.579
Anthony Taylor: make a list of words.

862
00:58:23.630 --> 00:58:24.960
Anthony Taylor: sub word

863
00:58:25.040 --> 00:58:28.070
Anthony Taylor: effectively cuts the word into its root

864
00:58:28.470 --> 00:58:41.519
Anthony Taylor: or not. Always just the root, like you guys are. Gonna see? Token to say, Oh, that's actually in this exercise. So if everybody's got that first one installed, we're gonna install a transformer, called Bert Tokenizer.

865
00:58:42.240 --> 00:58:46.309
Anthony Taylor: And this tokenizer. It comes from the Transformers library.

866
00:58:46.720 --> 00:58:51.009
Anthony Taylor: It's for a model called Burt, which we're going to get to soon.

867
00:58:51.160 --> 00:58:52.639
Anthony Taylor: Once you have that

868
00:58:52.670 --> 00:58:58.140
Anthony Taylor: go ahead and initialize it. Notice it says, from pre-trained.

869
00:58:58.650 --> 00:59:01.450
Anthony Taylor: So this Bert base on case

870
00:59:01.570 --> 00:59:10.079
Anthony Taylor: is one of the pre-trained models that comes in the Transformers Library. It is going to download and install stuff

871
00:59:10.360 --> 00:59:12.590
Anthony Taylor: from hugging phase.

872
00:59:15.800 --> 00:59:21.309
Anthony Taylor: Okay, transformers. The library itself is actually maintained by face.

873
00:59:23.180 --> 00:59:31.720
Anthony Taylor: Okay, so once it downloads, it's downloading all of this cool stuff. Basically, this is the configuration information for the Tokenizer.

874
00:59:32.810 --> 00:59:33.790
Anthony Taylor: Alright.

875
00:59:34.270 --> 00:59:36.800
Anthony Taylor: once you have that, we're going to give it a word

876
00:59:36.850 --> 00:59:43.130
Anthony Taylor: or give it a sentence. Oh, and then we need to tokenize. So

877
00:59:43.380 --> 00:59:45.499
Anthony Taylor: let's do that.

878
00:59:45.740 --> 00:59:53.269
Anthony Taylor: So this isn't too tough. We we initialized it. As Tokenizer, we're going to create a variable. We'll call it subwords.

879
00:59:54.360 --> 00:59:58.070
Anthony Taylor: And we'll say, here it comes. Organizer

880
00:59:59.500 --> 01:00:00.570
Anthony Taylor: dots.

881
01:00:03.440 --> 01:00:04.750
Anthony Taylor: tokenize

882
01:00:05.910 --> 01:00:08.270
Anthony Taylor: parentheses pass in your text.

883
01:00:09.140 --> 01:00:10.100
Anthony Taylor: Okay?

884
01:00:11.350 --> 01:00:15.119
Anthony Taylor: And then we're gonna output subwarp. So we can see them. And

885
01:00:15.130 --> 01:00:21.370
Anthony Taylor: this is what we get. Now let's look at this result. Talk about it. First one. Yeah, that makes sense. It's work.

886
01:00:21.560 --> 01:00:24.180
Anthony Taylor: One thing I want you to note, though, it's lowercase.

887
01:00:24.400 --> 01:00:27.829
Anthony Taylor: These tokenizers will make everything lower. Case

888
01:00:29.320 --> 01:00:33.689
Anthony Taylor: next one. Okay, that's clearly a word learning. Also a word

889
01:00:33.700 --> 01:00:35.279
Anthony Taylor: about okay.

890
01:00:35.490 --> 01:00:37.300
Anthony Taylor: sub word.

891
01:00:39.470 --> 01:00:40.950
Anthony Taylor: Now, that's interesting.

892
01:00:41.360 --> 01:00:53.929
Anthony Taylor: So what this means when you're looking at this. If this dollar sign dollar sign is here, that means it. Cut this word and this dollar sign dollar sign means it's connected to the word before it.

893
01:00:55.070 --> 01:01:01.209
Anthony Taylor: But it decided the Tokenizer that this was a sub word.

894
01:01:02.510 --> 01:01:07.100
Anthony Taylor: Okay? And then the same thing. This one's kind of interesting with tokenization.

895
01:01:08.830 --> 01:01:13.359
Anthony Taylor: So it looked at this and said, Hey, tokens a word that we're going to use again.

896
01:01:14.590 --> 01:01:16.420
Anthony Taylor: So let's separate

897
01:01:18.030 --> 01:01:20.749
Anthony Taylor: now. Why, it didn't do it with learning. I don't know.

898
01:01:22.570 --> 01:01:23.560
Anthony Taylor: Okay.

899
01:01:23.830 --> 01:01:24.700
Anthony Taylor: But

900
01:01:24.930 --> 01:01:31.260
Anthony Taylor: the the idea behind this is is by doing this. It's basically you're ready for this.

901
01:01:31.420 --> 01:01:34.289
Anthony Taylor: increasing its vocabulary.

902
01:01:36.250 --> 01:01:41.670
Anthony Taylor: Alright, knowing the word tokenization is great, but knowing the word token

903
01:01:43.130 --> 01:01:44.109
Anthony Taylor: is better.

904
01:01:44.220 --> 01:01:48.129
Anthony Taylor: because token can have lots of other uses.

905
01:01:49.120 --> 01:01:54.919
Anthony Taylor: Okay? So using the sub word capability, it's increasing its vocabulary.

906
01:01:55.300 --> 01:01:57.009
Anthony Taylor: So that was the whole point

907
01:01:57.050 --> 01:01:58.529
Anthony Taylor: of subwarts.

908
01:01:59.366 --> 01:02:05.420
Anthony Taylor: Okay, so that was, that's it. That's it. We just tokenized the whole thing. That was it?

909
01:02:05.920 --> 01:02:12.510
Anthony Taylor: Okay? You saw how fun that was. Remember how hard that was before. Well, let's remind ourselves. Let's install in Ltk.

910
01:02:12.870 --> 01:02:14.970
Anthony Taylor: Get Reuters back in there.

911
01:02:15.530 --> 01:02:20.629
Anthony Taylor: I I checked with my English teacher, friend, and she said, it is Reuters. So, Robbie, you're right.

912
01:02:21.163 --> 01:02:24.830
Anthony Taylor: I I never doubted you with be honest with you. But

913
01:02:27.270 --> 01:02:29.459
Anthony Taylor: So we're going to install that

914
01:02:30.380 --> 01:02:32.510
Anthony Taylor: we're gonna print out the categories.

915
01:02:33.620 --> 01:02:39.150
Anthony Taylor: Gonna grab one of them for cocoa cocoa. It's not Kakawa. It's cocoa.

916
01:02:39.690 --> 01:02:41.830
Anthony Taylor: Don't give me that. There's no a in there.

917
01:02:42.700 --> 01:02:44.460
Anthony Taylor: Okay, there's the article.

918
01:02:45.020 --> 01:02:49.799
Anthony Taylor: Okay? So here we are. We got our article. So let's do our tokenization

919
01:02:49.840 --> 01:02:51.069
Anthony Taylor: for this one.

920
01:02:52.400 --> 01:02:56.740
Anthony Taylor: Okay, how do we do that? Well, what did we call it?

921
01:03:00.340 --> 01:03:03.920
Anthony Taylor: Oh, this one we don't have call. We have to just say, sent

922
01:03:04.890 --> 01:03:07.660
Anthony Taylor: Tokena. It's right there. Above there, you guys can see it

923
01:03:07.870 --> 01:03:09.640
Anthony Taylor: and then say.

924
01:03:10.260 --> 01:03:11.300
Anthony Taylor: Article.

925
01:03:13.170 --> 01:03:16.510
Anthony Taylor: run that we get that tokenization.

926
01:03:17.654 --> 01:03:20.560
Anthony Taylor: We can print the first sentence

927
01:03:21.560 --> 01:03:22.890
Anthony Taylor: by doing

928
01:03:23.180 --> 01:03:24.240
Anthony Taylor: extent

929
01:03:24.810 --> 01:03:26.050
Anthony Taylor: equals

930
01:03:26.070 --> 01:03:28.590
Anthony Taylor: sent token eyes again.

931
01:03:29.370 --> 01:03:32.040
Anthony Taylor: So this is tokenizing the sentence.

932
01:03:33.480 --> 01:03:34.910
Anthony Taylor: Article

933
01:03:36.410 --> 01:03:37.710
Anthony Taylor: 0.

934
01:03:45.020 --> 01:03:48.870
Anthony Taylor: I didn't print it. Oh, I didn't tell it to to to print that out

935
01:03:49.020 --> 01:03:50.509
Anthony Taylor: that you print sent.

936
01:03:53.520 --> 01:04:00.120
Anthony Taylor: Okay, pretty cool. If we wanted to use the Nldk word tokenizer.

937
01:04:01.950 --> 01:04:05.400
Anthony Taylor: like what we did up above with the Bert one.

938
01:04:05.570 --> 01:04:09.990
Anthony Taylor: we can just do word token eyes of that sent of the sentences.

939
01:04:10.240 --> 01:04:11.649
Anthony Taylor: And then we get all of that.

940
01:04:12.260 --> 01:04:19.829
Anthony Taylor: Okay, so take us a little more to get to here and notice it does not do any kind of sub word. These are just straight words.

941
01:04:20.130 --> 01:04:25.569
Anthony Taylor: Okay, so now let's do space. Remember, Spacey

942
01:04:26.350 --> 01:04:30.099
Anthony Taylor: did our parts of parts of speech. It was a lot of fun.

943
01:04:32.980 --> 01:04:35.849
Anthony Taylor: so spacey we got it all set up

944
01:04:35.870 --> 01:04:40.250
Anthony Taylor: so we can just do spacey underscore stent

945
01:04:42.290 --> 01:04:43.410
Anthony Taylor: equals.

946
01:04:43.420 --> 01:04:46.339
Anthony Taylor: And remember all we did for Spacey, NLP.

947
01:04:46.910 --> 01:04:48.639
Anthony Taylor: Pass in the sentence

948
01:04:49.550 --> 01:04:51.470
Anthony Taylor: and

949
01:04:51.750 --> 01:05:03.539
Anthony Taylor: and then to output this because it's gonna return something you can't just read directly we're gonna do list comprehension. We're gonna say for token in Spacey

950
01:05:04.660 --> 01:05:05.720
Anthony Taylor: sent.

951
01:05:06.360 --> 01:05:08.490
Anthony Taylor: And then we're going to output

952
01:05:09.560 --> 01:05:11.120
Anthony Taylor: token dot text.

953
01:05:13.450 --> 01:05:15.230
Anthony Taylor: And you get that. Now

954
01:05:17.200 --> 01:05:20.239
Anthony Taylor: let's grab the first sentence. Now, remember.

955
01:05:20.660 --> 01:05:23.329
Anthony Taylor: Basic has the ability to do subway

956
01:05:24.870 --> 01:05:27.249
Anthony Taylor: right? Why do you think that.

957
01:05:30.190 --> 01:05:32.349
Mason, Natalie: We go back up. I missed something.

958
01:05:33.140 --> 01:05:33.910
Anthony Taylor: Sure

959
01:05:40.470 --> 01:05:41.729
Anthony Taylor: you need me to make it bigger.

960
01:05:42.390 --> 01:05:46.630
Mason, Natalie: I'm making it bigger. Okay, let's see spaces.

961
01:05:47.249 --> 01:05:52.640
Mason, Natalie: No, please. Thank you, Toby. Okay, so it's

962
01:05:53.940 --> 01:05:54.940
Mason, Natalie: I. Just

963
01:05:58.430 --> 01:05:59.130
Mason, Natalie: next.

964
01:06:01.200 --> 01:06:02.460
Mason, Natalie: okay, thank you.

965
01:06:06.710 --> 01:06:08.060
Anthony Taylor: Oh, okay, wait a minute.

966
01:06:09.950 --> 01:06:20.149
Anthony Taylor: Oh, no. This wants us to use, Bert. Okay. So Spacey does not have a way to do subwords. But Spacey can get us to the tokens, and then we can use the Bert that we did up above

967
01:06:20.680 --> 01:06:26.479
Anthony Taylor: to get our sub word. So here we're going to go tokenizer. So this is the one from up above.

968
01:06:26.730 --> 01:06:28.340
Anthony Taylor: Tokenize again

969
01:06:29.260 --> 01:06:32.220
Anthony Taylor: and pass in our spacey

970
01:06:32.460 --> 01:06:34.060
Anthony Taylor: stuff that we just did.

971
01:06:34.570 --> 01:06:37.159
Anthony Taylor: and then that will give us our

972
01:06:38.410 --> 01:06:40.190
Anthony Taylor: sentence.

973
01:06:40.730 --> 01:06:42.400
Anthony Taylor: I totally spelled it wrong to me.

974
01:06:42.480 --> 01:06:44.660
Anthony Taylor: Suborders.

975
01:06:47.680 --> 01:06:50.170
Anthony Taylor: There you go. So now we get.

976
01:06:50.800 --> 01:06:52.670
Anthony Taylor: You can see we have subwords again.

977
01:06:54.680 --> 01:06:58.280
Anthony Taylor: Okay, so what were the main differences between the 3

978
01:06:59.340 --> 01:07:01.370
Anthony Taylor: advantages, differences.

979
01:07:06.060 --> 01:07:08.249
Clayton Graves: Bert seemed to do a lot more

980
01:07:08.840 --> 01:07:12.869
Clayton Graves: with the code that we had, whereas the other stuff required

981
01:07:13.260 --> 01:07:18.609
Clayton Graves: the other. The other stuff, either Spacey or the other one either had limitations

982
01:07:18.640 --> 01:07:21.840
Clayton Graves: or required a lot of coding to achieve the same result.

983
01:07:22.970 --> 01:07:24.310
Anthony Taylor: I agree with that

984
01:07:24.470 --> 01:07:26.230
Anthony Taylor: any other thoughts, King.

985
01:07:27.532 --> 01:07:33.440
Mason, Natalie: Can you go back to the Bert Tokenizer and walk through again.

986
01:07:33.440 --> 01:07:34.039
Anthony Taylor: At the top.

987
01:07:34.040 --> 01:07:37.200
Mason, Natalie: The one with the green bars.

988
01:07:37.843 --> 01:07:38.830
Mason, Natalie: And just.

989
01:07:38.830 --> 01:07:39.220
Anthony Taylor: Yes.

990
01:07:39.220 --> 01:07:40.999
Mason, Natalie: Run that down for me again.

991
01:07:41.830 --> 01:07:44.410
Anthony Taylor: I wonder you guys see one that's kind of weird. Look at that.

992
01:07:50.510 --> 01:07:51.399
Baro, Sonja: Cpa, yeah.

993
01:07:51.400 --> 01:07:51.780
Anthony Taylor: Weird.

994
01:07:51.780 --> 01:07:52.370
Baro, Sonja: Hey!

995
01:07:52.830 --> 01:07:55.359
Anthony Taylor: Yeah, yeah, that's so weird.

996
01:07:55.510 --> 01:07:57.750
Anthony Taylor: We don't know why it just did.

997
01:07:57.750 --> 01:07:59.399
michael mcpherson: CAFA.

998
01:07:59.400 --> 01:08:00.200
Baro, Sonja: Hi!

999
01:08:00.930 --> 01:08:03.420
Baro, Sonja: What are the parentheses, too?

1000
01:08:04.500 --> 01:08:07.500
Anthony Taylor: Yeah, why did it? Well, the parentheses were included in here.

1001
01:08:08.293 --> 01:08:08.920
Baro, Sonja: So. Yeah.

1002
01:08:08.920 --> 01:08:13.320
Anthony Taylor: It left them in there. But alright! Let me go back up to the the import.

1003
01:08:13.490 --> 01:08:16.059
Anthony Taylor: Is this what you were talking about, Mary?

1004
01:08:16.740 --> 01:08:17.470
Mason, Natalie: Yeah.

1005
01:08:17.740 --> 01:08:21.200
Mason, Natalie: just the bars is, what is what is this showing us?

1006
01:08:21.970 --> 01:08:23.200
Mason, Natalie: Oh, just then loaded.

1007
01:08:23.200 --> 01:08:24.090
Anthony Taylor: And everything

1008
01:08:24.380 --> 01:08:34.300
Anthony Taylor: right? So this from Pre train, this goes out to the hugging face website and actually downloads all of the weights where you're gonna see this in every activity today.

1009
01:08:34.500 --> 01:08:37.260
Anthony Taylor: and some of them are, gonna take a while. Some of them are pretty big.

1010
01:08:38.050 --> 01:08:38.510
Mason, Natalie: Okay.

1011
01:08:38.510 --> 01:08:41.129
Anthony Taylor: Okay, so that's good. Good question.

1012
01:08:44.310 --> 01:08:49.860
Anthony Taylor: so there is another one. I don't think we have an activity to show it. Hold on.

1013
01:08:51.569 --> 01:08:57.769
Anthony Taylor: No. So there is another type of tokenization called character tokenization. Actually.

1014
01:08:58.000 --> 01:09:00.029
Anthony Taylor: me see if maybe it's in the slideshow.

1015
01:09:02.550 --> 01:09:04.830
Anthony Taylor: Hasn't been much in the sideshow.

1016
01:09:05.772 --> 01:09:09.430
Anthony Taylor: Nope, that's embeddings. That's not there. We're not there yet. Okay.

1017
01:09:11.149 --> 01:09:16.430
Anthony Taylor: so there's another one character tokenization. It's exactly like it sounds.

1018
01:09:16.529 --> 01:09:20.320
Anthony Taylor: It's literally taking every word in every sentence

1019
01:09:20.609 --> 01:09:22.499
Anthony Taylor: and splitting it into letters.

1020
01:09:22.680 --> 01:09:25.059
Anthony Taylor: Llms, do not use this.

1021
01:09:25.490 --> 01:09:26.470
Anthony Taylor: Okay.

1022
01:09:27.073 --> 01:09:30.439
Anthony Taylor: it's useful for like spelling and stuff like that.

1023
01:09:31.450 --> 01:09:32.910
Anthony Taylor: Okay, okay,

1024
01:09:34.240 --> 01:09:36.500
Anthony Taylor: I'm trying to think of what else it could be used for.

1025
01:09:37.859 --> 01:09:48.229
Anthony Taylor: Yeah, anytime, when you just really need fine grained, like, I need to be able to see the tenses of a word and stuff like that. Alright, yeah, son.

1026
01:09:48.640 --> 01:09:54.379
Baro, Sonja: So I just wanna finish the summary of the different tokenizers cause you're asking.

1027
01:09:54.380 --> 01:09:56.679
Anthony Taylor: Yeah. Oh, yeah, sorry. That's right.

1028
01:09:56.680 --> 01:09:57.340
Baro, Sonja: No, that's okay.

1029
01:09:57.340 --> 01:09:58.739
Anthony Taylor: Go ahead, what other things.

1030
01:09:59.180 --> 01:10:00.969
Baro, Sonja: Well, and so

1031
01:10:01.790 --> 01:10:07.259
Baro, Sonja: so Bert Tokenizer could do subwords right.

1032
01:10:07.260 --> 01:10:08.910
Anthony Taylor: In one line of code.

1033
01:10:08.910 --> 01:10:10.989
Baro, Sonja: And one line of code versus.

1034
01:10:10.990 --> 01:10:12.030
Anthony Taylor: Did the whole thing.

1035
01:10:12.030 --> 01:10:16.960
Baro, Sonja: The Nltk and the Spacey. We had to go step by step

1036
01:10:17.480 --> 01:10:18.389
Baro, Sonja: to get there.

1037
01:10:18.390 --> 01:10:21.770
Anthony Taylor: Exactly exactly, was a little better

1038
01:10:22.020 --> 01:10:26.460
Anthony Taylor: other than than the nltk. So remember. And and I want you, I I

1039
01:10:26.920 --> 01:10:34.589
Anthony Taylor: not gonna say it bugs me, but because it makes sense. Why they use an Ltk and Ltk is like the tool you use when you're learning

1040
01:10:35.450 --> 01:10:45.149
Anthony Taylor: there are. You could totally do production level stuff within Ltk, but it's not. I mean, Spacey is like a production level in Lp tool.

1041
01:10:45.640 --> 01:10:53.169
Anthony Taylor: So Spacey would be better than in Ltk. But the transformers you're going to get from hugging face. They do it like in one step.

1042
01:10:53.230 --> 01:10:54.900
Anthony Taylor: they're going to be way better.

1043
01:10:55.890 --> 01:10:57.199
Anthony Taylor: Okay, yeah. I'm here.

1044
01:10:58.510 --> 01:11:02.629
Meredith McCanse (she/her): For the bird that we did in this exercise, though we leveraged

1045
01:11:02.880 --> 01:11:09.649
Meredith McCanse (she/her): data that had already gone through steps with the other ones. Could Bert have handled it all in one step, and they just this is just the way.

1046
01:11:10.650 --> 01:11:11.260
Meredith McCanse (she/her): Set it up.

1047
01:11:11.260 --> 01:11:11.960
Anthony Taylor: Let's let's

1048
01:11:13.730 --> 01:11:17.279
Anthony Taylor: let's try it. So we did. Tokenizer

1049
01:11:17.600 --> 01:11:19.300
Anthony Taylor: article.

1050
01:11:19.730 --> 01:11:24.830
Anthony Taylor: Let's just try to pass an article. I don't know. I have not tried this flying by the seat of our fancy.

1051
01:11:25.240 --> 01:11:27.250
Anthony Taylor: So let's pass in

1052
01:11:28.360 --> 01:11:29.350
Anthony Taylor: Article

1053
01:11:32.270 --> 01:11:33.230
Anthony Taylor: Boom!

1054
01:11:35.570 --> 01:11:36.749
Anthony Taylor: Looks like it, did it?

1055
01:11:37.590 --> 01:11:40.680
Meredith McCanse (she/her): Is that the Bert step, though I thought the Bert was down at the bottom.

1056
01:11:41.140 --> 01:11:43.560
Anthony Taylor: No, no, remember, this was the first time we did it.

1057
01:11:45.330 --> 01:11:46.100
Meredith McCanse (she/her): Oh.

1058
01:11:46.460 --> 01:11:55.079
Meredith McCanse (she/her): what about? At the very oh, I see! What about, at the very cause, the very last step where we did tokenize the first sentence using vert based.

1059
01:12:00.560 --> 01:12:07.089
Anthony Taylor: So this one we pass. Yeah. But see this one. We only passed in the sentence that space or passed in a sentence

1060
01:12:07.100 --> 01:12:09.679
Anthony Taylor: that one I did up above. We did the whole.

1061
01:12:09.680 --> 01:12:11.610
Meredith McCanse (she/her): Whole. Article. Okay.

1062
01:12:12.110 --> 01:12:12.790
Anthony Taylor: Yeah.

1063
01:12:13.230 --> 01:12:15.900
Anthony Taylor: so so yeah, Burke can handle it. No problem.

1064
01:12:17.730 --> 01:12:19.309
Anthony Taylor: So pretty cool

1065
01:12:19.610 --> 01:12:21.889
Anthony Taylor: fun. Everybody like that.

1066
01:12:22.360 --> 01:12:25.929
Anthony Taylor: I enjoy doing that. Just so, you know this is hugging face.

1067
01:12:27.970 --> 01:12:32.659
Anthony Taylor: If you're interested, I do recommend it's free, does not cost anything.

1068
01:12:32.800 --> 01:12:38.290
Anthony Taylor: Go to hugging face, create an account there is pricing, but this is like, if you want your own like Workspace.

1069
01:12:38.480 --> 01:12:39.680
Anthony Taylor: we don't need that

1070
01:12:40.118 --> 01:12:43.400
Anthony Taylor: so you can go to hugging face, and then you

1071
01:12:43.570 --> 01:12:46.520
Anthony Taylor: click on the little blue thing

1072
01:12:46.650 --> 01:12:48.370
Anthony Taylor: go to settings.

1073
01:12:49.590 --> 01:12:52.430
Anthony Taylor: access tokens, and you can create an access token.

1074
01:12:52.730 --> 01:12:55.889
Anthony Taylor: We don't need it today, but just know you can make one.

1075
01:12:55.970 --> 01:12:58.349
Anthony Taylor: If you wanted to put it in your Google.

1076
01:12:58.890 --> 01:13:03.039
Anthony Taylor: you can actually put it in under the little key here

1077
01:13:03.310 --> 01:13:05.600
Anthony Taylor: and then make it accessible to your notebook.

1078
01:13:07.580 --> 01:13:08.450
Anthony Taylor: Okay.

1079
01:13:09.560 --> 01:13:13.530
Masarirambi, Rodney: Actually quick question on hugging phase. Say, if I wanted to

1080
01:13:13.900 --> 01:13:15.340
Masarirambi, Rodney: say if I had.

1081
01:13:16.260 --> 01:13:19.849
Masarirambi, Rodney: for example, an ios app or an android app, that I wanted to

1082
01:13:20.030 --> 01:13:26.190
Masarirambi, Rodney: work on using it. Is it under that creative common use, license or out, has paid for them.

1083
01:13:26.190 --> 01:13:27.120
Anthony Taylor: Different, different.

1084
01:13:27.120 --> 01:13:28.200
Masarirambi, Rodney: Be free for them.

1085
01:13:29.570 --> 01:13:33.750
Anthony Taylor: Most of the of them on here are.

1086
01:13:34.318 --> 01:13:37.450
Anthony Taylor: are are open, but you do want to check

1087
01:13:37.790 --> 01:13:39.299
Anthony Taylor: alright. Oh, what's this one

1088
01:13:39.890 --> 01:13:44.959
Anthony Taylor: now? I will tell you one of the things that's kind of fun about hugging face. Is these spaces?

1089
01:13:45.430 --> 01:13:46.560
Anthony Taylor: Okay?

1090
01:13:59.930 --> 01:14:02.000
Clayton Graves: That there looks like a comic book panel.

1091
01:14:03.010 --> 01:14:09.790
Anthony Taylor: And that's that's what it is. It's it's actually gonna generate. I I we probably won't wait for it. I'll let it run in the background.

1092
01:14:09.830 --> 01:14:13.180
Anthony Taylor: But these spaces they're actually like

1093
01:14:13.470 --> 01:14:18.889
Anthony Taylor: people are using the AI models that are here on hugging face. And they're giving like little fun

1094
01:14:20.050 --> 01:14:23.480
Anthony Taylor: things to to to do, and this is going to generate.

1095
01:14:25.280 --> 01:14:28.350
Anthony Taylor: I'm not. I'm not sure how that

1096
01:14:28.880 --> 01:14:30.010
Anthony Taylor: plies.

1097
01:14:32.400 --> 01:14:34.883
Baro, Sonja: Looks like Prince Valiant, with all the text.

1098
01:14:35.160 --> 01:14:36.190
Clayton Graves: I don't know that is trying.

1099
01:14:36.190 --> 01:14:37.270
Anthony Taylor: Yeah, if it.

1100
01:14:37.660 --> 01:14:38.049
Clayton Graves: Dudes.

1101
01:14:38.050 --> 01:14:41.690
Anthony Taylor: Did say, super AI teacher. I didn't say it was a male.

1102
01:14:42.410 --> 01:14:43.420
Anthony Taylor: so.

1103
01:14:43.860 --> 01:14:45.290
Baro, Sonja: It's 1,950.

1104
01:14:48.380 --> 01:14:49.020
Baro, Sonja: let's interrupt.

1105
01:14:49.020 --> 01:14:50.439
Anthony Taylor: I'll hear that. Let's

1106
01:14:51.440 --> 01:14:53.730
Anthony Taylor: see, that's pretty cool. I kind of think that's cool.

1107
01:14:53.800 --> 01:14:55.233
Anthony Taylor: But anyway,

1108
01:14:57.640 --> 01:15:00.910
Anthony Taylor: yeah, that is fun. Oh, yeah. And I did. I picked

1109
01:15:01.070 --> 01:15:01.810
Anthony Taylor: okay.

1110
01:15:02.220 --> 01:15:04.650
Anthony Taylor: anyway. So you guys can see Ivy face.

1111
01:15:04.750 --> 01:15:08.620
Anthony Taylor: You guys could lose yourself here for days. Trust me on this.

1112
01:15:08.810 --> 01:15:12.159
Anthony Taylor: because all of these spaces. These are all

1113
01:15:12.340 --> 01:15:15.429
Anthony Taylor: there's just I mean, you can go

1114
01:15:15.500 --> 01:15:16.860
Anthony Taylor: click spaces.

1115
01:15:17.080 --> 01:15:19.040
Anthony Taylor: And there's just

1116
01:15:19.910 --> 01:15:21.040
Anthony Taylor: it's nuts.

1117
01:15:21.210 --> 01:15:21.910
Anthony Taylor: Okay.

1118
01:15:22.600 --> 01:15:23.809
Anthony Taylor: Aye, aye.

1119
01:15:24.000 --> 01:15:30.319
Anthony Taylor: okay, enough of that. That's inside. We don't need that aside at this moment. Please do not play with it while I'm talking.

1120
01:15:31.440 --> 01:15:33.100
Anthony Taylor: I know you're thinking about.

1121
01:15:33.480 --> 01:15:36.397
Anthony Taylor: Okay. So

1122
01:15:38.590 --> 01:15:40.810
Anthony Taylor: did we do? Was that exercise, too?

1123
01:15:42.210 --> 01:15:43.210
Anthony Taylor: Yeah.

1124
01:15:46.240 --> 01:15:49.330
Anthony Taylor: think we can get through this next one and then do our break.

1125
01:15:50.730 --> 01:15:56.270
Anthony Taylor: Yeah. Oh, we're practically almost on time, if you can imagine that. Not even sure how that's possible.

1126
01:15:56.420 --> 01:15:57.310
Anthony Taylor: Okay.

1127
01:15:57.550 --> 01:15:59.680
Anthony Taylor: upload the next notebook.

1128
01:16:02.766 --> 01:16:05.100
Anthony Taylor: Activity 3.

1129
01:16:06.520 --> 01:16:09.030
Anthony Taylor: Oh, is everyone do so not to solve.

1130
01:16:09.090 --> 01:16:11.090
Anthony Taylor: I didn't give you guys to sell dude. I

1131
01:16:12.680 --> 01:16:14.039
Anthony Taylor: I will. Okay

1132
01:16:15.410 --> 01:16:19.230
Anthony Taylor: for tomorrow. I'm going to give you guys style.

1133
01:16:20.040 --> 01:16:26.910
Anthony Taylor: Listen to your instructor. I have no idea you might get like. There's a couple of instructors that have some really great

1134
01:16:27.360 --> 01:16:29.120
Anthony Taylor: current AI stuff.

1135
01:16:29.220 --> 01:16:37.000
Anthony Taylor: So you know you, you might get somebody really awesome with a different kind of experience than I have. So please

1136
01:16:37.250 --> 01:16:38.470
Anthony Taylor: enjoy.

1137
01:16:41.340 --> 01:16:43.409
Anthony Taylor: Okay, so for this one.

1138
01:16:43.450 --> 01:16:45.960
Clayton Graves: Like better than you. Is that possible?

1139
01:16:45.960 --> 01:16:48.319
Anthony Taylor: Dude any anything is possible.

1140
01:16:49.250 --> 01:16:53.349
Anthony Taylor: anything is possible. I will tell you, as the

1141
01:16:53.470 --> 01:16:55.430
Anthony Taylor: quote, lead

1142
01:16:55.490 --> 01:16:59.050
Anthony Taylor: AI instructor and data analytics instructor, one of them.

1143
01:16:59.517 --> 01:17:04.660
Anthony Taylor: There are a lot of very good instructors, at least from a knowledge standpoint

1144
01:17:06.350 --> 01:17:08.740
Anthony Taylor: that are doing some really cool stuff. So

1145
01:17:08.970 --> 01:17:14.950
Anthony Taylor: you never know. You never know. I don't know if you'll get the energy. I got pretty much. Everyone says they can't do that, but

1146
01:17:15.240 --> 01:17:16.190
Anthony Taylor: never know.

1147
01:17:16.560 --> 01:17:17.550
Anthony Taylor: Yes, Mike.

1148
01:17:18.320 --> 01:17:19.899
michael mcpherson: You said activity, too. Right?

1149
01:17:20.880 --> 01:17:24.269
Anthony Taylor: No, we just did activity 2. So activity 3. Token

1150
01:17:24.890 --> 01:17:27.669
Anthony Taylor: token to indices. Review.

1151
01:17:28.370 --> 01:17:29.340
Anthony Taylor: Is that right?

1152
01:17:32.010 --> 01:17:33.739
Anthony Taylor: I'm going to bring it up 1 s.

1153
01:17:36.310 --> 01:17:38.540
Anthony Taylor: Yes. Token to indices.

1154
01:17:39.190 --> 01:17:40.520
Anthony Taylor: Review.

1155
01:17:44.320 --> 01:17:48.690
Anthony Taylor: okay. Oh, oh, this is cool. Okay, there's a reason why we're doing this

1156
01:17:49.110 --> 01:17:53.110
Anthony Taylor: alright. So we're gonna do curios. Pre-processing text.

1157
01:17:53.390 --> 01:17:58.859
Anthony Taylor: okay, we've got some sentences here. I love my dog, love my family. My dog is a lab.

1158
01:18:00.430 --> 01:18:06.120
Anthony Taylor: Okay. My family was made in a lab that's weird, but it could also be accurate.

1159
01:18:07.180 --> 01:18:09.350
Anthony Taylor: At least 2 of my children were.

1160
01:18:09.900 --> 01:18:11.809
Anthony Taylor: So it is possible.

1161
01:18:13.025 --> 01:18:15.079
Anthony Taylor: We're gonna tokenize.

1162
01:18:15.780 --> 01:18:16.750
Anthony Taylor: Boo.

1163
01:18:17.730 --> 01:18:21.390
Anthony Taylor: Okay. So here we're going to

1164
01:18:24.740 --> 01:18:26.120
Anthony Taylor: print out

1165
01:18:26.810 --> 01:18:35.710
Anthony Taylor: our word index. And the good news is we've already done it so we can just do tokenizer dot word underscore index

1166
01:18:37.470 --> 01:18:43.650
Anthony Taylor: boom! Boom! Okay, I I didn't. I didn't mean for that to happen. Word underscore

1167
01:18:44.730 --> 01:18:45.910
Anthony Taylor: index.

1168
01:18:46.960 --> 01:18:48.900
Anthony Taylor: And then we can see our counts

1169
01:18:48.980 --> 01:18:50.100
Anthony Taylor: cool.

1170
01:18:51.790 --> 01:18:55.370
Anthony Taylor: we can create sequences.

1171
01:18:55.800 --> 01:19:01.129
Anthony Taylor: And this is. This is playing into something a little bit more important here in a second.

1172
01:19:01.620 --> 01:19:04.630
Anthony Taylor: So tokenizer dot text.

1173
01:19:05.550 --> 01:19:07.550
Anthony Taylor: underscore 2

1174
01:19:08.160 --> 01:19:09.550
Anthony Taylor: sequences.

1175
01:19:10.490 --> 01:19:13.519
Anthony Taylor: and then pass in the sentences from above

1176
01:19:16.870 --> 01:19:17.930
Anthony Taylor: and run that

1177
01:19:19.390 --> 01:19:22.800
Anthony Taylor: oh, and output it. So print sequences.

1178
01:19:27.560 --> 01:19:33.390
Anthony Taylor: And then last, but not least. Okay, so no, not last, but not least. So

1179
01:19:34.950 --> 01:19:37.210
Anthony Taylor: this. So these are not

1180
01:19:37.570 --> 01:19:38.860
Anthony Taylor: these counts.

1181
01:19:39.650 --> 01:19:46.199
Anthony Taylor: Okay, these are, and these are not counts. Sorry. These are indexes for these words.

1182
01:19:47.360 --> 01:19:52.310
Anthony Taylor: Think of it like, if the words are in a list. These are numbers. Represent that you know their location.

1183
01:19:52.940 --> 01:19:53.320
Clayton Graves: So my.

1184
01:19:53.320 --> 01:19:54.320
Anthony Taylor: Not what they are.

1185
01:19:54.510 --> 01:19:57.939
Clayton Graves: Bias. One is 2. Love is 3. Dark is 4.

1186
01:19:58.250 --> 01:19:59.410
Anthony Taylor: And you can see it.

1187
01:19:59.470 --> 01:20:06.729
Anthony Taylor: you and I, because these are so simple, can look at this and go look. Okay. It says, I. 3 is

1188
01:20:07.320 --> 01:20:11.180
Anthony Taylor: love. One is my 4 is dot

1189
01:20:12.320 --> 01:20:15.130
Anthony Taylor: okay? And then the next one, and then the next one.

1190
01:20:15.800 --> 01:20:17.500
Anthony Taylor: Okay, so

1191
01:20:18.430 --> 01:20:26.079
Anthony Taylor: these. So this is good. So and all I want to show you is that we can recreate our sentences

1192
01:20:26.560 --> 01:20:28.920
Anthony Taylor: from these numbers. Does everyone agree.

1193
01:20:29.390 --> 01:20:33.490
Clayton Graves: Yeah, it's almost like one of those oval team decoder rings from back in the day.

1194
01:20:33.490 --> 01:20:39.900
Anthony Taylor: Exactly. So we're gonna do this using our tool. So we're gonna say for

1195
01:20:40.210 --> 01:20:45.290
Anthony Taylor: sequence. So we're gonna do each sequence, not sequence. I don't know what I was typing

1196
01:20:45.900 --> 01:20:49.550
Anthony Taylor: sequence in tokenizer

1197
01:20:50.500 --> 01:20:54.410
Anthony Taylor: dot sequence to text generator.

1198
01:20:55.330 --> 01:20:56.690
Anthony Taylor: This one

1199
01:20:56.990 --> 01:20:58.080
Anthony Taylor: right there.

1200
01:20:58.390 --> 01:20:59.430
Anthony Taylor: Okay.

1201
01:21:00.850 --> 01:21:03.119
Anthony Taylor: pass in sequences.

1202
01:21:05.490 --> 01:21:06.460
Anthony Taylor: Aye.

1203
01:21:06.740 --> 01:21:10.459
Anthony Taylor: and then, so that's the for loop. And then at the beginning of that.

1204
01:21:10.550 --> 01:21:15.120
Anthony Taylor: what do we want from sequence? We basically just wanted to output the sequence.

1205
01:21:15.500 --> 01:21:16.380
Anthony Taylor: Okay?

1206
01:21:17.060 --> 01:21:20.130
Anthony Taylor: And that's it. So we can run that.

1207
01:21:20.680 --> 01:21:22.999
Anthony Taylor: And we can see we've rebuilt

1208
01:21:23.490 --> 01:21:25.469
Anthony Taylor: our sentences.

1209
01:21:25.840 --> 01:21:27.650
Anthony Taylor: Okay. Now, for the moment.

1210
01:21:27.700 --> 01:21:29.630
Anthony Taylor: this was like, Okay, that's cool.

1211
01:21:29.660 --> 01:21:34.209
Anthony Taylor: All right, we're going to get way more into this, and we're going to reuse this stuff in a little bit

1212
01:21:34.350 --> 01:21:35.940
Anthony Taylor: after break.

1213
01:21:36.130 --> 01:21:37.190
Anthony Taylor: Yes, Tanya.

1214
01:21:37.450 --> 01:21:39.159
Raugewitz, Tania: Oh, I know I didn't want to interrupt you.

1215
01:21:39.560 --> 01:21:40.489
Anthony Taylor: So you're okay.

1216
01:21:40.740 --> 01:21:46.549
Raugewitz, Tania: Okay, well, I maybe you already covered this. I'm not sure. But so why isn't I?

1217
01:21:47.353 --> 01:21:49.860
Raugewitz, Tania: Number one? And why is it? Number 2?

1218
01:21:51.330 --> 01:21:54.770
Anthony Taylor: Well, so I is not number one. I is

1219
01:21:54.880 --> 01:21:56.110
Anthony Taylor: number 2.

1220
01:21:56.110 --> 01:21:56.950
Raugewitz, Tania: Yeah, why?

1221
01:21:56.950 --> 01:21:59.580
Anthony Taylor: Right. And oh, it's just the way it's sequenced.

1222
01:21:59.970 --> 01:22:00.760
Clayton Graves: Is it random?

1223
01:22:00.760 --> 01:22:01.679
Anthony Taylor: Here's no reason

1224
01:22:03.202 --> 01:22:05.970
Anthony Taylor: tokenizer fits on text.

1225
01:22:06.560 --> 01:22:09.189
Anthony Taylor: No, it's just this is how it did

1226
01:22:09.820 --> 01:22:13.139
Anthony Taylor: right. There's no I mean.

1227
01:22:15.290 --> 01:22:19.169
Anthony Taylor: I suppose we could look into it. My guess is.

1228
01:22:21.070 --> 01:22:24.030
Raugewitz, Tania: And so we really don't need to know why it just is what it is.

1229
01:22:24.640 --> 01:22:26.210
Anthony Taylor: It's gonna

1230
01:22:26.480 --> 01:22:31.379
Anthony Taylor: here's what's really important. I'm not. Gonna I mean, I love this past, and I think it's great.

1231
01:22:31.410 --> 01:22:34.579
Anthony Taylor: Anybody wants to look it up can. But the.

1232
01:22:34.760 --> 01:22:35.309
Raugewitz, Tania: The matter.

1233
01:22:35.310 --> 01:22:36.260
Anthony Taylor: It's not.

1234
01:22:36.630 --> 01:22:37.760
Anthony Taylor: it's it's

1235
01:22:37.790 --> 01:22:42.590
Anthony Taylor: probably not important to this conversation. However.

1236
01:22:42.780 --> 01:22:45.110
Anthony Taylor: I love curiosity, you never know.

1237
01:22:45.250 --> 01:22:46.660
Anthony Taylor: And 2,

1238
01:22:46.850 --> 01:22:48.469
Anthony Taylor: what's important

1239
01:22:48.590 --> 01:22:49.720
Anthony Taylor: is that

1240
01:22:49.970 --> 01:22:55.600
Anthony Taylor: this tokenizer sets the into it could have done it backwards if you've done it, anyway.

1241
01:22:55.890 --> 01:23:01.969
Anthony Taylor: as long as it has this mapping and this information, it can recreate the date.

1242
01:23:02.960 --> 01:23:12.329
Anthony Taylor: Okay, these could have been random numbers. It makes no difference as long as it stores them in the tokenizer, so that it can recreate the data.

1243
01:23:12.520 --> 01:23:13.240
Raugewitz, Tania: Okay.

1244
01:23:13.240 --> 01:23:15.769
Anthony Taylor: Okay, but I don't. I don't have an answer on

1245
01:23:16.080 --> 01:23:19.260
Anthony Taylor: how it comes up. I mean, my first guess

1246
01:23:19.290 --> 01:23:22.760
Anthony Taylor: was mine shows up times.

1247
01:23:23.250 --> 01:23:25.580
Anthony Taylor: I shows up 2 times.

1248
01:23:26.010 --> 01:23:28.389
Anthony Taylor: Love shows up 2 times.

1249
01:23:28.940 --> 01:23:31.340
Anthony Taylor: Dog shows up 2 times.

1250
01:23:31.960 --> 01:23:33.840
Anthony Taylor: Family is once

1251
01:23:35.130 --> 01:23:38.629
Anthony Taylor: is, is once A is once

1252
01:23:38.720 --> 01:23:40.210
Anthony Taylor: lab is once

1253
01:23:40.540 --> 01:23:46.300
Anthony Taylor: so my guess is, it's doing them in order of how often they show up.

1254
01:23:46.300 --> 01:23:52.450
Clayton Graves: There are 3 ways. There are 3 ways of assigning an index. There's the alphabetical order.

1255
01:23:52.570 --> 01:23:56.369
Clayton Graves: There's frequency order like you just saw. And then there's random.

1256
01:23:57.390 --> 01:24:00.760
Anthony Taylor: And that's what our tokenizer does, or just what any tokenizer.

1257
01:24:00.760 --> 01:24:01.900
Raugewitz, Tania: Are the 3 options.

1258
01:24:01.900 --> 01:24:03.780
Clayton Graves: It just says any tokenizer.

1259
01:24:04.220 --> 01:24:05.440
Anthony Taylor: Okay, yeah.

1260
01:24:05.500 --> 01:24:10.459
Anthony Taylor: So in this case, it it does appear to be by frequency, makes sense

1261
01:24:10.920 --> 01:24:18.579
Anthony Taylor: all right. But again, no matter how it does it? What matters is is this is basically the Dakota ring, as Blake put it

1262
01:24:18.800 --> 01:24:19.920
Anthony Taylor: for this.

1263
01:24:21.850 --> 01:24:24.609
Anthony Taylor: Okay, so that it can get us this again

1264
01:24:24.790 --> 01:24:29.070
Anthony Taylor: because we can't do anything if we can't make it into words again.

1265
01:24:30.640 --> 01:24:33.279
Anthony Taylor: right? But the computer needs it to be numbers.

1266
01:24:33.600 --> 01:24:35.340
Anthony Taylor: So that's how this lesson was.

1267
01:24:35.450 --> 01:24:38.929
Anthony Taylor: That's all this was so let me make sure I covered everything in there.

1268
01:24:43.210 --> 01:24:45.929
Anthony Taylor: that looks good. That looks good.

1269
01:24:51.600 --> 01:24:56.379
Anthony Taylor: I really wanna add, I really wanna add your question to our our thing so that

1270
01:24:56.560 --> 01:24:57.489
Anthony Taylor: they know that.

1271
01:25:02.370 --> 01:25:04.729
Anthony Taylor: oh, okay, this is important.

1272
01:25:04.850 --> 01:25:07.559
Anthony Taylor: So this right here

1273
01:25:08.260 --> 01:25:13.760
Anthony Taylor: is this list of integers is in array. And how can they be used? Well, we haven't told you.

1274
01:25:13.880 --> 01:25:15.749
Anthony Taylor: but this term is important.

1275
01:25:16.980 --> 01:25:18.390
Anthony Taylor: It's an embedding.

1276
01:25:20.730 --> 01:25:22.700
Anthony Taylor: Okay, embedding.

1277
01:25:23.160 --> 01:25:28.229
Anthony Taylor: which we can use with machine learning models predict the next word in a sequence.

1278
01:25:28.880 --> 01:25:30.429
Anthony Taylor: Okay, like we did

1279
01:25:31.650 --> 01:25:33.530
Anthony Taylor: last class with the R. And N,

1280
01:25:34.370 --> 01:25:37.099
Anthony Taylor: right? It did. It wasn't great.

1281
01:25:37.240 --> 01:25:39.260
Anthony Taylor: This is going to be better when we're all done.

1282
01:25:39.700 --> 01:25:40.510
Anthony Taylor: Chair.

1283
01:25:41.956 --> 01:25:43.630
Anthony Taylor: That's it.

1284
01:25:45.070 --> 01:25:49.939
Anthony Taylor: So all we really did here is create the Ids, which is what we wanted. These Ids are going to

1285
01:25:49.950 --> 01:25:53.789
Anthony Taylor: help us with the encoding and decoding of our transformer after break.

1286
01:25:55.940 --> 01:25:57.089
Anthony Taylor: And that's it.

1287
01:25:57.180 --> 01:25:59.949
Anthony Taylor: Any questions about what we've done so far.

1288
01:26:01.360 --> 01:26:03.209
Anthony Taylor: we're doing really good.

1289
01:26:04.280 --> 01:26:06.260
Masarirambi, Rodney: Kind of feels like this should be more.

1290
01:26:07.830 --> 01:26:11.800
Anthony Taylor: I know, don't you, dog? Got it? Just I just want it to be more.

1291
01:26:11.870 --> 01:26:15.270
Anthony Taylor: Oh, what? Oh, here's a slide that actually goes with what we just did.

1292
01:26:15.450 --> 01:26:16.760
Anthony Taylor: Okay, but not yet.

1293
01:26:16.920 --> 01:26:18.299
Anthony Taylor: That's the one after break

1294
01:26:18.630 --> 01:26:23.340
Anthony Taylor: alright. So let's go on break. Come on back at now. 8, 20,

1295
01:26:23.410 --> 01:26:25.220
Anthony Taylor: and I'll see you there.

1296
01:26:25.730 --> 01:26:26.810
Baro, Sonja: Anthony.

1297
01:26:27.150 --> 01:26:28.020
Anthony Taylor: Yes.

1298
01:26:28.020 --> 01:26:32.029
Baro, Sonja: Can I show you an error that I had in Colab.

1299
01:26:32.940 --> 01:26:33.840
Anthony Taylor: Course.

1300
01:26:33.970 --> 01:26:36.340
Baro, Sonja: And that way don't hold up everybody.

1301
01:26:38.465 --> 01:26:40.539
Baro, Sonja: So I don't know if you'll

1302
01:26:40.570 --> 01:26:52.950
Baro, Sonja: remember when I are you guys. Yeah, you're seeing it in office hours. I mentioned that sometimes I would put in something in colab, and then it would error out, and if I just redid it.

1303
01:26:53.630 --> 01:26:58.669
Baro, Sonja: it would it would work. This is an example. It just happened to me here, so.

1304
01:26:58.670 --> 01:27:00.940
Anthony Taylor: Hold on! It's you got you spelled it wrong.

1305
01:27:01.490 --> 01:27:01.810
Baro, Sonja: Where.

1306
01:27:01.810 --> 01:27:05.160
Anthony Taylor: So sequences sequences to text.

1307
01:27:07.400 --> 01:27:08.720
Anthony Taylor: There's an S at the end of.

1308
01:27:08.720 --> 01:27:11.940
Baro, Sonja: Oh, thank you.

1309
01:27:11.940 --> 01:27:15.780
Anthony Taylor: And it looks like generator might be wrong too late.

1310
01:27:15.840 --> 01:27:18.000
Anthony Taylor: Nope, that's just some dust on my screen.

1311
01:27:18.420 --> 01:27:24.520
Baro, Sonja: No, okay. Here. I thought it was some is, of course it's me.

1312
01:27:24.650 --> 01:27:26.190
Baro, Sonja: Alright. Thank you.

1313
01:27:26.190 --> 01:27:26.829
Masarirambi, Rodney: I will. Yeah.

1314
01:27:26.830 --> 01:27:27.340
Anthony Taylor: Usually

1315
01:27:27.840 --> 01:27:28.610
Anthony Taylor: those.

1316
01:27:29.790 --> 01:27:36.360
Masarirambi, Rodney: I was looking to see if you did what I did, which is where I did sequence in the line above with a capital S. And then I was running.

1317
01:27:37.290 --> 01:27:40.571
Masarirambi, Rodney: What did I mess up? And then I was like no.

1318
01:27:40.870 --> 01:27:45.770
Baro, Sonja: I hate it, cause I'm like, no, there's nothing wrong here. And then, sure enough.

1319
01:27:48.120 --> 01:27:50.649
Anthony Taylor: Alright gang have a good break. Get back to it.

1320
01:27:51.260 --> 01:27:52.720
Anthony Taylor: Okay?

1321
01:27:53.540 --> 01:27:55.339
Anthony Taylor: So we're finishing today

1322
01:27:57.740 --> 01:28:00.409
Anthony Taylor: a hugging face. Now.

1323
01:28:00.700 --> 01:28:04.249
Anthony Taylor: it's interesting that they say, oh, whatever we're gonna do hugging face. Well, we can already do.

1324
01:28:04.380 --> 01:28:07.170
Anthony Taylor: But now we're gonna get a little more

1325
01:28:07.480 --> 01:28:09.129
Anthony Taylor: like way in the Hudson.

1326
01:28:09.750 --> 01:28:10.780
Anthony Taylor: Okay.

1327
01:28:13.510 --> 01:28:14.320
Anthony Taylor: Though.

1328
01:28:16.950 --> 01:28:21.290
Anthony Taylor: So the tokenizing tokenizers from hugging face they're going to give us.

1329
01:28:22.690 --> 01:28:28.730
Anthony Taylor: I mean, there's just so many of them you saw the one the Bert one. There's a lot to choose from

1330
01:28:28.920 --> 01:28:32.430
Anthony Taylor: again. Many of them have to do

1331
01:28:32.560 --> 01:28:34.480
Anthony Taylor: with the

1332
01:28:35.390 --> 01:28:36.720
Anthony Taylor: the right bottle

1333
01:28:36.950 --> 01:28:38.309
Anthony Taylor: that you're doing it with.

1334
01:28:38.999 --> 01:28:45.979
Anthony Taylor: And you just you know there's all kinds of stuff to do. Okay, it's a very good choice to use these when you're doing stuff.

1335
01:28:46.620 --> 01:28:47.480
Anthony Taylor: So

1336
01:28:48.030 --> 01:28:51.860
Anthony Taylor: embeddings embedding embeddings

1337
01:28:51.880 --> 01:28:54.459
Anthony Taylor: vector, embedding specifically

1338
01:28:54.600 --> 01:29:03.599
Anthony Taylor: so once the sequence of text has been converted to Ids using the tokenizer. The next step is to convert these Ids into

1339
01:29:03.820 --> 01:29:05.010
Anthony Taylor: embed it.

1340
01:29:05.240 --> 01:29:09.269
Anthony Taylor: They're vector representations of a text, speak with

1341
01:29:10.920 --> 01:29:16.910
Anthony Taylor: right that sequence captures their meaning, and can be used to perform in lp tasks.

1342
01:29:17.130 --> 01:29:18.090
Anthony Taylor: Alright.

1343
01:29:19.530 --> 01:29:24.830
Anthony Taylor: In the previous demonstration we showed you some embeddings. The embeddings were those vectors with the number?

1344
01:29:25.090 --> 01:29:28.829
Anthony Taylor: Remember, I gave us indexes, and then it gave us the number.

1345
01:29:29.770 --> 01:29:32.350
Anthony Taylor: They obviously have to be numeric.

1346
01:29:32.964 --> 01:29:38.509
Anthony Taylor: So one of the things it's going to do is it? Can now? And you can kind of see this here.

1347
01:29:39.410 --> 01:29:45.770
Anthony Taylor: Okay, am I sharing? Yeah, you could see this here. I mean some of the there's some similarities here, is there not

1348
01:29:47.740 --> 01:29:52.469
Anthony Taylor: the first 2 here? I mean, they're, you know. 3 of the 4 words are similar.

1349
01:29:53.850 --> 01:29:56.030
Anthony Taylor: Okay, so this is

1350
01:29:56.860 --> 01:30:00.119
Anthony Taylor: very rudimentary. But this is basically how

1351
01:30:00.340 --> 01:30:06.500
Anthony Taylor: these tools come up with a similarity thing. Now, why does why do we need a similarity check.

1352
01:30:07.820 --> 01:30:13.260
Anthony Taylor: What? What would be the purpose in, I mean, think, Chat Tpt, or any of the lens you've used so far.

1353
01:30:13.550 --> 01:30:15.639
Anthony Taylor: What do you think it's using similarity for.

1354
01:30:25.220 --> 01:30:28.260
Anthony Taylor: You said, you get it. It's yours.

1355
01:30:29.960 --> 01:30:31.160
Raugewitz, Tania: Because it's

1356
01:30:31.580 --> 01:30:34.170
Raugewitz, Tania: yep. I was. Gonna say, that's what I wanted to say. Pattern.

1357
01:30:35.220 --> 01:30:45.229
Anthony Taylor: There you go. Basically, yeah, it's basically searching. It's it's kinda like Google, right? Except for it's looking at the embeddings. And it's going, hey?

1358
01:30:45.810 --> 01:30:47.200
Anthony Taylor: What is like that?

1359
01:30:47.390 --> 01:30:49.169
Anthony Taylor: Right? What's similar to that?

1360
01:30:49.230 --> 01:30:54.270
Anthony Taylor: How can I find? So when you ask it a question, it takes your question.

1361
01:30:55.600 --> 01:30:56.810
Anthony Taylor: Okay.

1362
01:30:56.980 --> 01:31:01.690
Anthony Taylor: it encodes it embeds it, and then looks for

1363
01:31:02.060 --> 01:31:06.389
Anthony Taylor: answers that are similar to your question.

1364
01:31:07.680 --> 01:31:14.569
Anthony Taylor: And then that's how it comes up with the answer. So then it spits those out, those embedded tokens.

1365
01:31:14.710 --> 01:31:16.769
Anthony Taylor: It decodes them

1366
01:31:17.190 --> 01:31:19.790
Anthony Taylor: back into words and then

1367
01:31:19.850 --> 01:31:21.129
Anthony Taylor: presents them to you.

1368
01:31:22.050 --> 01:31:26.100
Anthony Taylor: Okay, so that's the idea behind

1369
01:31:26.210 --> 01:31:27.450
Anthony Taylor: embedding

1370
01:31:29.510 --> 01:31:36.580
Anthony Taylor: that way, you know what we're gonna get into similarity in just a minute. So let's go load exercise, for

1371
01:31:36.800 --> 01:31:38.090
Anthony Taylor: in Taco Lab.

1372
01:31:40.450 --> 01:31:41.280
Raugewitz, Tania: I think there are 2 numbers.

1373
01:31:41.280 --> 01:31:42.890
Clayton Graves: Are 2 notebooks.

1374
01:31:43.300 --> 01:31:45.880
Anthony Taylor: Yeah. Do the

1375
01:31:46.370 --> 01:31:47.780
Anthony Taylor: think it's the

1376
01:31:48.320 --> 01:31:49.820
Anthony Taylor: hold on. Let me go. Look.

1377
01:31:50.980 --> 01:31:53.480
Clayton Graves: So took this one, and a similarity, one.

1378
01:31:53.830 --> 01:31:55.059
Anthony Taylor: Token went first.

1379
01:31:55.250 --> 01:31:56.100
Anthony Taylor: Thank you.

1380
01:32:00.780 --> 01:32:07.290
Anthony Taylor: This is, oh, this is an instructor. Do? Okay? Good. Okay, good. Good. Good. So we're gonna do the sentence Tokenizer

1381
01:32:07.780 --> 01:32:08.690
Anthony Taylor: first.

1382
01:32:09.650 --> 01:32:12.040
Raugewitz, Tania: And do we need to do the pip install every time.

1383
01:32:12.040 --> 01:32:14.969
Anthony Taylor: Yeah, yeah, you have to do that. But it's not every time

1384
01:32:15.520 --> 01:32:16.560
Anthony Taylor: in Colette.

1385
01:32:18.250 --> 01:32:22.309
Anthony Taylor: Okay, so just uncomment and run that guy

1386
01:32:23.440 --> 01:32:28.460
Anthony Taylor: alright. So we're gonna get that installed. I think this is the one that takes a minute.

1387
01:32:30.170 --> 01:32:35.319
Anthony Taylor: Yeah, this is the one that takes a minute. Look. It's 737 makes

1388
01:32:37.980 --> 01:32:40.759
Anthony Taylor: alright. Now, earlier today, when I was doing this.

1389
01:32:40.900 --> 01:32:43.649
Anthony Taylor: the speed and co-lap was like

1390
01:32:44.010 --> 01:32:46.850
Anthony Taylor: 500 k. Per second was.

1391
01:32:47.880 --> 01:32:48.650
Anthony Taylor: so

1392
01:32:49.010 --> 01:32:51.199
Anthony Taylor: if it's that fast won't take too.

1393
01:32:59.310 --> 01:33:03.810
Anthony Taylor: So again you could tell. These are coming from hugging face. If you look up in the the

1394
01:33:04.190 --> 01:33:05.969
Anthony Taylor: the words here

1395
01:33:05.980 --> 01:33:09.080
Anthony Taylor: you can see hugging face, hub hugging, face. Hub!

1396
01:33:09.680 --> 01:33:13.249
Anthony Taylor: All of this is coming from the hucking thing.

1397
01:33:17.450 --> 01:33:19.489
michael mcpherson: Want to know where they came up with that name.

1398
01:33:20.540 --> 01:33:24.720
Anthony Taylor: I don't know it. It's funny cause I hadn't heard of them either, until

1399
01:33:25.660 --> 01:33:28.300
Anthony Taylor: probably February of this year.

1400
01:33:28.930 --> 01:33:33.329
Anthony Taylor: And then it was like and and again, it's one of those places. It's like Tiktok.

1401
01:33:33.510 --> 01:33:37.940
Anthony Taylor: You can get in there and you start looking around. And you're just gonna be lost for hours.

1402
01:33:38.150 --> 01:33:42.549
Anthony Taylor: because there's so much stuff in there that you can play with. It's it's kind of crazy.

1403
01:33:44.830 --> 01:33:52.562
Masarirambi, Rodney: Mike? You asked. Like you wonder how they came up? The name I mean, like, look at how we came up with non monotomic moms. It's.

1404
01:33:53.470 --> 01:34:00.300
Mason, Natalie: It was associated with a company called Hugging Face, Incorporated, known for their work with natural language, processing.

1405
01:34:01.610 --> 01:34:07.229
Anthony Taylor: Hey? Well, Natalie, that doesn't tell us how they came up with hugging face, how did the hugging face Incorporated come up with hugging face.

1406
01:34:07.230 --> 01:34:10.079
Mason, Natalie: Started as a Chatbot company.

1407
01:34:11.080 --> 01:34:13.797
Mason, Natalie: It's not giving an exact conversation.

1408
01:34:14.770 --> 01:34:27.160
Mason, Natalie: The name and smiling face logo are meant to reflect the company's focus on creating user friendly, accessible AI technology with an emphasis on community and open source collaboration.

1409
01:34:28.810 --> 01:34:29.530
Masarirambi, Rodney: I've got to wonder.

1410
01:34:29.530 --> 01:34:30.710
michael mcpherson: A salesperson.

1411
01:34:32.720 --> 01:34:33.400
michael mcpherson: Natalie has done.

1412
01:34:33.400 --> 01:34:35.620
Mason, Natalie: I've done marketing all my whole life.

1413
01:34:39.170 --> 01:34:39.540
Anthony Taylor: Okay.

1414
01:34:39.540 --> 01:34:41.280
Masarirambi, Rodney: Company, built itself.

1415
01:34:44.510 --> 01:34:47.930
Anthony Taylor: alright. So that finished for me. Hopefully it finished for all of you.

1416
01:34:48.210 --> 01:34:54.999
Anthony Taylor: We're also going to use a model called all Mini, LML. 6. B. 2.

1417
01:34:56.090 --> 01:34:59.620
Anthony Taylor: Okay, this is just a model. It's a pre-trained model

1418
01:34:59.630 --> 01:35:01.040
Anthony Taylor: for embedding.

1419
01:35:01.440 --> 01:35:04.779
Anthony Taylor: Okay, if you went to hiding page you could look it up, in fact.

1420
01:35:05.130 --> 01:35:06.569
Anthony Taylor: just to show you

1421
01:35:12.480 --> 01:35:13.620
Anthony Taylor: there. It is.

1422
01:35:14.080 --> 01:35:23.110
Anthony Taylor: Okay. This is a sentence transformer model, and math sends us a paragraph to 384 dimensional dense vector space

1423
01:35:23.180 --> 01:35:27.349
Anthony Taylor: and can be used for tasks like clustering, clustering, or semantic search

1424
01:35:27.520 --> 01:35:28.929
Anthony Taylor: shows you how to use it.

1425
01:35:30.130 --> 01:35:31.190
Anthony Taylor: It's pretty cool.

1426
01:35:31.530 --> 01:35:33.500
Anthony Taylor: Okay, hugging face. Rocks?

1427
01:35:34.756 --> 01:35:38.220
Anthony Taylor: So you can see it did this download thing again.

1428
01:35:38.510 --> 01:35:42.530
Anthony Taylor: Everybody should see that. So that's what happens with the transformer

1429
01:35:42.880 --> 01:35:48.039
Anthony Taylor: libraries that we did. When we ask it for something, it goes out and it gets it

1430
01:35:48.200 --> 01:35:49.619
Anthony Taylor: pretty much from hunting

1431
01:35:52.250 --> 01:35:54.549
Anthony Taylor: everybody good with that, everybody with me

1432
01:35:56.030 --> 01:36:00.819
Anthony Taylor: alright. So we're gonna give it a sentence. I am learning a lot about transformers.

1433
01:36:02.150 --> 01:36:04.360
Anthony Taylor: They're more than meets the eye.

1434
01:36:04.960 --> 01:36:06.010
Anthony Taylor: Okay?

1435
01:36:07.160 --> 01:36:08.150
Anthony Taylor: Why? And.

1436
01:36:08.150 --> 01:36:10.240
michael mcpherson: Holding that one back, all class.

1437
01:36:12.530 --> 01:36:13.759
Anthony Taylor: Okay, wait a minute.

1438
01:36:13.960 --> 01:36:16.110
Anthony Taylor: This is not an everyone do, is it?

1439
01:36:18.220 --> 01:36:20.269
Masarirambi, Rodney: It is. There's there's

1440
01:36:20.740 --> 01:36:21.830
Masarirambi, Rodney: yeah, it is.

1441
01:36:23.110 --> 01:36:24.480
Anthony Taylor: Well, fine!

1442
01:36:25.154 --> 01:36:28.709
Anthony Taylor: I thought this was an instructor due.

1443
01:36:29.630 --> 01:36:30.970
Masarirambi, Rodney: Think the next.

1444
01:36:30.970 --> 01:36:33.550
michael mcpherson: Go ahead and take over, and we'll just not.

1445
01:36:34.250 --> 01:36:37.619
Anthony Taylor: So number 4 is that instructor did.

1446
01:36:37.890 --> 01:36:38.960
Masarirambi, Rodney: Oh, yeah, this is Chicago.

1447
01:36:39.260 --> 01:36:39.560
Anthony Taylor: Certainly.

1448
01:36:39.560 --> 01:36:40.280
Masarirambi, Rodney: Yeah, we've.

1449
01:36:41.080 --> 01:36:45.019
Clayton Graves: Number 4 is instructor number 5 is student, and number 6 is.

1450
01:36:45.690 --> 01:36:46.810
Clayton Graves: I don't know.

1451
01:36:46.810 --> 01:36:48.180
Raugewitz, Tania: R. Sentence.

1452
01:36:49.310 --> 01:36:51.069
Anthony Taylor: Okay? Well, that's alright.

1453
01:36:51.681 --> 01:36:54.439
Anthony Taylor: Alright. So to tokenize it.

1454
01:36:54.470 --> 01:37:01.059
Anthony Taylor: you can do ids equals model dot tokenizer.

1455
01:37:02.358 --> 01:37:04.750
Anthony Taylor: Dot convert tokens to ids.

1456
01:37:04.840 --> 01:37:07.110
Anthony Taylor: Okay? Oh, wait, no, we're just tokenite.

1457
01:37:07.280 --> 01:37:09.240
Anthony Taylor: So we're gonna do token. I

1458
01:37:09.480 --> 01:37:10.600
Anthony Taylor: here it is.

1459
01:37:10.720 --> 01:37:12.679
Anthony Taylor: pass in our sentin.

1460
01:37:14.220 --> 01:37:16.830
Anthony Taylor: and we get a tokenized.

1461
01:37:19.220 --> 01:37:23.999
Anthony Taylor: Oh, I should change this to token. So we can stay with the with the solution.

1462
01:37:24.700 --> 01:37:25.720
Anthony Taylor: Okay?

1463
01:37:25.770 --> 01:37:27.750
Anthony Taylor: And if we print those out.

1464
01:37:31.460 --> 01:37:41.479
Anthony Taylor: that's what you get. So we've seen this before the way everything looks right. So here now we're going to convert them to Id, so we'll call it ids we'll say model

1465
01:37:41.520 --> 01:37:42.800
Anthony Taylor: dot

1466
01:37:43.300 --> 01:37:45.900
Anthony Taylor: tokenizer dot

1467
01:37:47.350 --> 01:37:48.060
Anthony Taylor: convert

1468
01:37:49.590 --> 01:38:00.789
Anthony Taylor: token. Now. No, you can see right here what's coming right ids to tokens, tokens, ids, tokens to drink. So we're gonna go tokens. Ids. We're gonna pass in our tokens.

1469
01:38:05.420 --> 01:38:08.610
Anthony Taylor: Oh, my goodness, pipe better! There we go.

1470
01:38:08.800 --> 01:38:11.740
Anthony Taylor: and we'll get yeah, prick them out.

1471
01:38:14.960 --> 01:38:17.439
Anthony Taylor: and we'll get our actual ids.

1472
01:38:17.630 --> 01:38:21.779
Anthony Taylor: Pretty exciting right now notice, these are a whole lot bigger than what we had earlier.

1473
01:38:22.350 --> 01:38:25.469
Anthony Taylor: Again, do we need to know how that's done? Not really.

1474
01:38:25.880 --> 01:38:26.860
Anthony Taylor: really don't.

1475
01:38:27.370 --> 01:38:31.100
Anthony Taylor: Okay, but it's it's good to see it. Now.

1476
01:38:31.360 --> 01:38:35.089
Anthony Taylor: here's where it's going to get interesting. So we're going to say embedding

1477
01:38:36.600 --> 01:38:40.049
Anthony Taylor: equals. And we're gonna do model.in code

1478
01:38:40.890 --> 01:38:43.509
Anthony Taylor: and encode our sentence.

1479
01:38:43.780 --> 01:38:47.620
Anthony Taylor: Now, this is the original sentence, this one up here.

1480
01:38:48.440 --> 01:38:49.510
Anthony Taylor: Alright.

1481
01:38:49.640 --> 01:38:51.070
Anthony Taylor: And

1482
01:38:51.110 --> 01:38:54.930
Anthony Taylor: then we're if we just say, Show me all the embeddings.

1483
01:38:56.890 --> 01:38:57.939
Anthony Taylor: It's a lot

1484
01:39:05.590 --> 01:39:06.620
Anthony Taylor: okay.

1485
01:39:07.130 --> 01:39:13.220
Anthony Taylor: Ray, we we can limit this if you don't want to see everything you can say. You know, 0 Colon

1486
01:39:13.450 --> 01:39:14.120
Anthony Taylor: 20,

1487
01:39:14.540 --> 01:39:17.200
Anthony Taylor: and you'll see a little less.

1488
01:39:18.760 --> 01:39:19.650
Anthony Taylor: Okay?

1489
01:39:21.670 --> 01:39:24.759
Anthony Taylor: let's check the length of that. Let's see. Length

1490
01:39:24.800 --> 01:39:26.270
Anthony Taylor: embedding

1491
01:39:27.480 --> 01:39:29.290
Anthony Taylor: 384.

1492
01:39:29.820 --> 01:39:34.859
Anthony Taylor: Cool thing is we can turn these back text. So we could say, decoded

1493
01:39:35.230 --> 01:39:36.490
Anthony Taylor: tokens

1494
01:39:36.780 --> 01:39:38.130
Anthony Taylor: equals

1495
01:39:38.300 --> 01:39:43.519
Anthony Taylor: model. And this is using. I don't know why they're going back there. But okay.

1496
01:39:43.570 --> 01:39:45.130
Anthony Taylor: Tokenizer.

1497
01:39:45.834 --> 01:39:46.480
Anthony Taylor: Dot

1498
01:39:46.940 --> 01:39:48.170
Anthony Taylor: the code

1499
01:39:48.230 --> 01:39:49.890
Anthony Taylor: ids

1500
01:39:51.560 --> 01:39:55.000
Anthony Taylor: and then print decoded token.

1501
01:40:01.170 --> 01:40:03.050
Anthony Taylor: It's taking a little longer. But there you go.

1502
01:40:03.410 --> 01:40:05.340
Anthony Taylor: Okay. So

1503
01:40:12.550 --> 01:40:13.400
Anthony Taylor: okay.

1504
01:40:14.100 --> 01:40:15.580
Anthony Taylor: so what are you guys thinking about all that.

1505
01:40:19.280 --> 01:40:20.300
Clayton Graves: Again secret.

1506
01:40:20.300 --> 01:40:20.890
Anthony Taylor: Questions.

1507
01:40:20.890 --> 01:40:22.959
Clayton Graves: Mobile team, secret Dakota ring.

1508
01:40:24.040 --> 01:40:31.489
Anthony Taylor: Well, yeah, there's definitely that. But forget that. Because I'm actually what I was most surprised whenever I started looking at this this section

1509
01:40:31.990 --> 01:40:34.590
Anthony Taylor: 384

1510
01:40:35.710 --> 01:40:37.040
Anthony Taylor: values.

1511
01:40:37.240 --> 01:40:38.050
Anthony Taylor: to create.

1512
01:40:38.050 --> 01:40:39.120
Clayton Graves: One little sentence.

1513
01:40:39.120 --> 01:40:40.669
Anthony Taylor: To embed that sent.

1514
01:40:41.750 --> 01:40:42.860
Anthony Taylor: Okay.

1515
01:40:43.440 --> 01:40:45.069
Anthony Taylor: so what do you guys think about that?

1516
01:40:49.610 --> 01:40:51.050
Clayton Graves: I think there's a lot.

1517
01:40:51.050 --> 01:40:52.000
Mason, Natalie: Aye, sure! Go ahead.

1518
01:40:52.000 --> 01:40:52.820
Clayton Graves: Times

1519
01:40:53.490 --> 01:40:55.159
Clayton Graves: a simple sentence

1520
01:40:55.230 --> 01:41:00.920
Clayton Graves: to to get it, to get it to a point where the computer understands it, and then to

1521
01:41:01.810 --> 01:41:07.520
Clayton Graves: turn it back into so that the computer could do something with it and then turn it back. It's just a lot of work.

1522
01:41:07.520 --> 01:41:08.140
Anthony Taylor: Right

1523
01:41:08.790 --> 01:41:18.689
Anthony Taylor: so and this is also the inputs that will be used for the neural network itself. So the idea behind this, these embeddings that are created by this model

1524
01:41:19.660 --> 01:41:23.289
Anthony Taylor: is this model. So let's just back up for a second.

1525
01:41:23.410 --> 01:41:29.230
Anthony Taylor: What we did earlier was we just tokenize this and created id, do we need a machine learning model to do that?

1526
01:41:30.520 --> 01:41:31.860
Anthony Taylor: Absolutely not.

1527
01:41:31.980 --> 01:41:35.610
Anthony Taylor: Okay. These embeddings. On the other hand.

1528
01:41:36.800 --> 01:41:38.680
Anthony Taylor: okay, this is.

1529
01:41:38.740 --> 01:41:41.939
Anthony Taylor: I am looking at the sentence, and

1530
01:41:42.170 --> 01:41:46.509
Anthony Taylor: this is my model is deciding what this sentence is trying to say.

1531
01:41:46.780 --> 01:41:48.990
Anthony Taylor: It's coming up with context.

1532
01:41:50.080 --> 01:41:51.500
Anthony Taylor: And that

1533
01:41:51.700 --> 01:41:53.340
Anthony Taylor: is what all of this is.

1534
01:41:53.540 --> 01:41:57.480
Clayton Graves: So is Chat Gpt, doing something similar. When I ask.

1535
01:41:57.480 --> 01:42:00.859
Anthony Taylor: 100%. Yes, that is how it's getting there.

1536
01:42:01.050 --> 01:42:04.729
Anthony Taylor: It has to do this with everything. Every time you type something

1537
01:42:04.750 --> 01:42:09.639
Anthony Taylor: it has to encode it and send it in and do what it does in background.

1538
01:42:10.350 --> 01:42:17.520
Anthony Taylor: Okay, now, the models already trained. So there's an encoding model. Probably not this one, but one similar.

1539
01:42:17.610 --> 01:42:19.050
Anthony Taylor: Okay, that will

1540
01:42:19.110 --> 01:42:22.130
Anthony Taylor: that will basically create something like this.

1541
01:42:22.200 --> 01:42:27.269
Anthony Taylor: Send it in, and then that will use its train model to come up with an answer and send it back.

1542
01:42:28.120 --> 01:42:31.169
Anthony Taylor: And of course, that all happens very, very fast.

1543
01:42:31.910 --> 01:42:32.750
Anthony Taylor: But

1544
01:42:32.850 --> 01:42:42.289
Anthony Taylor: yeah, but I mean, honestly, I'll tell you. It's kind of funny. When we first came in here this morning when I I logged in

1545
01:42:42.460 --> 01:42:44.529
Anthony Taylor: to my environment.

1546
01:42:46.550 --> 01:42:47.620
Anthony Taylor: this

1547
01:42:49.440 --> 01:42:53.940
Anthony Taylor: was there. So you guys hear me say, data breaks all the time. I'm kind of the expert in that

1548
01:42:54.010 --> 01:42:58.000
Anthony Taylor: right. But this morning this was in my data breaks

1549
01:42:59.510 --> 01:43:00.809
Anthony Taylor: this model.

1550
01:43:00.920 --> 01:43:03.549
Anthony Taylor: Holy, how is it that

1551
01:43:04.120 --> 01:43:07.480
Anthony Taylor: I mean you could type a question, and it's instantly

1552
01:43:07.890 --> 01:43:09.599
Anthony Taylor: all of the text comes back.

1553
01:43:10.080 --> 01:43:12.719
Anthony Taylor: It is so fast it is crazy. But anyway.

1554
01:43:12.760 --> 01:43:15.969
Anthony Taylor: so the models are getting faster. But they all are using the same method.

1555
01:43:16.210 --> 01:43:19.690
Anthony Taylor: Basically, I mean, they, they got better methods or whatever.

1556
01:43:20.070 --> 01:43:21.980
Anthony Taylor: Okay. So

1557
01:43:23.550 --> 01:43:24.980
Anthony Taylor: now we know

1558
01:43:25.040 --> 01:43:28.090
Anthony Taylor: we can get ids, we can get

1559
01:43:28.370 --> 01:43:29.700
Anthony Taylor: embeddings.

1560
01:43:29.780 --> 01:43:30.790
Anthony Taylor: Okay.

1561
01:43:33.320 --> 01:43:35.569
Anthony Taylor: now, we're going to talk about similarity.

1562
01:43:36.340 --> 01:43:42.069
Anthony Taylor: This was kind of the goal of today was to get to this similarity discussion

1563
01:43:42.800 --> 01:43:46.820
Anthony Taylor: alright with similarity. We can.

1564
01:43:48.440 --> 01:43:54.510
Anthony Taylor: I mean, it's how we find things. It's how we find related information. So if you had an FAQ,

1565
01:43:55.500 --> 01:43:56.790
Anthony Taylor: so you guys.

1566
01:43:56.820 --> 01:43:58.730
Anthony Taylor: I don't know whatever you're an expert in.

1567
01:43:58.740 --> 01:44:02.850
Anthony Taylor: You make a frequently asked questions, document. You

1568
01:44:03.080 --> 01:44:06.969
Anthony Taylor: tokenize and embed it into an Ln model.

1569
01:44:07.560 --> 01:44:12.440
Anthony Taylor: Somebody comes in. They ask a question, though I don't know. We'll do tough.

1570
01:44:12.740 --> 01:44:15.400
Anthony Taylor: hey? Making rugs with the cool gut.

1571
01:44:15.500 --> 01:44:19.380
Anthony Taylor: Okay, somebody comes in and say, Which gun should I use to make a shag carp.

1572
01:44:19.840 --> 01:44:20.880
Anthony Taylor: It's gonna

1573
01:44:21.020 --> 01:44:22.600
Anthony Taylor: look at

1574
01:44:22.810 --> 01:44:25.620
Anthony Taylor: gun shack. Blah, blah, blah!

1575
01:44:25.720 --> 01:44:32.350
Anthony Taylor: Find an answer that has that is similar to that question. Return that answer.

1576
01:44:33.170 --> 01:44:34.470
Anthony Taylor: That's basically it.

1577
01:44:34.600 --> 01:44:36.010
Anthony Taylor: That's very basic.

1578
01:44:36.060 --> 01:44:37.650
Anthony Taylor: But that's basically it.

1579
01:44:38.030 --> 01:44:40.120
Anthony Taylor: It is done in this manner.

1580
01:44:40.640 --> 01:44:42.570
Anthony Taylor: Okay, with similarity.

1581
01:44:47.480 --> 01:44:48.660
Anthony Taylor: they're so silly.

1582
01:44:48.790 --> 01:44:53.240
Anthony Taylor: So that's it, hey? That's cool there it is geometric familiar. So you could kind of see

1583
01:44:53.480 --> 01:45:00.710
Anthony Taylor: like one is closer to, I mean is kind of in the middle of these 2 I don't know. This is really not a very helpful background, but

1584
01:45:01.470 --> 01:45:03.219
Anthony Taylor: using these embeddings.

1585
01:45:03.620 --> 01:45:09.549
Anthony Taylor: it could come up with the math to to figure out if these are similar or not

1586
01:45:09.960 --> 01:45:10.730
Anthony Taylor: so.

1587
01:45:11.388 --> 01:45:18.220
Clayton Graves: I wanted to build a knowledge base that was powered by an Ln.

1588
01:45:18.730 --> 01:45:22.960
Clayton Graves: and I've got, you know, documents that I could feed into the Lm.

1589
01:45:23.630 --> 01:45:27.979
Clayton Graves: And and have a pre trained model for language and all that good stuff.

1590
01:45:28.030 --> 01:45:29.340
Clayton Graves: How many

1591
01:45:29.650 --> 01:45:32.660
Clayton Graves: documents, what I have to come up with

1592
01:45:33.680 --> 01:45:35.799
Clayton Graves: in order to properly train this thing.

1593
01:45:36.640 --> 01:45:37.969
Anthony Taylor: You could do it in one.

1594
01:45:39.850 --> 01:45:41.370
Anthony Taylor: you could do it with

1595
01:45:41.670 --> 01:45:42.900
Anthony Taylor: one document.

1596
01:45:44.380 --> 01:45:46.470
Clayton Graves: So I could take my 40 documents

1597
01:45:47.470 --> 01:45:48.400
Clayton Graves: and

1598
01:45:48.950 --> 01:45:50.420
Clayton Graves: create an Ll. M.

1599
01:45:51.350 --> 01:45:56.880
Clayton Graves: And just have a user. Ask, Hey, how do I? How do I install sent to us, and it'll.

1600
01:45:57.980 --> 01:46:00.279
Anthony Taylor: Yup 100%

1601
01:46:00.690 --> 01:46:04.030
Anthony Taylor: 100%. And and like, I said, it's something that I

1602
01:46:04.500 --> 01:46:06.320
Anthony Taylor: it's so simple

1603
01:46:06.550 --> 01:46:11.859
Anthony Taylor: that I can probably show you, even if we don't show you in like 15 min.

1604
01:46:14.010 --> 01:46:19.589
Anthony Taylor: I know we're gonna cover lane chain. I know we're gonna cover lane chain, and that's what you would use to do that.

1605
01:46:19.590 --> 01:46:23.929
Clayton Graves: As an aside like an offline type thing. I would be interested in pursuing that.

1606
01:46:24.910 --> 01:46:28.599
Anthony Taylor: Well, I'm hoping we show you that I haven't looked at next week.

1607
01:46:28.650 --> 01:46:37.390
Anthony Taylor: Well, deep into next week. I know. Next week we cover lane chain which that's the tool that you would use to ingest the documents.

1608
01:46:38.097 --> 01:46:40.660
Anthony Taylor: Into a vector, database. And then

1609
01:46:40.760 --> 01:46:41.949
Anthony Taylor: you're good to go.

1610
01:46:43.220 --> 01:46:46.399
Anthony Taylor: It's it's it's fairly straightforward.

1611
01:46:48.990 --> 01:46:52.009
Anthony Taylor: simpler than making a neural network. How's that like?

1612
01:47:01.060 --> 01:47:03.840
Anthony Taylor: So we're gonna do sentence similarity here.

1613
01:47:05.850 --> 01:47:08.620
Anthony Taylor: one of the ways that you can do this

1614
01:47:08.910 --> 01:47:10.180
Anthony Taylor: is.

1615
01:47:11.422 --> 01:47:13.260
Anthony Taylor: with similarity measures.

1616
01:47:13.430 --> 01:47:18.449
Anthony Taylor: And they're basically just math medical methods to calculate the distance between the vector

1617
01:47:18.460 --> 01:47:20.030
Anthony Taylor: so the vector

1618
01:47:20.040 --> 01:47:23.320
Anthony Taylor: is the content is the embedding itself.

1619
01:47:24.140 --> 01:47:25.340
Anthony Taylor: Okay?

1620
01:47:25.460 --> 01:47:26.620
Anthony Taylor: And

1621
01:47:26.710 --> 01:47:31.730
Anthony Taylor: it is going to do math math to come up with the distance between these 2 vectors.

1622
01:47:31.870 --> 01:47:34.600
Anthony Taylor: and that is going to determine how similar they are

1623
01:47:34.910 --> 01:47:37.700
Anthony Taylor: okay. So we gotta wait for this to run.

1624
01:47:39.760 --> 01:47:42.819
Anthony Taylor: Do, do, do. Do.

1625
01:47:45.920 --> 01:47:48.140
Anthony Taylor: This is still supposed to be. You know what

1626
01:47:48.720 --> 01:47:51.610
Anthony Taylor: I know. I just let that run. But I'm going to do it again

1627
01:47:51.920 --> 01:47:55.440
Anthony Taylor: because I am not gonna type. All this if I don't have to.

1628
01:47:55.500 --> 01:47:57.150
Anthony Taylor: This is an instructor, Demo?

1629
01:48:00.180 --> 01:48:00.970
Anthony Taylor: I don't know.

1630
01:48:02.090 --> 01:48:05.900
Anthony Taylor: Does it help you when I type it, or do you prefer.

1631
01:48:08.010 --> 01:48:18.749
Meredith McCanse (she/her): It's easier to engage and participate when you're typing, because it gives time for us to also type it. And that helps to absorb the information, just watching someone do it. It's harder to retain it.

1632
01:48:21.220 --> 01:48:22.370
Meredith McCanse (she/her): Be my vote.

1633
01:48:25.090 --> 01:48:26.110
Anthony Taylor: Okay.

1634
01:48:26.110 --> 01:48:27.060
Raugewitz, Tania: I concur.

1635
01:48:28.370 --> 01:48:30.490
Anthony Taylor: Alright! Alright! Alright!

1636
01:48:30.640 --> 01:48:32.920
Anthony Taylor: You guys know how much I

1637
01:48:34.950 --> 01:48:38.149
Anthony Taylor: I don't even type on my phone. I use my voice alright.

1638
01:48:38.150 --> 01:48:39.760
Mason, Natalie: Your gerb.

1639
01:48:43.800 --> 01:48:44.720
Raugewitz, Tania: That's funny.

1640
01:48:45.530 --> 01:48:47.659
Anthony Taylor: Natalie is going to the waiting room for.

1641
01:48:50.980 --> 01:48:51.979
Anthony Taylor: But no, okay.

1642
01:48:51.980 --> 01:48:53.240
michael mcpherson: Are you doing off now?

1643
01:48:54.525 --> 01:48:55.160
Anthony Taylor: Similarity.

1644
01:48:55.160 --> 01:48:55.510
Masarirambi, Rodney: And money.

1645
01:48:55.510 --> 01:48:59.769
Anthony Taylor: Same one for just the next notebook in that that group.

1646
01:49:00.220 --> 01:49:03.889
Anthony Taylor: Alright. So here we're gonna bring in the same sentence. Transformer.

1647
01:49:04.860 --> 01:49:07.839
Anthony Taylor: I don't know this is gonna work. Cause I left and came back. Let's find out.

1648
01:49:10.740 --> 01:49:12.300
Anthony Taylor: Yeah. It is cool

1649
01:49:15.450 --> 01:49:17.600
Anthony Taylor: and the same sentences.

1650
01:49:18.230 --> 01:49:20.110
Anthony Taylor: And we're going to go in. And we're going to

1651
01:49:20.340 --> 01:49:23.189
Anthony Taylor: just tokenize and and and encode right away.

1652
01:49:23.720 --> 01:49:26.649
Anthony Taylor: So I'm gonna let those run and then start the next one.

1653
01:49:26.800 --> 01:49:28.780
Anthony Taylor: So there's a couple of different

1654
01:49:29.390 --> 01:49:31.400
Anthony Taylor: similarity measures.

1655
01:49:31.650 --> 01:49:38.839
Anthony Taylor: There's Euclidean distance, cosine similarity, and Pearson's correlation coefficient.

1656
01:49:39.040 --> 01:49:40.680
Anthony Taylor: We've heard of that one, haven't

1657
01:49:41.970 --> 01:49:45.099
Anthony Taylor: we've used Pearson's correlation coefficient before

1658
01:49:45.960 --> 01:49:47.090
Anthony Taylor: okay?

1659
01:49:47.510 --> 01:49:50.379
Anthony Taylor: We could take a look at. Well.

1660
01:49:50.780 --> 01:49:56.379
Anthony Taylor: so let's generate a cosine similarity score using our embeddings. Well, let's print them out.

1661
01:49:57.120 --> 01:49:58.569
Anthony Taylor: I'm gonna make a new spell, though.

1662
01:50:04.900 --> 01:50:05.960
Anthony Taylor: So there they are.

1663
01:50:06.460 --> 01:50:09.379
Anthony Taylor: Okay. That's the embeddings that were created

1664
01:50:09.490 --> 01:50:12.359
Anthony Taylor: with our sentence, Tokenizer. So

1665
01:50:12.450 --> 01:50:14.800
Anthony Taylor: let's do our cosine similarity

1666
01:50:15.070 --> 01:50:17.550
Anthony Taylor: of these embedding.

1667
01:50:21.270 --> 01:50:22.319
Anthony Taylor: oh, darn it.

1668
01:50:28.240 --> 01:50:29.230
Anthony Taylor: there it is

1669
01:50:32.120 --> 01:50:36.069
Anthony Taylor: of how fast that came up. So we're gonna do cosine scores.

1670
01:50:38.430 --> 01:50:41.139
Anthony Taylor: cosine scores

1671
01:50:41.360 --> 01:50:44.009
Anthony Taylor: equals. And then it's just util

1672
01:50:44.380 --> 01:50:47.840
Anthony Taylor: dot COS. Underscore

1673
01:50:47.940 --> 01:50:49.120
Anthony Taylor: SIM.

1674
01:50:49.840 --> 01:50:51.840
Anthony Taylor: And then you're just going to pass in

1675
01:50:52.130 --> 01:50:53.820
Anthony Taylor: the embeddings

1676
01:50:56.470 --> 01:50:57.410
Anthony Taylor: twice.

1677
01:50:58.560 --> 01:51:04.250
Anthony Taylor: Okay, now notice what it's doing. We're calculating the similarity between the Embeddings

1678
01:51:04.470 --> 01:51:05.790
Anthony Taylor: themselves.

1679
01:51:06.390 --> 01:51:09.889
Anthony Taylor: Alright. So that's why we need it twice. We need an A and a B if you will.

1680
01:51:10.090 --> 01:51:12.780
Anthony Taylor: and then we're going to output the Coside

1681
01:51:12.920 --> 01:51:13.960
Anthony Taylor: scores

1682
01:51:15.700 --> 01:51:16.809
Anthony Taylor: and we get

1683
01:51:16.880 --> 01:51:18.530
Anthony Taylor: a cool little grid.

1684
01:51:18.700 --> 01:51:19.990
Anthony Taylor: obviously.

1685
01:51:20.020 --> 01:51:22.620
Anthony Taylor: Sentence, one. Sentence, 2. Sentence, 3.

1686
01:51:22.930 --> 01:51:26.099
Anthony Taylor: Sentence, one as a point 5 6 9

1687
01:51:26.440 --> 01:51:30.180
Anthony Taylor: to sentence 2 and a 7, 5, 4 to sentence. 3.

1688
01:51:31.100 --> 01:51:32.950
Anthony Taylor: Okay, the least

1689
01:51:33.220 --> 01:51:35.120
Anthony Taylor: is this number 2

1690
01:51:35.970 --> 01:51:39.839
Anthony Taylor: and number 3. Those are really not similar at all.

1691
01:51:39.860 --> 01:51:41.020
Anthony Taylor: Which was what

1692
01:51:42.490 --> 01:51:44.930
Anthony Taylor: I love my family. My dog is a lag.

1693
01:51:45.690 --> 01:51:47.260
Anthony Taylor: Okay, those are not similar.

1694
01:51:47.980 --> 01:51:50.490
Anthony Taylor: but we can see how I go ahead.

1695
01:51:50.740 --> 01:51:57.590
Baro, Sonja: Anthony, when you come back down to that, those lists can you go? Which which way we're reading them?

1696
01:51:57.970 --> 01:51:58.690
Baro, Sonja: So.

1697
01:51:58.690 --> 01:52:00.549
Anthony Taylor: Well, it's a grid, so it's it's.

1698
01:52:00.550 --> 01:52:01.030
Baro, Sonja: Yeah.

1699
01:52:01.030 --> 01:52:06.060
Anthony Taylor: It's this is sentence one. Well, it could be the other way. Well, let's see.

1700
01:52:06.060 --> 01:52:07.159
Baro, Sonja: So one to one.

1701
01:52:07.160 --> 01:52:09.889
Anthony Taylor: Say 1 one and

1702
01:52:10.270 --> 01:52:13.709
Anthony Taylor: one should be close to all of them, and they are.

1703
01:52:14.140 --> 01:52:14.630
Baro, Sonja: So that's.

1704
01:52:14.630 --> 01:52:15.950
Anthony Taylor: They're all pretty hot.

1705
01:52:15.950 --> 01:52:16.630
Baro, Sonja: Yeah.

1706
01:52:16.630 --> 01:52:19.190
Anthony Taylor: So. So this is sentence one. I love my dog

1707
01:52:19.400 --> 01:52:23.469
Anthony Taylor: right. This is sentence 2, which is, I love my family.

1708
01:52:23.470 --> 01:52:24.160
Baro, Sonja: Okay.

1709
01:52:24.160 --> 01:52:29.930
Anthony Taylor: Okay? And then this is sentence 3, which is, my dog is flat. So we wouldn't expect 2

1710
01:52:30.180 --> 01:52:31.580
Anthony Taylor: and 3

1711
01:52:31.650 --> 01:52:33.530
Anthony Taylor: to be similar at all

1712
01:52:33.980 --> 01:52:35.810
Anthony Taylor: right. Okay.

1713
01:52:36.110 --> 01:52:39.419
Anthony Taylor: but one I love. My dog

1714
01:52:39.550 --> 01:52:45.489
Anthony Taylor: would be similar to. I love my family, and my dog is a lab similar to. I love my dog.

1715
01:52:46.590 --> 01:52:47.799
Anthony Taylor: You guys see that.

1716
01:52:48.360 --> 01:52:50.209
Anthony Taylor: That's why you see what you see.

1717
01:52:50.720 --> 01:52:51.720
Anthony Taylor: Okay.

1718
01:52:52.410 --> 01:52:54.610
Anthony Taylor: but we can actually

1719
01:52:55.160 --> 01:52:58.060
Anthony Taylor: like, use some compute

1720
01:52:58.430 --> 01:53:01.269
Anthony Taylor: and figure this out. So let's do pairs

1721
01:53:01.390 --> 01:53:02.570
Anthony Taylor: equals

1722
01:53:03.000 --> 01:53:04.439
Anthony Taylor: an empty list.

1723
01:53:04.600 --> 01:53:07.629
Anthony Taylor: And then we're going to say for I

1724
01:53:07.800 --> 01:53:09.540
Anthony Taylor: in range.

1725
01:53:09.720 --> 01:53:12.429
Anthony Taylor: and we're going to get the length of our scores.

1726
01:53:12.510 --> 01:53:13.690
Anthony Taylor: So length

1727
01:53:14.080 --> 01:53:17.160
Anthony Taylor: of cosine scores.

1728
01:53:18.580 --> 01:53:19.559
Anthony Taylor: Minus what?

1729
01:53:19.870 --> 01:53:21.769
Anthony Taylor: Okay? Because remember, it's an array.

1730
01:53:22.430 --> 01:53:23.320
Anthony Taylor: Alright.

1731
01:53:24.850 --> 01:53:25.940
Anthony Taylor: And

1732
01:53:26.110 --> 01:53:27.880
Anthony Taylor: we're going to do another loop.

1733
01:53:29.590 --> 01:53:33.140
Anthony Taylor: Oh, okay, I see what we're doing. So we're basically gonna say.

1734
01:53:34.600 --> 01:53:37.549
Anthony Taylor: go through this this way and then this way

1735
01:53:37.580 --> 01:53:41.689
Anthony Taylor: and give us the values that we have. So we'll do 4 J

1736
01:53:41.950 --> 01:53:44.070
Anthony Taylor: in range.

1737
01:53:46.010 --> 01:53:48.230
Anthony Taylor: length of cosine scores

1738
01:53:54.400 --> 01:53:57.349
Anthony Taylor: minus one. No, not minus one this time.

1739
01:54:00.600 --> 01:54:05.609
Anthony Taylor: And then we're going to in our empty list up there. We're going to do append?

1740
01:54:06.458 --> 01:54:11.510
Anthony Taylor: Oh, this is just making it prettier to look at. Really? Okay, whatever

1741
01:54:11.930 --> 01:54:13.389
Anthony Taylor: in text.

1742
01:54:15.450 --> 01:54:16.450
Anthony Taylor: Poland

1743
01:54:16.620 --> 01:54:19.220
Anthony Taylor: in brackets, do I? Comma? J,

1744
01:54:20.100 --> 01:54:22.900
Anthony Taylor: okay, comma score

1745
01:54:24.120 --> 01:54:26.870
Anthony Taylor: Colon, and then passing

1746
01:54:26.980 --> 01:54:29.490
Anthony Taylor: cosign scores.

1747
01:54:30.020 --> 01:54:31.630
Anthony Taylor: I comma. J,

1748
01:54:35.310 --> 01:54:36.280
Anthony Taylor: oh, no.

1749
01:54:36.530 --> 01:54:39.689
Anthony Taylor: this is a little different. This would be, I brackets

1750
01:54:40.150 --> 01:54:41.889
Anthony Taylor: and yeah, and no comma

1751
01:54:42.580 --> 01:54:44.839
Anthony Taylor: and J. Bracket.

1752
01:54:45.510 --> 01:54:46.520
Anthony Taylor: Okay?

1753
01:54:47.980 --> 01:54:54.990
Anthony Taylor: So it's just going to go through this grid and give us the index, and then the score for each one of these.

1754
01:54:56.032 --> 01:55:02.060
Anthony Taylor: After that we'll sort them out. So we'll say, pairs equals sorted

1755
01:55:02.520 --> 01:55:03.840
Anthony Taylor: pairs.

1756
01:55:04.380 --> 01:55:05.550
Anthony Taylor: Comma

1757
01:55:06.108 --> 01:55:10.649
Anthony Taylor: the key. We're gonna use an anonymous function for my lab defense.

1758
01:55:12.320 --> 01:55:14.730
Anthony Taylor: I have a developer at work.

1759
01:55:15.060 --> 01:55:17.619
Anthony Taylor: and she's she doesn't. She's learning

1760
01:55:18.060 --> 01:55:19.750
Anthony Taylor: her python.

1761
01:55:20.220 --> 01:55:24.050
Anthony Taylor: And she asked me for help because she was having trouble with her code.

1762
01:55:24.100 --> 01:55:27.540
Anthony Taylor: and she's so many lambda functions. And I'm like.

1763
01:55:28.370 --> 01:55:30.390
Anthony Taylor: do you know what? That is?

1764
01:55:33.670 --> 01:55:36.519
Anthony Taylor: Too much chat? Gpt, that's what that was.

1765
01:55:37.555 --> 01:55:39.760
Anthony Taylor: But that's okay.

1766
01:55:40.570 --> 01:55:42.290
Anthony Taylor: I educated her.

1767
01:55:43.840 --> 01:55:46.250
Anthony Taylor: and we got a syntax here. What's my error?

1768
01:55:46.640 --> 01:55:50.000
Anthony Taylor: Index? Oh, this needs to be in a

1769
01:55:50.320 --> 01:55:51.380
Anthony Taylor: dictionary.

1770
01:55:54.110 --> 01:55:55.390
Anthony Taylor: Yeah, I should do it.

1771
01:55:55.570 --> 01:55:56.390
Anthony Taylor: There we go.

1772
01:55:57.460 --> 01:56:01.869
Anthony Taylor: so we could see our scores.

1773
01:56:05.300 --> 01:56:08.880
Anthony Taylor: I don't like the ones. I wish we could get rid of them.

1774
01:56:11.740 --> 01:56:13.020
Anthony Taylor: but we can't

1775
01:56:13.610 --> 01:56:18.450
Anthony Taylor: alright. So now we could. Then we could print out the actual sentences.

1776
01:56:18.450 --> 01:56:18.770
Clayton Graves: Pick, a.

1777
01:56:19.140 --> 01:56:19.880
Anthony Taylor: To see.

1778
01:56:20.290 --> 01:56:22.520
Clayton Graves: Multiplication table, really.

1779
01:56:23.170 --> 01:56:25.390
Anthony Taylor: Basically, yeah, I mean this.

1780
01:56:25.600 --> 01:56:33.750
Anthony Taylor: I personally think we just talking about it way we did comparing the numbers was probably enough. But that's okay.

1781
01:56:34.010 --> 01:56:36.340
Anthony Taylor: we'll finish this out. So you guys have

1782
01:56:36.770 --> 01:56:37.580
Anthony Taylor: this.

1783
01:56:39.870 --> 01:56:41.210
Anthony Taylor: this example.

1784
01:56:41.890 --> 01:56:46.159
Anthony Taylor: Okay, so here, all we're gonna do is print.

1785
01:56:48.710 --> 01:56:50.280
Anthony Taylor: the sentence

1786
01:56:51.250 --> 01:56:52.910
Anthony Taylor: from up above

1787
01:56:53.420 --> 01:56:57.310
Anthony Taylor: sentence, the the using, the index

1788
01:56:58.070 --> 01:57:00.529
Anthony Taylor: from our stuff

1789
01:57:01.140 --> 01:57:02.500
Anthony Taylor: divided, though.

1790
01:57:03.070 --> 01:57:06.130
Anthony Taylor: No, that's the end of that. Not divided. Wait!

1791
01:57:06.300 --> 01:57:07.190
Anthony Taylor: Oh.

1792
01:57:07.870 --> 01:57:10.420
Anthony Taylor: anybody know what that slash T is?

1793
01:57:11.520 --> 01:57:13.299
Anthony Taylor: It's not a divided by trust me.

1794
01:57:16.490 --> 01:57:17.420
Anthony Taylor: no.

1795
01:57:18.400 --> 01:57:20.140
Anthony Taylor: How about slash in.

1796
01:57:20.730 --> 01:57:21.760
Meredith McCanse (she/her): And escaping.

1797
01:57:21.760 --> 01:57:22.689
Anthony Taylor: Thank you, Derek.

1798
01:57:23.110 --> 01:57:26.120
Anthony Taylor: Derek, Derek got it. It's it's a tab.

1799
01:57:27.980 --> 01:57:29.600
Anthony Taylor: Somebody else said that, too.

1800
01:57:29.600 --> 01:57:31.010
Raugewitz, Tania: Panya believe it or not.

1801
01:57:31.570 --> 01:57:33.100
Anthony Taylor: Yeah. Tanya.

1802
01:57:33.740 --> 01:57:35.270
Anthony Taylor: Okay, my python courses. I got.

1803
01:57:35.270 --> 01:57:35.920
Raugewitz, Tania: Hiking.

1804
01:57:35.920 --> 01:57:37.780
Anthony Taylor: I got all excited about

1805
01:57:37.830 --> 01:57:39.750
Anthony Taylor: urgent Tab, but not Derek.

1806
01:57:40.020 --> 01:57:41.520
Anthony Taylor: Good job, Derek.

1807
01:57:41.520 --> 01:57:42.329
Raugewitz, Tania: We expect more.

1808
01:57:42.330 --> 01:57:42.910
Anthony Taylor: Perfect, Derek.

1809
01:57:42.910 --> 01:57:44.309
Raugewitz, Tania: Than we do of Tanya.

1810
01:57:44.910 --> 01:57:47.079
Anthony Taylor: That is not true.

1811
01:57:47.560 --> 01:57:48.833
Anthony Taylor: That mesh.

1812
01:57:52.630 --> 01:57:53.560
Anthony Taylor: What.

1813
01:57:55.770 --> 01:57:56.230
Clayton Graves: That was.

1814
01:57:56.230 --> 01:57:57.070
Anthony Taylor: Okay.

1815
01:57:57.070 --> 01:58:01.870
Clayton Graves: Terrible, but it was funny. It was terrible, but Tanya's Tanya's awesome. No, no.

1816
01:58:03.040 --> 01:58:03.920
Anthony Taylor: Exactly

1817
01:58:04.500 --> 01:58:08.979
Anthony Taylor: okay. That should do it. Now I knew that's gonna get up and gone.

1818
01:58:09.070 --> 01:58:10.330
Anthony Taylor: What am I missing?

1819
01:58:12.760 --> 01:58:14.549
Anthony Taylor: where's Matt when you need them?

1820
01:58:15.240 --> 01:58:16.220
Baro, Sonja: Spaces.

1821
01:58:16.220 --> 01:58:18.300
Anthony Taylor: And not include a backstage.

1822
01:58:20.350 --> 01:58:21.840
Anthony Taylor: Okay, wait. Oh.

1823
01:58:22.040 --> 01:58:22.839
Baro, Sonja: To need a space.

1824
01:58:22.840 --> 01:58:23.340
Anthony Taylor: The scene.

1825
01:58:23.340 --> 01:58:23.930
Baro, Sonja: Ward.

1826
01:58:23.930 --> 01:58:25.600
Anthony Taylor: No, I'm missing a curly breath.

1827
01:58:26.690 --> 01:58:27.899
Anthony Taylor: Alright. Let's try that

1828
01:58:28.280 --> 01:58:29.090
Anthony Taylor: now.

1829
01:58:30.830 --> 01:58:34.469
Anthony Taylor: Okay, so boom, boom, boom, yeah, I probably need a space

1830
01:58:34.890 --> 01:58:36.010
Anthony Taylor: here.

1831
01:58:36.190 --> 01:58:36.700
Baro, Sonja: In there.

1832
01:58:36.700 --> 01:58:37.700
Anthony Taylor: Here.

1833
01:58:40.050 --> 01:58:41.820
Anthony Taylor: and then I have an extra woman.

1834
01:58:45.310 --> 01:58:47.380
Anthony Taylor: Sentence is not defined.

1835
01:58:47.930 --> 01:58:50.329
Anthony Taylor: It's called sentences, isn't yeah.

1836
01:58:53.360 --> 01:58:54.700
Anthony Taylor: You would think

1837
01:58:56.090 --> 01:59:00.310
Anthony Taylor: I could remember that long enough to tell. There it is.

1838
01:59:00.690 --> 01:59:03.649
Anthony Taylor: Okay. So obviously, these 2 are the same.

1839
01:59:03.850 --> 01:59:05.089
Anthony Taylor: These 2 are the same.

1840
01:59:05.390 --> 01:59:07.509
Anthony Taylor: these 2 fairly similar.

1841
01:59:07.850 --> 01:59:09.050
Anthony Taylor: these 2

1842
01:59:09.220 --> 01:59:10.470
Anthony Taylor: fairly similar.

1843
01:59:10.870 --> 01:59:14.400
Anthony Taylor: These 2 fairly similar, these 2, not similar at all.

1844
01:59:14.780 --> 01:59:17.419
Anthony Taylor: Which is basically what I was pointing out. Right?

1845
01:59:18.120 --> 01:59:19.760
Anthony Taylor: Everybody with us on this.

1846
01:59:22.300 --> 01:59:23.180
Anthony Taylor: Okay?

1847
01:59:23.380 --> 01:59:27.019
Anthony Taylor: And and you can also convert it into a pandas data frame and that kind of thing.

1848
01:59:27.730 --> 01:59:29.869
Anthony Taylor: Alright, there's no point. You can see it right there.

1849
01:59:30.560 --> 01:59:31.540
Anthony Taylor: Alright!

1850
01:59:31.880 --> 01:59:33.490
Anthony Taylor: Everybody good. With that.

1851
01:59:37.180 --> 01:59:39.377
Baro, Sonja: That is relative.

1852
01:59:41.460 --> 01:59:43.509
Baro, Sonja: I said, good is relative.

1853
01:59:44.190 --> 01:59:45.710
Anthony Taylor: You're not good with that.

1854
01:59:45.880 --> 01:59:51.090
Baro, Sonja: No, I I follow it. It's just the recall probably is not gonna be there.

1855
01:59:51.090 --> 01:59:56.149
Anthony Taylor: Well, and I the important thing here. So I'm gonna if I hit the highlights.

1856
01:59:57.690 --> 01:59:59.060
Anthony Taylor: we embedded it.

1857
01:59:59.780 --> 02:00:02.640
Anthony Taylor: or I mean it was encoded and embedded

1858
02:00:03.210 --> 02:00:04.879
Anthony Taylor: right there. That's it! That's the whole thing.

1859
02:00:06.400 --> 02:00:07.270
Anthony Taylor: Okay.

1860
02:00:08.032 --> 02:00:11.480
Anthony Taylor: and actually, you did you even this.

1861
02:00:12.880 --> 02:00:16.239
Anthony Taylor: I don't even think you needed that first one that was just stop.

1862
02:00:16.440 --> 02:00:18.339
Anthony Taylor: Okay, this is what we really need.

1863
02:00:19.210 --> 02:00:21.140
Anthony Taylor: Alright, once we hit that.

1864
02:00:21.190 --> 02:00:23.529
Anthony Taylor: we could do a similarity score. That's it.

1865
02:00:24.310 --> 02:00:25.130
Anthony Taylor: That's it.

1866
02:00:26.270 --> 02:00:27.190
Anthony Taylor: Okay.

1867
02:00:28.840 --> 02:00:31.100
Anthony Taylor: and that's pretty much the deal with that.

1868
02:00:31.450 --> 02:00:36.319
Anthony Taylor: Once we had the similarity score. Now we can see these are similar sentences. That's all we wanted to do

1869
02:00:36.330 --> 02:00:39.520
Anthony Taylor: was identify similar sentences.

1870
02:00:43.670 --> 02:00:45.290
Anthony Taylor: alright!

1871
02:00:54.950 --> 02:00:59.829
Anthony Taylor: Oh, now you guys get to do it! How exciting is that finally an activity.

1872
02:01:00.270 --> 02:01:02.939
Anthony Taylor: 20 min activity. How are we doing on time?

1873
02:01:03.660 --> 02:01:05.740
Anthony Taylor: Oh.

1874
02:01:08.230 --> 02:01:09.789
Anthony Taylor: I don't know how we ended up

1875
02:01:10.340 --> 02:01:11.610
Anthony Taylor: off on time.

1876
02:01:12.660 --> 02:01:14.799
Anthony Taylor: Alright. So we're gonna do.

1877
02:01:16.730 --> 02:01:20.550
Anthony Taylor: And we're gonna keep doing everyone dues for right now, as I want to get through all this.

1878
02:01:20.680 --> 02:01:21.600
Anthony Taylor: So

1879
02:01:21.970 --> 02:01:24.650
Anthony Taylor: go, and but but try to do it with me.

1880
02:01:26.340 --> 02:01:29.110
Anthony Taylor: Go to 5. Bring in the unsolved.

1881
02:01:30.530 --> 02:01:31.400
Anthony Taylor: Okay.

1882
02:01:31.550 --> 02:01:34.789
Anthony Taylor: this one also has resources. We have to bring those into.

1883
02:01:35.790 --> 02:01:39.420
Anthony Taylor: Okay? So you could do this one.

1884
02:01:39.560 --> 02:01:42.029
Anthony Taylor: Go ahead and start your piff install

1885
02:01:44.260 --> 02:01:47.039
Anthony Taylor: while that's running. Go to resources.

1886
02:01:48.070 --> 02:01:52.889
Anthony Taylor: What I would recommend is you? Right? Click and do new folder.

1887
02:01:52.970 --> 02:01:54.780
Anthony Taylor: Call it resources.

1888
02:01:55.980 --> 02:01:57.040
Anthony Taylor: Okay?

1889
02:01:57.190 --> 02:01:59.780
Anthony Taylor: And then click right here

1890
02:01:59.870 --> 02:02:01.509
Anthony Taylor: to upload a file.

1891
02:02:02.410 --> 02:02:03.609
Anthony Taylor: go in there

1892
02:02:03.810 --> 02:02:05.230
Anthony Taylor: and upload

1893
02:02:05.970 --> 02:02:10.639
Anthony Taylor: that file. Now it doesn't go into resources, just drag it and drop

1894
02:02:12.000 --> 02:02:14.629
Anthony Taylor: alright. And then you're good to go.

1895
02:02:14.750 --> 02:02:15.960
Anthony Taylor: You probably could do it

1896
02:02:16.180 --> 02:02:17.240
Anthony Taylor: from here.

1897
02:02:17.910 --> 02:02:19.889
Anthony Taylor: Yeah, you couldn't. But anyway.

1898
02:02:20.770 --> 02:02:24.049
Anthony Taylor: alright! So you are now ready to do this exercise.

1899
02:02:25.230 --> 02:02:26.659
Anthony Taylor: Thereby follow that

1900
02:02:30.290 --> 02:02:32.979
Anthony Taylor: I'm looking around where I see noddings.

1901
02:02:34.810 --> 02:02:35.870
Anthony Taylor: thumbs.

1902
02:02:38.160 --> 02:02:40.359
Anthony Taylor: Alright. Thank you. They again

1903
02:02:41.990 --> 02:02:45.070
Anthony Taylor: thank you, Natalie Cindy. How you doing over there?

1904
02:02:49.285 --> 02:02:53.740
Sihong Zhou: I'm trying to peep. People install the sentence transformer now.

1905
02:02:54.600 --> 02:02:57.060
Anthony Taylor: Okay. But you did. You get your file uploaded?

1906
02:02:57.480 --> 02:02:59.940
Anthony Taylor: You could do that while the pip installer's running.

1907
02:03:00.360 --> 02:03:02.049
Sihong Zhou: Okay, I'll do it now.

1908
02:03:02.250 --> 02:03:05.399
Anthony Taylor: Yeah, yeah, cause you'll need it. And pretty quick. So.

1909
02:03:06.059 --> 02:03:06.480
Sihong Zhou: Okay.

1910
02:03:07.510 --> 02:03:08.110
Sihong Zhou: Alright.

1911
02:03:08.110 --> 02:03:11.139
Anthony Taylor: So while that's running, we're gonna go ahead and start

1912
02:03:12.800 --> 02:03:14.800
Anthony Taylor: with the data frame.

1913
02:03:21.090 --> 02:03:24.520
Anthony Taylor: I don't know how they ended up behind. We were doing so good.

1914
02:03:27.740 --> 02:03:31.070
Anthony Taylor: But there's no way we're gonna get to the last one, and I wouldn't get to it. So

1915
02:03:31.640 --> 02:03:33.129
Anthony Taylor: if we don't do this

1916
02:03:33.240 --> 02:03:35.340
Anthony Taylor: alright, so once that's loaded.

1917
02:03:35.350 --> 02:03:40.360
Anthony Taylor: go ahead and run the next one. This will get us our all. Mini. Lml. 6, v. 2. Model.

1918
02:03:41.080 --> 02:03:43.939
Anthony Taylor: And while that's running we can start

1919
02:03:44.000 --> 02:03:45.330
Anthony Taylor: reading in.

1920
02:03:45.680 --> 02:03:47.240
Anthony Taylor: No, we can't have the other

1921
02:03:47.430 --> 02:03:48.849
Anthony Taylor: doing all that mess.

1922
02:03:51.890 --> 02:03:52.880
Anthony Taylor: It is done.

1923
02:03:53.120 --> 02:03:55.309
Anthony Taylor: Okay, so let's read in our file

1924
02:03:56.000 --> 02:03:59.580
Anthony Taylor: news headlines. Underscore. Df.

1925
02:03:59.590 --> 02:04:03.690
Anthony Taylor: equals. Pd, dot read underscore Csv.

1926
02:04:03.730 --> 02:04:05.279
Anthony Taylor: and then this is

1927
02:04:06.040 --> 02:04:07.550
Anthony Taylor: resources

1928
02:04:08.180 --> 02:04:09.540
Anthony Taylor: slash.

1929
02:04:09.730 --> 02:04:12.450
Anthony Taylor: And if you didn't put it in resources, just

1930
02:04:12.650 --> 02:04:15.630
Anthony Taylor: type news headlines. Dot. Csv.

1931
02:04:16.520 --> 02:04:17.470
Anthony Taylor: okay?

1932
02:04:18.220 --> 02:04:20.230
Anthony Taylor: And then let's just take a quick look

1933
02:04:20.390 --> 02:04:22.270
Anthony Taylor: at what this looks like.

1934
02:04:26.170 --> 02:04:28.699
Anthony Taylor: Alright. So we've got some news headlines.

1935
02:04:29.920 --> 02:04:32.419
Anthony Taylor: Think we've seen these before? Right? I don't know.

1936
02:04:35.280 --> 02:04:36.170
Anthony Taylor: Okay.

1937
02:04:36.823 --> 02:04:40.360
Anthony Taylor: so we got that. So we're gonna convert the headlines

1938
02:04:40.800 --> 02:04:42.270
Anthony Taylor: to enlist.

1939
02:04:42.390 --> 02:04:46.499
Anthony Taylor: So we're just gonna grab the headlines column. So we're gonna say, news

1940
02:04:46.700 --> 02:04:49.170
Anthony Taylor: headlines, just news headlines.

1941
02:04:49.500 --> 02:04:52.790
Anthony Taylor: equals news headlines. Yeah.

1942
02:04:54.064 --> 02:04:56.920
Anthony Taylor: bracket, the column headlights

1943
02:04:57.470 --> 02:04:59.280
Anthony Taylor: and then got to list.

1944
02:05:01.490 --> 02:05:04.250
Anthony Taylor: Okay? And we can output that.

1945
02:05:06.960 --> 02:05:08.090
Anthony Taylor: And

1946
02:05:08.100 --> 02:05:11.040
Anthony Taylor: I got it. And when I type broad news.

1947
02:05:11.800 --> 02:05:12.980
Anthony Taylor: a Crc

1948
02:05:13.430 --> 02:05:15.250
Anthony Taylor: key error headlines.

1949
02:05:15.500 --> 02:05:16.590
Anthony Taylor: headline.

1950
02:05:20.570 --> 02:05:21.480
Anthony Taylor: there we go.

1951
02:05:21.760 --> 02:05:24.820
Anthony Taylor: So now we have a cool list of all our headlights.

1952
02:05:25.590 --> 02:05:27.440
Anthony Taylor: Okay, pretty exciting.

1953
02:05:27.740 --> 02:05:32.040
Anthony Taylor: So let's embed them. How do we do that? You guys remember?

1954
02:05:32.630 --> 02:05:34.380
Anthony Taylor: Remember, it was just one word.

1955
02:05:37.060 --> 02:05:37.770
Raugewitz, Tania: Token.

1956
02:05:37.960 --> 02:05:38.750
Anthony Taylor: Sorry

1957
02:05:39.960 --> 02:05:42.279
Anthony Taylor: not tokenized, because we're doing embeddings.

1958
02:05:42.280 --> 02:05:44.251
Raugewitz, Tania: I'm talking out loud. Sorry? Yeah.

1959
02:05:44.580 --> 02:05:45.280
Anthony Taylor: Oh.

1960
02:05:46.280 --> 02:05:48.510
Anthony Taylor: but you're doing good. You're so close

1961
02:05:49.780 --> 02:05:51.360
Anthony Taylor: model dot

1962
02:05:53.950 --> 02:05:54.980
Anthony Taylor: in code.

1963
02:05:56.820 --> 02:06:00.419
Anthony Taylor: Okay? And then we just pass in our list of headlights.

1964
02:06:05.230 --> 02:06:07.430
Anthony Taylor: Okay, once we run that

1965
02:06:07.600 --> 02:06:09.699
Anthony Taylor: we'll have our embeddings.

1966
02:06:11.004 --> 02:06:13.580
Anthony Taylor: Okay, so that's it.

1967
02:06:14.120 --> 02:06:15.489
Anthony Taylor: We? Oh, yeah, Mike.

1968
02:06:17.210 --> 02:06:18.590
michael mcpherson: He showed me how to load the

1969
02:06:18.620 --> 02:06:20.240
michael mcpherson: Dane resources.

1970
02:06:21.660 --> 02:06:22.700
Anthony Taylor: Over here.

1971
02:06:22.800 --> 02:06:25.360
Anthony Taylor: So yeah, right click, do new folder

1972
02:06:27.850 --> 02:06:31.110
Anthony Taylor: name it resources. And then you can either

1973
02:06:31.280 --> 02:06:36.079
Anthony Taylor: you can either upload here or you can click the 3 dots and do upload

1974
02:06:36.090 --> 02:06:38.640
Anthony Taylor: and then upload that file from resources.

1975
02:06:39.540 --> 02:06:40.200
Anthony Taylor: We good.

1976
02:06:40.200 --> 02:06:40.810
michael mcpherson: Thank you.

1977
02:06:41.560 --> 02:06:43.680
Anthony Taylor: Not a pro, not a pro.

1978
02:06:44.270 --> 02:06:49.179
Anthony Taylor: Okay. So here we have a new headline. So we've embedded. We're done.

1979
02:06:49.330 --> 02:06:51.569
Anthony Taylor: So we're going to get this new headline.

1980
02:06:52.490 --> 02:06:56.679
Anthony Taylor: And we're going to create a list of similarities.

1981
02:06:57.290 --> 02:07:01.069
Anthony Taylor: Okay? Oh, well, we didn't do anything. We just create that list.

1982
02:07:01.170 --> 02:07:03.419
Anthony Taylor: And then we're just going to. Now

1983
02:07:03.550 --> 02:07:06.389
Anthony Taylor: again, we're doing this kind of the hard way just to show it.

1984
02:07:07.118 --> 02:07:09.390
Anthony Taylor: We're gonna say 4.

1985
02:07:09.470 --> 02:07:12.750
Anthony Taylor: I comma headline and Betty

1986
02:07:14.280 --> 02:07:16.550
Anthony Taylor: headline

1987
02:07:18.040 --> 02:07:19.720
Anthony Taylor: as a news headline.

1988
02:07:23.990 --> 02:07:28.040
Anthony Taylor: Oh, oh, we can call it whatever we want. Headline. This is just a variable

1989
02:07:28.540 --> 02:07:30.120
Anthony Taylor: headline and vetting

1990
02:07:30.300 --> 02:07:32.690
Anthony Taylor: and enumerate

1991
02:07:33.870 --> 02:07:35.940
Anthony Taylor: news headline and business.

1992
02:07:36.280 --> 02:07:37.220
Anthony Taylor: There we go

1993
02:07:37.850 --> 02:07:40.510
Anthony Taylor: alright. So we have our for loop.

1994
02:07:40.640 --> 02:07:41.680
Anthony Taylor: We're going to do.

1995
02:07:42.100 --> 02:07:44.280
Anthony Taylor: I can't find my mouse there it is.

1996
02:07:44.720 --> 02:07:49.240
Anthony Taylor: We're going to do cosine similarities, so we'll do cosine

1997
02:07:49.410 --> 02:07:51.420
Anthony Taylor: similarity score

1998
02:07:53.940 --> 02:07:56.850
Anthony Taylor: equals util dot

1999
02:07:56.960 --> 02:07:59.600
Anthony Taylor: COS. Underscore SIM.

2000
02:08:00.070 --> 02:08:04.569
Anthony Taylor: Pass in the embedding from up above from right here in the for loop.

2001
02:08:05.000 --> 02:08:08.320
Anthony Taylor: Okay, and the new

2002
02:08:08.450 --> 02:08:09.700
Anthony Taylor: headline

2003
02:08:10.270 --> 02:08:13.099
Anthony Taylor: Embedding. Oh, did we encode it?

2004
02:08:17.590 --> 02:08:19.650
Anthony Taylor: Did I encode the new headline.

2005
02:08:23.680 --> 02:08:24.670
Anthony Taylor: anybody.

2006
02:08:26.060 --> 02:08:27.070
Dipinto, Matt: Yeah. Anthony

2007
02:08:27.590 --> 02:08:28.119
Dipinto, Matt: was nice.

2008
02:08:28.120 --> 02:08:28.950
Anthony Taylor: Didn't, did I?

2009
02:08:28.950 --> 02:08:30.120
Raugewitz, Tania: Auto. Oh, okay.

2010
02:08:30.810 --> 02:08:33.159
Anthony Taylor: No cause. See we we, we created it.

2011
02:08:34.000 --> 02:08:37.050
Anthony Taylor: but at no point did we encode it. So how do we encode it?

2012
02:08:39.730 --> 02:08:41.360
Anthony Taylor: We've already done it once.

2013
02:08:45.400 --> 02:08:46.620
Dipinto, Matt: Modeled on carrot.

2014
02:08:47.280 --> 02:08:48.260
Anthony Taylor: Thank you, Matt.

2015
02:08:50.540 --> 02:08:52.800
Anthony Taylor: model.in code.

2016
02:08:52.870 --> 02:08:54.490
Anthony Taylor: And then we just pass in

2017
02:08:54.700 --> 02:08:55.610
Anthony Taylor: new headline.

2018
02:08:57.950 --> 02:09:00.930
Anthony Taylor: Okay? So now we have the new headlining bit

2019
02:09:01.840 --> 02:09:02.750
Anthony Taylor: who just left.

2020
02:09:04.810 --> 02:09:07.360
Anthony Taylor: Okay, so we have the new headline embedding.

2021
02:09:07.560 --> 02:09:10.859
Anthony Taylor: and we have all of the embeddings for the all of it.

2022
02:09:10.880 --> 02:09:12.409
Anthony Taylor: So here we're saying.

2023
02:09:12.570 --> 02:09:17.600
Anthony Taylor: do a cosine similarity with the all of the headline, Embeddings from above.

2024
02:09:18.410 --> 02:09:20.420
Anthony Taylor: Actually, let me make sure they're all done.

2025
02:09:21.550 --> 02:09:24.350
Anthony Taylor: News headline Embeddings.

2026
02:09:24.520 --> 02:09:26.480
Anthony Taylor: right for each one

2027
02:09:26.570 --> 02:09:28.850
Anthony Taylor: in that news headline, Embeddings.

2028
02:09:28.960 --> 02:09:31.610
Anthony Taylor: Compare it to this headline Embedding.

2029
02:09:32.810 --> 02:09:35.479
Anthony Taylor: and then give us a score.

2030
02:09:35.780 --> 02:09:39.730
Anthony Taylor: And then with that we're going to add each score

2031
02:09:39.860 --> 02:09:41.430
Anthony Taylor: to this list

2032
02:09:43.090 --> 02:09:44.500
Anthony Taylor: and

2033
02:09:45.920 --> 02:09:47.980
Anthony Taylor: news headlines.

2034
02:09:48.870 --> 02:09:49.980
Anthony Taylor: I

2035
02:09:50.230 --> 02:09:52.500
Anthony Taylor: to get the the one we're in

2036
02:09:52.640 --> 02:09:53.900
Anthony Taylor: and the score

2037
02:09:59.100 --> 02:09:59.970
Anthony Taylor: there we go.

2038
02:10:00.560 --> 02:10:03.900
Anthony Taylor: Okay. And then when it's all done.

2039
02:10:04.440 --> 02:10:07.220
Anthony Taylor: we're gonna start it. Oh.

2040
02:10:07.410 --> 02:10:10.669
Anthony Taylor: well, that's nice something to just give it to us. Did they give it to us?

2041
02:10:10.780 --> 02:10:13.850
Anthony Taylor: Similarities? That sort? Yeah, that's good.

2042
02:10:16.700 --> 02:10:19.920
Anthony Taylor: List of pen takes exactly one argument 2. We're given.

2043
02:10:20.320 --> 02:10:23.729
Anthony Taylor: So where did I? Oh, put this in parentheses.

2044
02:10:29.750 --> 02:10:30.869
Anthony Taylor: Let's try that.

2045
02:10:31.660 --> 02:10:32.849
Anthony Taylor: There we go.

2046
02:10:37.090 --> 02:10:38.770
Anthony Taylor: And then now we can

2047
02:10:40.860 --> 02:10:47.780
Anthony Taylor: print it out. So here we're gonna say, to categorize the new headline, we're gonna loop through this list we just made

2048
02:10:48.090 --> 02:10:51.989
Anthony Taylor: and output the similarity scores.

2049
02:10:52.730 --> 02:10:57.030
Anthony Taylor: Okay, so we're gonna say, for I

2050
02:10:58.590 --> 02:10:59.040
Anthony Taylor: Comma

2051
02:11:01.090 --> 02:11:04.019
Anthony Taylor: wow! I have no idea what I was typing

2052
02:11:04.230 --> 02:11:05.709
Anthony Taylor: for. I

2053
02:11:05.720 --> 02:11:07.100
Anthony Taylor: parentheses

2054
02:11:07.210 --> 02:11:08.510
Anthony Taylor: headline

2055
02:11:09.020 --> 02:11:10.040
Anthony Taylor: comma

2056
02:11:10.360 --> 02:11:12.430
Anthony Taylor: similarity score.

2057
02:11:20.750 --> 02:11:22.050
Anthony Taylor: Oh, my goodness!

2058
02:11:23.890 --> 02:11:29.070
Anthony Taylor: Both parentheses. And and then we're going to enumerate that list of similarities.

2059
02:11:38.570 --> 02:11:39.430
Anthony Taylor: Okay.

2060
02:11:39.710 --> 02:11:40.430
Anthony Taylor: alright.

2061
02:11:40.770 --> 02:11:42.560
Anthony Taylor: So we have that.

2062
02:11:45.390 --> 02:11:48.270
Anthony Taylor: We're gonna get the category. Holy.

2063
02:11:48.700 --> 02:11:52.259
Anthony Taylor: Alright, I'm gonna get through this one. I'm gonna give you guys the code for this one.

2064
02:11:52.440 --> 02:11:54.649
Anthony Taylor: I know you want me to type it.

2065
02:11:54.890 --> 02:11:57.890
Anthony Taylor: You do this too much, and I'm gonna run out of time. So.

2066
02:11:57.890 --> 02:12:00.019
Raugewitz, Tania: Going to change the fur to 4 right.

2067
02:12:00.650 --> 02:12:01.689
Anthony Taylor: Sure why not?

2068
02:12:01.740 --> 02:12:03.210
Anthony Taylor: Did I did fur.

2069
02:12:03.430 --> 02:12:04.070
Raugewitz, Tania: FIR.

2070
02:12:04.070 --> 02:12:06.930
Anthony Taylor: That's so funny. I sure did. That's hilarious.

2071
02:12:08.070 --> 02:12:10.580
Anthony Taylor: Okay, and live. Is this whole cell.

2072
02:12:11.450 --> 02:12:13.120
Anthony Taylor: or well, is the whole loop?

2073
02:12:13.660 --> 02:12:14.910
Anthony Taylor: I should say

2074
02:12:16.140 --> 02:12:18.960
Anthony Taylor: alright. So I will go through it, though at least

2075
02:12:22.530 --> 02:12:34.550
Anthony Taylor: So we're going to do 4. We have an I, and then in this parentheses, we're basically going to pass in 2 values. So the I is the index. And these are the 2 values that we're going to enumerate

2076
02:12:34.570 --> 02:12:42.359
Anthony Taylor: out of our similarities list. So category equals the news headline data frame. Locate

2077
02:12:42.400 --> 02:12:45.310
Anthony Taylor: the headline that equals headline

2078
02:12:45.780 --> 02:12:47.120
Anthony Taylor: what we pass in.

2079
02:12:47.400 --> 02:12:48.593
Anthony Taylor: And then,

2080
02:12:50.650 --> 02:12:52.970
Anthony Taylor: yeah. And then kind of and then it's category.

2081
02:12:53.330 --> 02:12:58.360
Anthony Taylor: Then we're going to get the values of that or print out the rank category. The headline.

2082
02:12:58.400 --> 02:13:01.029
Anthony Taylor: the Similarity Score prints it all out.

2083
02:13:01.280 --> 02:13:02.730
Anthony Taylor: It's a beautiful thing.

2084
02:13:03.100 --> 02:13:05.460
Anthony Taylor: Okay? So here we could see

2085
02:13:05.930 --> 02:13:09.019
Anthony Taylor: top 10 hacks for traveling like a pro was the one we

2086
02:13:10.830 --> 02:13:12.640
Anthony Taylor: that's most like

2087
02:13:13.120 --> 02:13:14.960
Anthony Taylor: technology headline.

2088
02:13:14.970 --> 02:13:18.699
Anthony Taylor: Acker pleads guilty to stealing over 77,000 passwords.

2089
02:13:19.500 --> 02:13:20.600
Anthony Taylor: Is that right?

2090
02:13:21.240 --> 02:13:22.789
Anthony Taylor: That was impressive?

2091
02:13:23.600 --> 02:13:24.750
Anthony Taylor: Okay.

2092
02:13:26.540 --> 02:13:28.120
Anthony Taylor: that is what it's most like.

2093
02:13:28.320 --> 02:13:31.650
Anthony Taylor: So what category is the new headlight?

2094
02:13:43.090 --> 02:13:45.730
Anthony Taylor: What do you guys think, is it technology.

2095
02:13:47.550 --> 02:13:48.800
Anthony Taylor: this headline?

2096
02:13:51.380 --> 02:13:53.310
Anthony Taylor: It is not Gabe

2097
02:13:54.190 --> 02:13:58.219
Anthony Taylor: travel. This is what came up. Yeah, this is really travel.

2098
02:13:58.410 --> 02:14:00.750
Anthony Taylor: Okay? It clearly missed missed it

2099
02:14:01.070 --> 02:14:02.540
Anthony Taylor: alright. But

2100
02:14:03.650 --> 02:14:04.809
Anthony Taylor: that's when it came up.

2101
02:14:05.100 --> 02:14:09.649
Anthony Taylor: Okay, now, what can we do to fix this? Not a whole lot, really.

2102
02:14:10.400 --> 02:14:14.679
Anthony Taylor: Okay, there's really not a ton you can do to actually go and fix it.

2103
02:14:15.503 --> 02:14:18.249
Anthony Taylor: It's just one of those things. It's it's

2104
02:14:18.590 --> 02:14:22.920
Anthony Taylor: it's so weird that they gave you a demo that actually gave you the wrong category.

2105
02:14:23.120 --> 02:14:24.070
Anthony Taylor: But

2106
02:14:28.270 --> 02:14:29.380
Anthony Taylor: That's how you do.

2107
02:14:29.560 --> 02:14:43.689
Anthony Taylor: The important thing is not that it match to the wrong one, but these are the steps to do it. So just a couple of quick notes. You know, I've been telling you guys lately, we have a lot of stuff that's in here just for purpose of demonstration.

2108
02:14:44.190 --> 02:14:47.210
Anthony Taylor: Okay, what did we really need to do

2109
02:14:48.050 --> 02:14:49.680
Anthony Taylor: needed to load the data

2110
02:14:51.530 --> 02:14:53.549
Anthony Taylor: we needed to

2111
02:14:54.730 --> 02:14:56.430
Anthony Taylor: put the data in a list

2112
02:14:56.510 --> 02:14:57.930
Anthony Taylor: and encoded.

2113
02:15:01.070 --> 02:15:02.660
Anthony Taylor: That's it, the prep it

2114
02:15:02.950 --> 02:15:07.349
Anthony Taylor: to do the similarity test. We needed something to compare to.

2115
02:15:07.930 --> 02:15:09.160
Anthony Taylor: And

2116
02:15:10.580 --> 02:15:12.260
Anthony Taylor: basically this.

2117
02:15:13.100 --> 02:15:17.869
Anthony Taylor: okay? And then you just get the one with the highest similarity. That's it.

2118
02:15:19.160 --> 02:15:24.199
Anthony Taylor: that's all you needed. And you're done. All this other stuff is just Fluff. So you have something to look at

2119
02:15:25.930 --> 02:15:31.310
Anthony Taylor: alright, because it makes more sense when you see the answer, even though it wasn't right, it was close.

2120
02:15:31.660 --> 02:15:32.810
Anthony Taylor: But it was right.

2121
02:15:33.430 --> 02:15:34.300
Anthony Taylor: Okay.

2122
02:15:35.812 --> 02:15:38.559
Anthony Taylor: I want to get one more in.

2123
02:15:42.120 --> 02:15:42.900
Anthony Taylor: Oh.

2124
02:15:43.940 --> 02:15:44.760
Anthony Taylor: okay.

2125
02:15:44.900 --> 02:15:46.090
Anthony Taylor: fine.

2126
02:15:46.860 --> 02:15:48.980
Anthony Taylor: Sonya had to go. Had a technical issue.

2127
02:15:49.190 --> 02:15:53.069
Anthony Taylor: Okay, let's try to get this last one in.

2128
02:15:53.370 --> 02:15:56.919
Anthony Taylor: It's a very cool one. That's why I wanted us to get here.

2129
02:15:58.240 --> 02:16:05.380
Anthony Taylor: It's a partner do that would have been really, that means it's hard anytime. You see, partner, do. That means it's hard.

2130
02:16:09.170 --> 02:16:12.710
Anthony Taylor: Okay? And this has resources also. So do the same thing.

2131
02:16:13.090 --> 02:16:15.090
Anthony Taylor: Start this first cell

2132
02:16:18.650 --> 02:16:20.300
Anthony Taylor: while it's running.

2133
02:16:21.070 --> 02:16:25.360
Anthony Taylor: Come in here. I'm going to create a resources folder.

2134
02:16:28.180 --> 02:16:33.160
Anthony Taylor: And then I'm going to upload into it my resources. There's 2

2135
02:16:33.500 --> 02:16:35.099
Anthony Taylor: files in this one.

2136
02:16:39.870 --> 02:16:41.930
Anthony Taylor: Make sure they're in there. Okay.

2137
02:16:42.570 --> 02:16:43.849
Anthony Taylor: once you have that.

2138
02:16:44.700 --> 02:16:47.009
Anthony Taylor: It's just a matter of waiting for the Pip install

2139
02:16:47.690 --> 02:16:52.279
Anthony Taylor: which fortunately is going way faster tonight than it did earlier today. Let me tell you

2140
02:17:03.459 --> 02:17:04.440
Anthony Taylor: all right.

2141
02:17:28.170 --> 02:17:30.430
Meredith McCanse (she/her): Hey, Anthony, can I ask a question while we're waiting on that.

2142
02:17:30.430 --> 02:17:31.359
Anthony Taylor: Yeah. Go ahead.

2143
02:17:32.049 --> 02:17:36.719
Meredith McCanse (she/her): On the last one that we did in this folder on the left hand or the folder

2144
02:17:36.799 --> 02:17:49.826
Meredith McCanse (she/her): thing on the left hand column. I accidentally clicked accidentally, clicked that sort of folder with this 2 dots after it, and it, I think it took me like up a level to like a whole subset of fold, or a whole other set of folders.

2145
02:17:50.510 --> 02:17:50.870
Anthony Taylor: Got it.

2146
02:17:50.879 --> 02:17:56.959
Meredith McCanse (she/her): Yup, and I couldn't get out of that like if you land here, how do you get back to the other view? You were just in.

2147
02:17:57.700 --> 02:17:59.180
Anthony Taylor: A good question.

2148
02:18:00.840 --> 02:18:02.089
Anthony Taylor: That's not it.

2149
02:18:04.470 --> 02:18:10.590
Anthony Taylor: We gotta figure out it's probably content. Yeah, content. So click on content.

2150
02:18:10.799 --> 02:18:11.709
Meredith McCanse (she/her): Okay.

2151
02:18:11.979 --> 02:18:12.519
Meredith McCanse (she/her): got it?

2152
02:18:12.520 --> 02:18:12.880
Anthony Taylor: In, your.

2153
02:18:12.889 --> 02:18:13.429
Meredith McCanse (she/her): Thank you.

2154
02:18:13.430 --> 02:18:15.220
Anthony Taylor: Resources. Yeah, there you go.

2155
02:18:15.500 --> 02:18:16.209
Meredith McCanse (she/her): Okay.

2156
02:18:16.469 --> 02:18:20.429
Anthony Taylor: Okay. So we got this first one, we got our pip install done.

2157
02:18:20.579 --> 02:18:24.269
Anthony Taylor: and we're gonna bring in the same models. We're gonna bring pandas

2158
02:18:29.249 --> 02:18:32.919
Anthony Taylor: and then we got to bring in our ham. Spam. Csv.

2159
02:18:34.229 --> 02:18:37.479
Anthony Taylor: After that finish, pushing my screen off

2160
02:18:38.129 --> 02:18:39.589
Anthony Taylor: SMS,

2161
02:18:40.069 --> 02:18:42.819
Anthony Taylor: and this we've seen this one before. This is that

2162
02:18:45.279 --> 02:18:46.779
Anthony Taylor: basically the

2163
02:18:47.829 --> 02:18:50.999
Anthony Taylor: ham or spam dot Csv file

2164
02:18:55.119 --> 02:18:56.239
Anthony Taylor: who came back.

2165
02:18:57.839 --> 02:19:00.009
Anthony Taylor: Oh, you got disconnected, too.

2166
02:19:06.919 --> 02:19:07.999
Anthony Taylor: Okay.

2167
02:19:08.519 --> 02:19:13.249
Anthony Taylor: and there you go. So we have seen this before feel pretty good about that.

2168
02:19:13.861 --> 02:19:16.149
Anthony Taylor: We could do an inf, or

2169
02:19:16.539 --> 02:19:20.339
Anthony Taylor: we can do. We want to see how many labels we have.

2170
02:19:20.549 --> 02:19:23.309
Anthony Taylor: so we'll do. What? What's the python

2171
02:19:24.049 --> 02:19:25.609
Anthony Taylor: command to get

2172
02:19:26.579 --> 02:19:27.809
Anthony Taylor: this output.

2173
02:19:31.536 --> 02:19:32.349
Meredith McCanse (she/her): You counts.

2174
02:19:33.129 --> 02:19:34.339
Anthony Taylor: Yeah.

2175
02:19:35.189 --> 02:19:38.209
Anthony Taylor: love that. I love it. When you guys get that stuff.

2176
02:19:42.279 --> 02:19:46.079
Anthony Taylor: So we have 83 of those 17 of those.

2177
02:19:48.789 --> 02:19:53.549
Anthony Taylor: oh, okay? So now we're gonna get the unclassified text messages.

2178
02:19:54.039 --> 02:19:59.039
Anthony Taylor: Okay? So we're gonna say, unclassified

2179
02:20:02.089 --> 02:20:03.139
Anthony Taylor: text.

2180
02:20:03.349 --> 02:20:06.109
Anthony Taylor: yeah, make it text with an S

2181
02:20:06.669 --> 02:20:07.919
Anthony Taylor: just to be

2182
02:20:08.229 --> 02:20:09.529
Anthony Taylor: the same

2183
02:20:11.131 --> 02:20:13.539
Anthony Taylor: equals. And then we're going to read in

2184
02:20:20.969 --> 02:20:22.149
Anthony Taylor: resource

2185
02:20:22.369 --> 02:20:23.329
Anthony Taylor: quotes.

2186
02:20:28.129 --> 02:20:31.469
Anthony Taylor: it's funny. It started to give them to me. I would have been happy with that.

2187
02:20:43.149 --> 02:20:44.639
Anthony Taylor: Okay.

2188
02:20:51.439 --> 02:20:52.669
Anthony Taylor: the

2189
02:20:56.249 --> 02:21:01.169
Anthony Taylor: alright. So we've got our unclassified ones, even though they do say, label on them.

2190
02:21:01.549 --> 02:21:02.919
Anthony Taylor: But that's all right.

2191
02:21:03.499 --> 02:21:07.159
Anthony Taylor: Okay, so let's do our value counts on these.

2192
02:21:07.709 --> 02:21:10.289
Anthony Taylor: So we have unclassified

2193
02:21:10.319 --> 02:21:12.019
Anthony Taylor: text, es

2194
02:21:12.409 --> 02:21:14.279
Anthony Taylor: a label

2195
02:21:16.889 --> 02:21:19.079
Anthony Taylor: dot value counts.

2196
02:21:21.569 --> 02:21:26.619
Anthony Taylor: Alright got 25 of those. Alright. So now that we have these.

2197
02:21:26.629 --> 02:21:31.029
Anthony Taylor: let's go ahead. And what do we got to do first? What do we say in that last one?

2198
02:21:31.419 --> 02:21:33.269
Anthony Taylor: What's the first thing we need to do

2199
02:21:38.229 --> 02:21:39.319
Anthony Taylor: anybody

2200
02:21:41.639 --> 02:21:43.829
Anthony Taylor: before encoding? What did we do.

2201
02:21:45.130 --> 02:21:46.530
Meredith McCanse (she/her): Turn it into a list.

2202
02:21:47.230 --> 02:21:49.330
Anthony Taylor: Exactly.

2203
02:21:49.480 --> 02:21:52.269
Anthony Taylor: So. First thing I do is grab our messages

2204
02:21:55.360 --> 02:21:57.539
Anthony Taylor: and then say to Liz.

2205
02:22:03.430 --> 02:22:04.480
Anthony Taylor: okay.

2206
02:22:12.590 --> 02:22:16.800
Anthony Taylor: you know. Just for consistency. Make it messages.

2207
02:22:17.110 --> 02:22:21.330
Anthony Taylor: And then now we can see there's all of it there that it's gonna show us.

2208
02:22:21.500 --> 02:22:26.000
Anthony Taylor: So now we just need to do what after we make it a list. What's the next step.

2209
02:22:31.960 --> 02:22:33.290
Meredith McCanse (she/her): Auto and code.

2210
02:22:34.280 --> 02:22:36.279
Anthony Taylor: Just encode, absolutely.

2211
02:22:36.280 --> 02:22:36.720
Meredith McCanse (she/her): No.

2212
02:22:36.720 --> 02:22:38.070
Anthony Taylor: And code.

2213
02:22:39.120 --> 02:22:43.869
Anthony Taylor: So we're gonna we're gonna encode, which gives us our embeddings

2214
02:22:45.000 --> 02:22:46.440
Anthony Taylor: equals

2215
02:22:46.540 --> 02:22:48.290
Anthony Taylor: a model

2216
02:22:48.410 --> 02:22:52.490
Anthony Taylor: dot in code. We're passing in our

2217
02:22:53.190 --> 02:22:55.660
Anthony Taylor: whatever we called this one text messages.

2218
02:22:57.300 --> 02:22:59.899
Anthony Taylor: texts, classified text messages.

2219
02:23:04.140 --> 02:23:05.130
Anthony Taylor: There's so much.

2220
02:23:06.990 --> 02:23:07.730
Anthony Taylor: Duke.

2221
02:23:07.890 --> 02:23:08.810
Anthony Taylor: okay?

2222
02:23:09.630 --> 02:23:11.839
Anthony Taylor: And now they're encoded.

2223
02:23:12.330 --> 02:23:14.300
Anthony Taylor: Well, they're on their way to be in the code.

2224
02:23:14.790 --> 02:23:19.660
Anthony Taylor: While that's happening, we'll do the same thing with the unclassified.

2225
02:23:20.040 --> 02:23:22.699
Anthony Taylor: Okay, so we have our unclassified

2226
02:23:25.670 --> 02:23:26.630
Anthony Taylor: text

2227
02:23:28.000 --> 02:23:29.260
Anthony Taylor: messages

2228
02:23:30.300 --> 02:23:33.500
Anthony Taylor: equals. And we gotta make them into a list. So we're gonna do.

2229
02:23:34.520 --> 02:23:37.969
Anthony Taylor: I'm I'm like pushing buttons all over my keyboard here.

2230
02:23:38.440 --> 02:23:40.649
Anthony Taylor: unclassified text, yeah.

2231
02:23:41.190 --> 02:23:43.859
Anthony Taylor: Brackets. Text message

2232
02:23:45.950 --> 02:23:47.599
Anthony Taylor: dot to this.

2233
02:23:48.030 --> 02:23:49.250
Anthony Taylor: And we've done it.

2234
02:23:49.420 --> 02:23:50.400
Anthony Taylor: J.

2235
02:23:50.640 --> 02:23:53.390
Anthony Taylor: I'm not going to print those out. They take up too much space.

2236
02:23:53.910 --> 02:23:59.010
Anthony Taylor: And then, last, but not least, we need to do the embeddings. So we're gonna do

2237
02:24:00.220 --> 02:24:02.170
Anthony Taylor: unclassified

2238
02:24:05.640 --> 02:24:07.500
Anthony Taylor: underscore text.

2239
02:24:07.790 --> 02:24:09.439
Anthony Taylor: So no message

2240
02:24:11.000 --> 02:24:12.989
Anthony Taylor: underscore embeddings

2241
02:24:14.240 --> 02:24:15.550
Anthony Taylor: equals

2242
02:24:16.100 --> 02:24:17.909
Anthony Taylor: model.in code.

2243
02:24:23.360 --> 02:24:25.410
Anthony Taylor: unclassified

2244
02:24:26.860 --> 02:24:28.489
Anthony Taylor: text messages.

2245
02:24:30.250 --> 02:24:31.210
Anthony Taylor: Prana

2246
02:24:31.830 --> 02:24:32.640
Anthony Taylor: done.

2247
02:24:32.890 --> 02:24:35.190
Anthony Taylor: Okay. So now we have

2248
02:24:35.810 --> 02:24:37.050
Anthony Taylor: our

2249
02:24:38.326 --> 02:24:40.190
Anthony Taylor: both of them embedded.

2250
02:24:40.250 --> 02:24:42.560
Anthony Taylor: encoded, and embedded and ready to go.

2251
02:24:42.820 --> 02:24:44.710
Anthony Taylor: Okay. So

2252
02:24:44.840 --> 02:24:47.030
Anthony Taylor: again, I'm not gonna

2253
02:24:47.080 --> 02:24:48.609
Anthony Taylor: type all of this.

2254
02:24:49.165 --> 02:24:53.580
Anthony Taylor: This for loop, but we will go through it. So here, I'll give it to you

2255
02:24:54.840 --> 02:24:57.730
Anthony Taylor: just the just from the loop down.

2256
02:24:59.460 --> 02:25:04.680
Anthony Taylor: It's not that I don't want to type it. It's just I know I'm going to make like 52 mistakes.

2257
02:25:05.500 --> 02:25:07.320
Anthony Taylor: and I'd rather just give it to

2258
02:25:08.280 --> 02:25:10.460
Anthony Taylor: alright. So from here

2259
02:25:10.820 --> 02:25:12.000
Anthony Taylor: to here

2260
02:25:12.200 --> 02:25:14.179
Anthony Taylor: paste what I just put in the slap.

2261
02:25:16.040 --> 02:25:17.040
Anthony Taylor: Aye.

2262
02:25:17.440 --> 02:25:20.190
Anthony Taylor: and let's go through

2263
02:25:23.810 --> 02:25:29.000
Anthony Taylor: Did I not call it classified message? I called it classified text embeddings.

2264
02:25:30.180 --> 02:25:35.039
Anthony Taylor: So you can either change the variable up there or change it right here. It doesn't.

2265
02:25:38.130 --> 02:25:40.340
Anthony Taylor: Okay. So let's walk through this.

2266
02:25:40.400 --> 02:25:43.690
Anthony Taylor: So we created 2 empty lists.

2267
02:25:44.270 --> 02:25:47.280
Anthony Taylor: Okay, unclassified similarities.

2268
02:25:47.290 --> 02:25:48.960
Anthony Taylor: classified similarity.

2269
02:25:51.450 --> 02:25:56.950
Anthony Taylor: we're going to use the Zip function to pack the unclassified text and their embedding list.

2270
02:25:57.060 --> 02:26:01.300
Anthony Taylor: So foreign classified message, unclassified message, embedding in

2271
02:26:01.730 --> 02:26:02.600
Anthony Taylor: dip.

2272
02:26:02.720 --> 02:26:07.249
Anthony Taylor: unclassified text message and unclassified message embedding.

2273
02:26:07.860 --> 02:26:08.990
Anthony Taylor: Okay.

2274
02:26:10.180 --> 02:26:12.139
Anthony Taylor: so that's going to loop us through that.

2275
02:26:12.520 --> 02:26:16.550
Anthony Taylor: Then we're going to also. So we have that. So for each one of those

2276
02:26:16.610 --> 02:26:21.650
Anthony Taylor: we're going to loop through the classified message and embedding

2277
02:26:22.840 --> 02:26:30.700
Anthony Taylor: alright and do a cosine similarity. So was that me? So the first one you'll take the first unclassified text message.

2278
02:26:31.180 --> 02:26:32.690
Anthony Taylor: and it's embedding.

2279
02:26:33.620 --> 02:26:35.049
Anthony Taylor: Come to here.

2280
02:26:35.200 --> 02:26:37.749
Anthony Taylor: Go to the first classified one

2281
02:26:37.940 --> 02:26:41.339
Anthony Taylor: and embedding, do a cosine similarity

2282
02:26:41.880 --> 02:26:43.130
Anthony Taylor: and

2283
02:26:44.390 --> 02:26:48.180
Anthony Taylor: append that to the classified similarities list.

2284
02:26:48.490 --> 02:26:50.060
Anthony Taylor: Then it's going to do that

2285
02:26:50.710 --> 02:26:52.910
Anthony Taylor: for all of the messages

2286
02:26:52.930 --> 02:26:54.000
Anthony Taylor: in

2287
02:26:54.230 --> 02:26:56.430
Anthony Taylor: the first data set. The classified.

2288
02:26:57.130 --> 02:27:01.870
Anthony Taylor: Okay, then gonna come up here, get the next unclassified message

2289
02:27:02.150 --> 02:27:03.440
Anthony Taylor: do the same thing.

2290
02:27:03.940 --> 02:27:09.050
Anthony Taylor: So there's 25 of these. There's a whole bunch of these. This is, gonna take a second.

2291
02:27:09.740 --> 02:27:13.339
Anthony Taylor: When it's all done. It's going to give us the top 5

2292
02:27:13.630 --> 02:27:15.690
Anthony Taylor: classified similarities.

2293
02:27:16.725 --> 02:27:20.610
Anthony Taylor: And then put those into this larger

2294
02:27:21.170 --> 02:27:24.020
Anthony Taylor: list. Okay, so we'll run that.

2295
02:27:29.860 --> 02:27:32.249
Anthony Taylor: And then we have one more loop

2296
02:27:39.920 --> 02:27:42.559
Anthony Taylor: alright, and I'm gonna give you this loop, too.

2297
02:27:45.700 --> 02:27:47.960
Anthony Taylor: And again, we'll walk through it.

2298
02:27:53.400 --> 02:27:54.970
Anthony Taylor: So for this one just

2299
02:27:55.030 --> 02:27:57.540
Anthony Taylor: highlight the wholesale and paste.

2300
02:27:58.920 --> 02:27:59.910
Anthony Taylor: Alright.

2301
02:28:00.370 --> 02:28:03.470
Anthony Taylor: Okay. So now we're gonna say.

2302
02:28:04.010 --> 02:28:07.619
Anthony Taylor: in this unclassified similarities list.

2303
02:28:08.520 --> 02:28:09.370
Anthony Taylor: which

2304
02:28:09.630 --> 02:28:10.580
Anthony Taylor: I think

2305
02:28:11.610 --> 02:28:12.390
Anthony Taylor: color

2306
02:28:17.300 --> 02:28:22.270
Anthony Taylor: you could see there is. I wanted to wondered if there was a variable sign.

2307
02:28:23.710 --> 02:28:25.310
Anthony Taylor: you know, like we have in our

2308
02:28:27.950 --> 02:28:28.800
Anthony Taylor: yeah

2309
02:28:28.980 --> 02:28:30.560
Anthony Taylor: know that there is

2310
02:28:33.955 --> 02:28:34.900
Anthony Taylor: There we go.

2311
02:28:37.670 --> 02:28:39.759
Anthony Taylor: So if we look at

2312
02:28:40.470 --> 02:28:44.080
Anthony Taylor: unclassified similarities, this list right here.

2313
02:28:46.050 --> 02:28:47.419
Anthony Taylor: you can see down here.

2314
02:28:47.430 --> 02:28:53.580
Anthony Taylor: Would your little bow on your time you didn't complete? So this is what's in that list? Basically.

2315
02:28:53.940 --> 02:28:54.920
Anthony Taylor: Okay.

2316
02:28:55.870 --> 02:28:58.340
Anthony Taylor: alright. So that's cool.

2317
02:29:03.050 --> 02:29:10.340
Anthony Taylor: So it's going to go through that list. It's going to get the unclassified message and the top 5 similar messages

2318
02:29:10.860 --> 02:29:12.610
Anthony Taylor: gonna print the message.

2319
02:29:12.790 --> 02:29:20.430
Anthony Taylor: print top 5 similarities loop through those 5 similarities and give you the 5 messages that are most similar.

2320
02:29:20.900 --> 02:29:23.960
Anthony Taylor: Okay? So it's gonna get through those

2321
02:29:23.990 --> 02:29:26.920
Anthony Taylor: label. It's gonna grab the first message

2322
02:29:27.550 --> 02:29:31.100
Anthony Taylor: in this label, and then it's going to give you the rank.

2323
02:29:31.450 --> 02:29:32.680
Anthony Taylor: the message.

2324
02:29:33.060 --> 02:29:34.810
Anthony Taylor: and the similarity score.

2325
02:29:34.890 --> 02:29:36.980
Anthony Taylor: And with that you should get

2326
02:29:37.620 --> 02:29:38.400
Anthony Taylor: the.

2327
02:29:38.980 --> 02:29:44.140
Anthony Taylor: So the unclassified mess, so that takes away some money worries

2328
02:29:44.930 --> 02:29:46.379
Anthony Taylor: which similar

2329
02:29:46.640 --> 02:29:50.699
Anthony Taylor: winner as a valued network for you have been select, received prize award

2330
02:29:50.880 --> 02:29:53.019
Anthony Taylor: the claim call blah blah, blah

2331
02:29:53.250 --> 02:29:58.060
Anthony Taylor: urgent. You have 1 one week free membership label Blah blah.

2332
02:29:58.890 --> 02:30:02.529
Anthony Taylor: I we could pick some up you open before tonight.

2333
02:30:07.630 --> 02:30:11.129
Anthony Taylor: It's funny that it did the same one. It looks like it's the same one on every one of

2334
02:30:14.630 --> 02:30:15.950
Anthony Taylor: that's kind of interesting. Huh?

2335
02:30:17.180 --> 02:30:18.349
Anthony Taylor: Same one

2336
02:30:18.910 --> 02:30:20.130
Anthony Taylor: same one

2337
02:30:21.120 --> 02:30:22.370
Anthony Taylor: that doesn't look great.

2338
02:30:30.250 --> 02:30:33.229
Anthony Taylor: Oh, I see. I'm like, way at the bottom.

2339
02:30:35.250 --> 02:30:36.140
Anthony Taylor: Okay.

2340
02:30:37.290 --> 02:30:38.330
Anthony Taylor: that was weird.

2341
02:30:38.360 --> 02:30:43.420
Anthony Taylor: Would your little ones like a call from Santa Christmas Eve text. Yes, so it has similarity.

2342
02:30:43.860 --> 02:30:46.280
Anthony Taylor: Did you call me just now? A

2343
02:30:46.750 --> 02:30:51.699
Anthony Taylor: sorry? I'll call you later. So, in my opinion, this is definitely spam.

2344
02:30:52.300 --> 02:30:53.899
Anthony Taylor: but it identified it

2345
02:30:53.920 --> 02:30:55.070
Anthony Taylor: as ham.

2346
02:30:57.750 --> 02:31:00.509
Anthony Taylor: You didn't complete your gift. Oh.

2347
02:31:01.820 --> 02:31:02.710
Anthony Taylor: okay.

2348
02:31:04.840 --> 02:31:06.569
Anthony Taylor: But anyway, so

2349
02:31:06.990 --> 02:31:09.349
Anthony Taylor: this is what similarity testing looks like.

2350
02:31:09.920 --> 02:31:11.050
Anthony Taylor: okay.

2351
02:31:12.300 --> 02:31:16.430
Anthony Taylor: it's a lot. There's a lot to to take in there a whole lot.

2352
02:31:17.190 --> 02:31:19.670
Anthony Taylor: Alright. But the bottom line is

2353
02:31:19.780 --> 02:31:21.300
Anthony Taylor: just to summarize.

2354
02:31:22.150 --> 02:31:23.510
Anthony Taylor: You read in the data.

2355
02:31:24.200 --> 02:31:25.989
Anthony Taylor: create a list from the data.

2356
02:31:26.270 --> 02:31:29.439
Anthony Taylor: encode the data which creates your embeddings.

2357
02:31:29.620 --> 02:31:35.190
Anthony Taylor: And then using one of 3 options at least that we've talked about.

2358
02:31:35.420 --> 02:31:36.220
Anthony Taylor: you

2359
02:31:36.470 --> 02:31:38.360
Anthony Taylor: calculate the similarity.

2360
02:31:39.310 --> 02:31:40.350
Anthony Taylor: Okay?

2361
02:31:40.430 --> 02:31:43.790
Anthony Taylor: And then, ultimately, all you really care about

2362
02:31:43.930 --> 02:31:47.200
Anthony Taylor: is the ones that it's most likely

2363
02:31:47.410 --> 02:31:48.689
Anthony Taylor: to be. Now.

2364
02:31:48.990 --> 02:31:50.349
Anthony Taylor: I would say

2365
02:31:51.810 --> 02:31:53.869
Anthony Taylor: from a confidence level.

2366
02:31:54.100 --> 02:31:56.300
Anthony Taylor: these similarity scores are pretty low.

2367
02:31:58.080 --> 02:32:05.059
Anthony Taylor: Okay, so maybe you're just like, you know what I mean? Obviously, this is spam, or I don't know that one, that first one.

2368
02:32:05.110 --> 02:32:07.289
Anthony Taylor: no question. This is spam.

2369
02:32:07.330 --> 02:32:09.030
Anthony Taylor: Look at these similarities, folks.

2370
02:32:10.570 --> 02:32:14.349
Anthony Taylor: very little confidence that we have anything similar.

2371
02:32:15.240 --> 02:32:16.800
Anthony Taylor: But that's part of the job.

2372
02:32:17.440 --> 02:32:19.290
Anthony Taylor: Okay, yeah, that is out of the data

2373
02:32:20.620 --> 02:32:21.730
Anthony Taylor: best I could do.

2374
02:32:22.190 --> 02:32:23.080
Anthony Taylor: Okay.

2375
02:32:25.110 --> 02:32:26.400
Anthony Taylor: questions.

2376
02:32:33.240 --> 02:32:36.130
Anthony Taylor: was that so much fun? Did you guys enjoy that?

2377
02:32:37.613 --> 02:32:40.329
Anthony Taylor: Before I let you go?

2378
02:32:57.170 --> 02:32:58.990
Anthony Taylor: So what's some other?

2379
02:32:59.760 --> 02:33:02.650
Anthony Taylor: Let's talk about. There's a couple of quick questions on this.

2380
02:33:03.000 --> 02:33:07.399
Anthony Taylor: So did the similarity scores for the unclassified text messages agree.

2381
02:33:07.970 --> 02:33:12.970
Anthony Taylor: Hmm, with the label that was given. Yeah. Hmm, not really

2382
02:33:13.110 --> 02:33:15.560
Anthony Taylor: right. We saw some, some bad ones.

2383
02:33:15.840 --> 02:33:19.099
Anthony Taylor: What other method would you use to confirm

2384
02:33:19.350 --> 02:33:21.950
Anthony Taylor: the classification of the text messages?

2385
02:33:29.140 --> 02:33:31.639
Anthony Taylor: Well, what was the one thing

2386
02:33:32.270 --> 02:33:35.259
Anthony Taylor: from all of the modeling that we've learned so far.

2387
02:33:35.690 --> 02:33:38.560
Anthony Taylor: What was the one thing that was?

2388
02:33:39.640 --> 02:33:41.739
Anthony Taylor: It kind of jumped out at you

2389
02:33:41.820 --> 02:33:44.360
Anthony Taylor: when we looked at this data for training.

2390
02:33:52.550 --> 02:33:54.110
Derek Rikke: It's in balance.

2391
02:33:54.560 --> 02:33:56.450
Anthony Taylor: Very imbalanced.

2392
02:33:56.750 --> 02:33:59.649
Anthony Taylor: We have very few examples of spam.

2393
02:34:00.810 --> 02:34:03.389
Anthony Taylor: and even less to test.

2394
02:34:04.520 --> 02:34:06.880
Anthony Taylor: Okay? So because of that

2395
02:34:07.290 --> 02:34:11.750
Anthony Taylor: one thing you could do to make this a little better is add more spam.

2396
02:34:12.690 --> 02:34:16.309
Anthony Taylor: I mean, we probably all have enough spam to add to it. Right?

2397
02:34:20.630 --> 02:34:23.490
Anthony Taylor: You could even do this with Svc, because

2398
02:34:23.500 --> 02:34:24.820
Anthony Taylor: it's binary.

2399
02:34:24.860 --> 02:34:29.560
Anthony Taylor: and it's so imbalanced that you might be able to to make it work in Svc.

2400
02:34:30.560 --> 02:34:36.489
Anthony Taylor: so we've covered a lot today. Guys, I mean. The history of Ll. M. Alone is probably a full lecture in most places.

2401
02:34:37.970 --> 02:34:41.080
Anthony Taylor: so that was pretty cool. That was exciting.

2402
02:34:44.580 --> 02:34:48.220
Anthony Taylor: couple of quickie questions. And then I'm gonna let you guys go, I promise.

2403
02:34:48.300 --> 02:34:51.260
Anthony Taylor: But these are just fun, like questions.

2404
02:34:51.470 --> 02:34:52.370
Anthony Taylor: hey?

2405
02:34:55.290 --> 02:35:00.200
Anthony Taylor: consider this sentence. I walked along the river bank to ruminate upon a solution.

2406
02:35:00.810 --> 02:35:01.860
Anthony Taylor: Now.

2407
02:35:02.810 --> 02:35:04.469
Anthony Taylor: this is in our slideshow. I think.

2408
02:35:05.150 --> 02:35:07.850
Anthony Taylor: I hope if it's not, I don't know how we're gonna do this.

2409
02:35:09.500 --> 02:35:10.570
Anthony Taylor: Yeah, here we go.

2410
02:35:11.100 --> 02:35:16.100
Anthony Taylor: walked along a river bank to ruminate upon a solution. Now look at each tokenizer

2411
02:35:16.320 --> 02:35:22.750
Anthony Taylor: for the sentence below, and identify what Tokenizer was used. So what tokenizer was used for this.

2412
02:35:27.840 --> 02:35:29.280
Dipinto, Matt: Word tokenizer.

2413
02:35:29.930 --> 02:35:33.340
Anthony Taylor: Definitely word tokenizer. Right? It's all just words.

2414
02:35:33.390 --> 02:35:34.959
Anthony Taylor: And this one.

2415
02:35:36.580 --> 02:35:37.560
Meredith McCanse (she/her): Character.

2416
02:35:37.560 --> 02:35:38.180
Sihong Zhou: Character.

2417
02:35:38.180 --> 02:35:41.350
Anthony Taylor: Character, and the last one.

2418
02:35:41.560 --> 02:35:43.410
Raugewitz, Tania: port.org.

2419
02:35:44.210 --> 02:35:46.430
Anthony Taylor: Sub word definitely right?

2420
02:35:46.480 --> 02:35:49.559
Anthony Taylor: Alright. Consider this. Oh, that was the answer normally.

2421
02:35:49.800 --> 02:35:54.999
Anthony Taylor: which 2 Nlp challenges are addressed using tokenization.

2422
02:35:55.310 --> 02:35:57.740
Anthony Taylor: Hey? I'll read each of these out loud. Think about

2423
02:35:58.040 --> 02:36:02.659
Anthony Taylor: Nlp. Models require large training sets before they perform well.

2424
02:36:02.670 --> 02:36:04.000
Anthony Taylor: Consistently.

2425
02:36:04.370 --> 02:36:10.560
Anthony Taylor: Nlp problems are classified as AI hard problems.

2426
02:36:11.420 --> 02:36:17.330
Anthony Taylor: Natural language is translated into equivalent numerical representations

2427
02:36:17.340 --> 02:36:21.489
Anthony Taylor: that computers can interpret and models can use.

2428
02:36:22.160 --> 02:36:28.410
Anthony Taylor: Variable text links can be made into fixed link. Vectors that make up

2429
02:36:28.420 --> 02:36:30.710
Anthony Taylor: sparse matches.

2430
02:36:30.710 --> 02:36:31.690
Clayton Graves: 3 and 4.

2431
02:36:31.690 --> 02:36:32.380
Raugewitz, Tania: Painful.

2432
02:36:32.380 --> 02:36:33.849
Sihong Zhou: One and 3.

2433
02:36:35.890 --> 02:36:36.369
Clayton Graves: I say? 3.

2434
02:36:36.370 --> 02:36:37.110
Anthony Taylor: Okay.

2435
02:36:37.690 --> 02:36:39.239
Anthony Taylor: who says 3 and 4

2436
02:36:40.780 --> 02:36:42.270
Anthony Taylor: who says, one and 3.

2437
02:36:43.970 --> 02:36:45.410
Anthony Taylor: Okay, we got couple.

2438
02:36:45.700 --> 02:36:49.770
Anthony Taylor: Okay? So the correct answer is, 3 and 4,

2439
02:36:50.210 --> 02:36:59.119
Anthony Taylor: tokenization helps convert natural language into a format that computers can understand, and addresses the issue of variable text links. Now

2440
02:37:00.140 --> 02:37:03.499
Anthony Taylor: we, I could understand you saying one.

2441
02:37:03.600 --> 02:37:06.269
Anthony Taylor: because we talk about this all the time

2442
02:37:06.560 --> 02:37:09.699
Anthony Taylor: large training sets do give us better

2443
02:37:09.850 --> 02:37:11.910
Anthony Taylor: in LP. Models. There's no question.

2444
02:37:12.070 --> 02:37:13.320
Anthony Taylor: It's not a requirement.

2445
02:37:13.930 --> 02:37:14.750
Anthony Taylor: So

2446
02:37:15.400 --> 02:37:18.460
Anthony Taylor: it's kind of lame. It's like a certification test.

2447
02:37:18.810 --> 02:37:19.643
Anthony Taylor: Okay?

2448
02:37:21.310 --> 02:37:23.270
Clayton Graves: The questions asking

2449
02:37:23.870 --> 02:37:27.499
Clayton Graves: W. What challenges does tokenization address.

2450
02:37:27.500 --> 02:37:31.349
Anthony Taylor: Good. There you go! There you go! That's that's right, it doesn't.

2451
02:37:31.670 --> 02:37:33.789
Anthony Taylor: This does not apply to tokenation.

2452
02:37:34.370 --> 02:37:42.860
Anthony Taylor: Alright. Last one, I promise I'll let you guys go. Imagine you've used similarity search model in this lessons demonstration on 4 new sentences.

2453
02:37:42.980 --> 02:37:47.210
Anthony Taylor: Similarity source. Comparing each of the 4 are in this order.

2454
02:37:47.540 --> 02:37:52.839
Anthony Taylor: Okay, our search period, in order R, which sentence is least

2455
02:37:53.210 --> 02:37:54.490
Anthony Taylor: relevant

2456
02:37:54.830 --> 02:37:57.279
Anthony Taylor: to the search. Query.

2457
02:37:57.960 --> 02:37:59.349
Clayton Graves: 73.

2458
02:37:59.350 --> 02:38:00.470
Sihong Zhou: Start a one.

2459
02:38:02.600 --> 02:38:03.640
Anthony Taylor: Sentence, 3.

2460
02:38:03.760 --> 02:38:05.330
Anthony Taylor: It's got the lowest score.

2461
02:38:06.790 --> 02:38:10.949
Anthony Taylor: Okay. So whatever sentence 4 was was probably exactly what they went.

2462
02:38:11.640 --> 02:38:13.920
Anthony Taylor: Nobody noticed. The answer was down here.

2463
02:38:17.730 --> 02:38:20.359
Anthony Taylor: anyway. Alright, that's it. Guys.

2464
02:38:21.880 --> 02:38:24.769
Anthony Taylor: Good job. Today. Tomorrow you got a stub.

2465
02:38:25.000 --> 02:38:26.050
Anthony Taylor: Be nice

2466
02:38:27.010 --> 02:38:31.619
Anthony Taylor: and have a great lesson, and I will see you guys after Easter

2467
02:38:31.730 --> 02:38:33.090
Anthony Taylor: on Monday.

2468
02:38:33.500 --> 02:38:35.390
Anthony Taylor: they'll take care. Be safe.

2469
02:38:36.030 --> 02:38:38.230
Anthony Taylor: Have a great weekend. We'll be here for 30.

